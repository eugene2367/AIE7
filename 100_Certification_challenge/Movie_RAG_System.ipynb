{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# ðŸŽ¬ Movie Reviews Agentic RAG System\n",
    "\n",
    "A simplified, production-ready implementation of the enhanced agentic RAG system for movie reviews.\n",
    "\n",
    "## Features:\n",
    "- ðŸŽ­ Multi-tool agent with specialized movie analysis capabilities\n",
    "- ðŸ” Multiple retriever methods (Naive, BM25, Multi-Query, Parent-Document, Contextual Compression, Ensemble)\n",
    "- ðŸŒ External search integration (Tavily)\n",
    "- ðŸ“Š LangSmith tracing and monitoring\n",
    "- ðŸ… Rotten Tomatoes dataset analysis\n",
    "\n",
    "## Quick Start:\n",
    "1. Run all cells in order\n",
    "2. Use `query_enhanced_agent_with_tracing(\"your question\")` to interact with the system\n",
    "3. Switch retrievers by updating the `base_retriever` variable\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 1: Setup and Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”‘ Setting up API Keys\n",
      "========================================\n",
      "âœ… OpenAI API key already set\n",
      "âœ… Cohere API key already set\n",
      "âœ… Tavily API key already set\n",
      "âœ… LangSmith API key already set\n",
      "\n",
      "ðŸŽ¯ API Key Setup Complete!\n",
      "âœ… Ready for enhanced movie review RAG system!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import getpass\n",
    "import warnings\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Optional\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from uuid import uuid4\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Apply nest_asyncio for Jupyter compatibility\n",
    "nest_asyncio.apply()\n",
    "\n",
    "print(\"ðŸ”‘ Setting up API Keys\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# OpenAI API Key (required)\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"ðŸ¤– Enter your OpenAI API Key: \")\n",
    "    print(\"âœ… OpenAI API key set\")\n",
    "else:\n",
    "    print(\"âœ… OpenAI API key already set\")\n",
    "\n",
    "# Cohere API Key (required for reranking)\n",
    "if not os.getenv(\"COHERE_API_KEY\"):\n",
    "    os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"ðŸ”„ Enter your Cohere API Key: \")\n",
    "    print(\"âœ… Cohere API key set\")\n",
    "else:\n",
    "    print(\"âœ… Cohere API key already set\")\n",
    "\n",
    "# Tavily API Key (recommended for external search)\n",
    "if not os.getenv(\"TAVILY_API_KEY\"):\n",
    "    tavily_key = getpass.getpass(\"ðŸ” Enter your Tavily API Key (or press Enter to skip): \")\n",
    "    if tavily_key.strip():\n",
    "        os.environ[\"TAVILY_API_KEY\"] = tavily_key\n",
    "        print(\"âœ… Tavily API key set\")\n",
    "    else:\n",
    "        print(\"âš ï¸ Tavily API key skipped - external search will be limited\")\n",
    "else:\n",
    "    print(\"âœ… Tavily API key already set\")\n",
    "\n",
    "# LangSmith API Key (optional for monitoring)\n",
    "if not os.getenv(\"LANGSMITH_API_KEY\"):\n",
    "    langsmith_key = getpass.getpass(\"ðŸ“Š Enter your LangSmith API Key (or press Enter to skip): \")\n",
    "    if langsmith_key.strip():\n",
    "        os.environ[\"LANGSMITH_API_KEY\"] = langsmith_key\n",
    "        os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "        os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "        os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "        os.environ[\"LANGSMITH_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "        print(\"âœ… LangSmith API key set and tracing enabled\")\n",
    "    else:\n",
    "        os.environ[\"LANGSMITH_TRACING\"] = \"false\"\n",
    "        os.environ[\"LANGCHAIN_TRACING_V2\"] = \"false\"\n",
    "        print(\"âš ï¸ LangSmith skipped - no monitoring/tracing\")\n",
    "else:\n",
    "    print(\"âœ… LangSmith API key already set\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ API Key Setup Complete!\")\n",
    "print(\"âœ… Ready for enhanced movie review RAG system!\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 2: Load and Prepare Movie Reviews Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ… Loading Rotten Tomatoes movie review datasets...\n",
      "Loading Rotten Tomatoes movies metadata...\n",
      "  Trying encoding: utf-8\n",
      "  âœ… Success with utf-8\n",
      "Movies dataset: 143258 movies\n",
      "\n",
      "Loading Rotten Tomatoes reviews...\n",
      "  Trying encoding: utf-8\n",
      "  âœ… Success with utf-8\n",
      "Reviews dataset: 1444963 reviews\n",
      "\n",
      "ðŸ“Š Dataset Statistics:\n",
      "â€¢ Total movies: 143,258\n",
      "â€¢ Total reviews: 1,444,963\n",
      "â€¢ Average reviews per movie: 10.1\n"
     ]
    }
   ],
   "source": [
    "# Load the Rotten Tomatoes datasets\n",
    "print(\"ðŸ… Loading Rotten Tomatoes movie review datasets...\")\n",
    "\n",
    "# Robust CSV loading function with error handling\n",
    "def load_csv_robust(filepath):\n",
    "    \"\"\"Load CSV with robust error handling for malformed data\"\"\"\n",
    "    encodings = ['utf-8', 'latin1', 'cp1252', 'iso-8859-1']\n",
    "    \n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            print(f\"  Trying encoding: {encoding}\")\n",
    "            # Try with error handling for malformed lines\n",
    "            df = pd.read_csv(\n",
    "                filepath, \n",
    "                encoding=encoding,\n",
    "                on_bad_lines='skip',  # Skip bad lines instead of failing\n",
    "                engine='python',      # Use Python engine for better error handling\n",
    "                quoting=1,           # Quote all fields\n",
    "                skipinitialspace=True\n",
    "            )\n",
    "            print(f\"  âœ… Success with {encoding}\")\n",
    "            return df\n",
    "        except UnicodeDecodeError:\n",
    "            print(f\"  âŒ Failed with {encoding}\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ Failed with {encoding}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # If all encodings fail, try with minimal options\n",
    "    print(\"  Trying with basic fallback...\")\n",
    "    try:\n",
    "        df = pd.read_csv(filepath, encoding='latin1', on_bad_lines='skip', engine='python')\n",
    "        print(\"  âœ… Success with fallback method\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Could not read {filepath}: {str(e)}\")\n",
    "\n",
    "# Load Rotten Tomatoes movies metadata\n",
    "print(\"Loading Rotten Tomatoes movies metadata...\")\n",
    "movies_df = load_csv_robust(\"data/rotten_tomatoes_movies.csv\")\n",
    "print(f\"Movies dataset: {len(movies_df)} movies\")\n",
    "\n",
    "# Load Rotten Tomatoes reviews\n",
    "print(\"\\nLoading Rotten Tomatoes reviews...\")\n",
    "reviews_df = load_csv_robust(\"data/rotten_tomatoes_movie_reviews.csv\")\n",
    "print(f\"Reviews dataset: {len(reviews_df)} reviews\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Dataset Statistics:\")\n",
    "print(f\"â€¢ Total movies: {len(movies_df):,}\")\n",
    "print(f\"â€¢ Total reviews: {len(reviews_df):,}\")\n",
    "print(f\"â€¢ Average reviews per movie: {len(reviews_df)/len(movies_df):.1f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§¹ Cleaning and preparing movie data...\n",
      "ðŸ”— Merging movies and reviews data...\n",
      "âœ… Merged dataset: 1469840 reviews with movie metadata\n",
      "âœ… Reviews with movie titles: 1469543 / 1469840\n"
     ]
    }
   ],
   "source": [
    "# Clean and prepare the data\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean text data for better processing\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text)\n",
    "    # Remove extra whitespace and normalize\n",
    "    text = ' '.join(text.split())\n",
    "    return text.strip()\n",
    "\n",
    "# Clean movie data\n",
    "print(\"ðŸ§¹ Cleaning and preparing movie data...\")\n",
    "\n",
    "movies_df['title_clean'] = movies_df['title'].apply(clean_text)\n",
    "movies_df['genre_clean'] = movies_df['genre'].apply(clean_text) \n",
    "movies_df['director_clean'] = movies_df['director'].apply(clean_text)\n",
    "\n",
    "# Clean reviews data  \n",
    "reviews_df['reviewText_clean'] = reviews_df['reviewText'].apply(clean_text)\n",
    "reviews_df['criticName_clean'] = reviews_df['criticName'].apply(clean_text)\n",
    "reviews_df['publicatioName_clean'] = reviews_df['publicatioName'].apply(clean_text)\n",
    "\n",
    "print(\"ðŸ”— Merging movies and reviews data...\")\n",
    "\n",
    "# Merge movies and reviews\n",
    "merged_df = reviews_df.merge(\n",
    "    movies_df, \n",
    "    left_on='id', \n",
    "    right_on='id', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(f\"âœ… Merged dataset: {len(merged_df)} reviews with movie metadata\")\n",
    "print(f\"âœ… Reviews with movie titles: {merged_df['title_clean'].notna().sum()} / {len(merged_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ… Creating review documents from Rotten Tomatoes data...\n",
      "ðŸ§ª Using sample of 1000 reviews...\n",
      "âœ… Created 1000 total review documents\n",
      "   - Source: Rotten Tomatoes\n",
      "   - Reviews with movie metadata included\n",
      "\n",
      "ðŸ“„ Sample document:\n",
      "Movie: Beavers\n",
      "Genre: Documentary\n",
      "Director: Stephen Low\n",
      "Rating: nan\n",
      "Release Date: nan\n",
      "Critic: Ivan M. Lincoln\n",
      "Publication: Deseret News (Salt Lake City)\n",
      "Score: 3.5/4\n",
      "Review State: fresh\n",
      "Sentiment: POSITIVE\n",
      "Review: Timed to be just long enough for most youngsters' brief attention spans -- and it's pa...\n",
      "\n",
      "ðŸ“Š Document Statistics:\n",
      "â€¢ Unique movies: 122\n",
      "â€¢ Unique critics: 660\n",
      "â€¢ Average content length: 333 characters\n"
     ]
    }
   ],
   "source": [
    "# Create unified data structure for processing Rotten Tomatoes data\n",
    "def create_review_documents(df, max_reviews=1000):\n",
    "    \"\"\"Convert merged DataFrame to list of review documents\"\"\"\n",
    "    documents = []\n",
    "    \n",
    "    # Use a sample for better performance\n",
    "    if len(df) > max_reviews:\n",
    "        print(f\"ðŸ§ª Using sample of {max_reviews} reviews...\")\n",
    "        df_sample = df.head(max_reviews)\n",
    "    else:\n",
    "        df_sample = df\n",
    "    \n",
    "    for idx, row in df_sample.iterrows():\n",
    "        # Create comprehensive metadata\n",
    "        metadata = {\n",
    "            'source': 'rotten_tomatoes',\n",
    "            'movie_id': row.get('id', ''),\n",
    "            'movie_title': row.get('title_clean', row.get('id', 'Unknown')),\n",
    "            'critic_name': row.get('criticName_clean', 'Anonymous'),\n",
    "            'publication': row.get('publicatioName_clean', 'Unknown'),\n",
    "            'review_date': row.get('creationDate', 'Unknown'),\n",
    "            'original_score': row.get('originalScore', ''),\n",
    "            'review_state': row.get('reviewState', ''),\n",
    "            'sentiment': row.get('scoreSentiment', ''),\n",
    "            'is_top_critic': row.get('isTopCritic', False),\n",
    "            'genre': row.get('genre_clean', ''),\n",
    "            'director': row.get('director_clean', ''),\n",
    "            'rating': row.get('rating', ''),\n",
    "            'audience_score': row.get('audienceScore', ''),\n",
    "            'tomato_meter': row.get('tomatoMeter', ''),\n",
    "            'release_date': row.get('releaseDateTheaters', ''),\n",
    "            'runtime': row.get('runtimeMinutes', ''),\n",
    "            'index': idx\n",
    "        }\n",
    "        \n",
    "        # Create rich content for embedding\n",
    "        content = f\"Movie: {row.get('title_clean', row.get('id', 'Unknown'))}\\n\"\n",
    "        \n",
    "        # Add movie metadata\n",
    "        if row.get('genre_clean'):\n",
    "            content += f\"Genre: {row.get('genre_clean')}\\n\"\n",
    "        if row.get('director_clean'):\n",
    "            content += f\"Director: {row.get('director_clean')}\\n\"\n",
    "        if row.get('rating'):\n",
    "            content += f\"Rating: {row.get('rating')}\\n\"\n",
    "        if row.get('releaseDateTheaters'):\n",
    "            content += f\"Release Date: {row.get('releaseDateTheaters')}\\n\"\n",
    "        \n",
    "        # Add review information\n",
    "        content += f\"Critic: {row.get('criticName_clean', 'Anonymous')}\\n\"\n",
    "        if row.get('publicatioName_clean'):\n",
    "            content += f\"Publication: {row.get('publicatioName_clean')}\\n\"\n",
    "        if row.get('originalScore'):\n",
    "            content += f\"Score: {row.get('originalScore')}\\n\"\n",
    "        if row.get('reviewState'):\n",
    "            content += f\"Review State: {row.get('reviewState')}\\n\"\n",
    "        if row.get('scoreSentiment'):\n",
    "            content += f\"Sentiment: {row.get('scoreSentiment')}\\n\"\n",
    "        \n",
    "        # Add the main review text\n",
    "        review_text = row.get('reviewText_clean', '')\n",
    "        if review_text:\n",
    "            content += f\"Review: {review_text}\"\n",
    "        \n",
    "        documents.append({\n",
    "            'content': content,\n",
    "            'metadata': metadata\n",
    "        })\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# Create documents from merged Rotten Tomatoes data\n",
    "print(\"ðŸ… Creating review documents from Rotten Tomatoes data...\")\n",
    "all_documents = create_review_documents(merged_df, max_reviews=1000)\n",
    "\n",
    "print(f\"âœ… Created {len(all_documents)} total review documents\")\n",
    "print(f\"   - Source: Rotten Tomatoes\")\n",
    "print(f\"   - Reviews with movie metadata included\")\n",
    "\n",
    "# Show sample document\n",
    "print(\"\\nðŸ“„ Sample document:\")\n",
    "print(all_documents[0]['content'][:300] + \"...\")\n",
    "\n",
    "# Basic statistics\n",
    "print(f\"\\nðŸ“Š Document Statistics:\")\n",
    "unique_movies = len(set([doc['metadata']['movie_title'] for doc in all_documents]))\n",
    "unique_critics = len(set([doc['metadata']['critic_name'] for doc in all_documents]))\n",
    "print(f\"â€¢ Unique movies: {unique_movies}\")\n",
    "print(f\"â€¢ Unique critics: {unique_critics}\")\n",
    "print(f\"â€¢ Average content length: {np.mean([len(doc['content']) for doc in all_documents]):.0f} characters\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 3: Setup Basic RAG Components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”ª Using each review as a separate chunk...\n",
      "âœ… Created 1000 chunks from 1000 reviews\n",
      "   Each review is treated as a separate chunk for better semantic coherence\n",
      "ðŸ“ Maximum chunk length: 144 tokens\n",
      "ðŸ“ Average chunk length: 88 tokens\n",
      "ðŸ§  Initializing embedding model...\n",
      "âœ… Basic RAG components initialized!\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "# Token counting function\n",
    "def tiktoken_len(text):\n",
    "    tokens = tiktoken.encoding_for_model(\"gpt-4o\").encode(text)\n",
    "    return len(tokens)\n",
    "\n",
    "# Convert our documents to LangChain Document format - each review is already a chunk\n",
    "print(\"ðŸ”ª Using each review as a separate chunk...\")\n",
    "chunks = []\n",
    "for doc in all_documents:\n",
    "    langchain_doc = Document(\n",
    "        page_content=doc['content'],\n",
    "        metadata=doc['metadata']\n",
    "    )\n",
    "    chunks.append(langchain_doc)\n",
    "\n",
    "print(f\"âœ… Created {len(chunks)} chunks from {len(all_documents)} reviews\")\n",
    "print(\"   Each review is treated as a separate chunk for better semantic coherence\")\n",
    "\n",
    "# Verify chunk sizes\n",
    "chunk_lengths = [tiktoken_len(chunk.page_content) for chunk in chunks]\n",
    "max_chunk_length = max(chunk_lengths)\n",
    "avg_chunk_length = sum(chunk_lengths) / len(chunk_lengths)\n",
    "print(f\"ðŸ“ Maximum chunk length: {max_chunk_length} tokens\")\n",
    "print(f\"ðŸ“ Average chunk length: {avg_chunk_length:.0f} tokens\")\n",
    "\n",
    "# Initialize embedding model\n",
    "print(\"ðŸ§  Initializing embedding model...\")\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Initialize chat model\n",
    "chat_model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.1)\n",
    "\n",
    "print(\"âœ… Basic RAG components initialized!\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 4: Setup External Search and Agent Tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Setting up external search tools...\n",
      "âœ… Tavily search tool configured\n",
      "ðŸ” Using Tavily for external search\n"
     ]
    }
   ],
   "source": [
    "# External search tools setup\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain.tools import Tool\n",
    "from langchain_core.tools import tool\n",
    "import json\n",
    "\n",
    "# Setup external search tools\n",
    "print(\"ðŸ”§ Setting up external search tools...\")\n",
    "\n",
    "# Option 1: Tavily Search (recommended)\n",
    "try:\n",
    "    tavily_search = TavilySearchResults(\n",
    "        max_results=3,\n",
    "        search_depth=\"basic\",\n",
    "        include_answer=True,\n",
    "        include_raw_content=True\n",
    "    )\n",
    "    print(\"âœ… Tavily search tool configured\")\n",
    "    has_tavily = True\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Tavily not configured: {e}\")\n",
    "    has_tavily = False\n",
    "\n",
    "# Create a fallback search function if no external APIs are configured\n",
    "def fallback_search(query: str) -> str:\n",
    "    \"\"\"Fallback search when no external APIs are available\"\"\"\n",
    "    return f\"External search not available. Query '{query}' would require external movie database access. Please configure Tavily API key for enhanced search capabilities.\"\n",
    "\n",
    "# Choose which search tool to use\n",
    "if has_tavily:\n",
    "    external_search_tool = tavily_search\n",
    "    search_tool_name = \"Tavily\"\n",
    "else:\n",
    "    external_search_tool = Tool(\n",
    "        name=\"fallback_search\",\n",
    "        description=\"Fallback search tool when external APIs are not configured\",\n",
    "        func=fallback_search\n",
    "    )\n",
    "    search_tool_name = \"Fallback\"\n",
    "\n",
    "print(f\"ðŸ” Using {search_tool_name} for external search\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up retrievers...\n",
      "âœ… Default retriever initialized\n"
     ]
    }
   ],
   "source": [
    "# Setup retrievers first (we'll use naive as default)\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.retrievers import ParentDocumentRetriever, EnsembleRetriever\n",
    "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
    "from langchain_cohere import CohereRerank\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient, models\n",
    "\n",
    "print(\"Setting up retrievers...\")\n",
    "\n",
    "# 1. Naive Retriever (Embedding-based) - Default\n",
    "def get_base_retriever():\n",
    "    \"\"\"Get the base retriever (can be dynamically switched)\"\"\"\n",
    "    global base_retriever\n",
    "    if 'base_retriever' not in globals() or base_retriever is None:\n",
    "        vectorstore = Qdrant.from_documents(\n",
    "            chunks,\n",
    "            embedding_model,\n",
    "            location=\":memory:\",\n",
    "            collection_name=\"MovieReviews_Default\"\n",
    "        )\n",
    "        base_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})\n",
    "    return base_retriever\n",
    "\n",
    "# Initialize default retriever\n",
    "base_retriever = get_base_retriever()\n",
    "print(\"âœ… Default retriever initialized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Created search_movie_reviews tool\n"
     ]
    }
   ],
   "source": [
    "# Tool 1: Movie Review Search Tool\n",
    "@tool\n",
    "def search_movie_reviews(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Search through embedded movie reviews from Rotten Tomatoes.\n",
    "    Use this for questions about specific movies, ratings, or review content.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use the current base retriever\n",
    "        retriever = get_base_retriever()\n",
    "        docs = retriever.invoke(query)\n",
    "        \n",
    "        if not docs:\n",
    "            return f\"No relevant movie reviews found for: {query}\"\n",
    "        \n",
    "        # Format results\n",
    "        results = f\"Found {len(docs)} relevant movie reviews for '{query}':\\n\\n\"\n",
    "        \n",
    "        for i, doc in enumerate(docs, 1):\n",
    "            metadata = doc.metadata\n",
    "            content = doc.page_content\n",
    "            \n",
    "            results += f\"ðŸ“½ï¸ Result {i}:\\n\"\n",
    "            results += f\"Movie: {metadata.get('movie_title', 'Unknown')}\\n\"\n",
    "            results += f\"Critic: {metadata.get('critic_name', 'Unknown')}\\n\"\n",
    "            if metadata.get('publication'):\n",
    "                results += f\"Publication: {metadata.get('publication')}\\n\"\n",
    "            if metadata.get('original_score'):\n",
    "                results += f\"Score: {metadata.get('original_score')}\\n\"\n",
    "            results += f\"Content: {content[:200]}...\\n\\n\"\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error searching reviews: {str(e)}\"\n",
    "\n",
    "print(\"âœ… Created search_movie_reviews tool\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Created 3 specialized tools:\n",
      "  - search_movie_reviews\n",
      "  - analyze_movie_statistics\n",
      "  - search_external_movie_info\n",
      "\n",
      "âœ… All agent tools ready!\n"
     ]
    }
   ],
   "source": [
    "# Tool 2: Movie Statistics Analysis Tool\n",
    "@tool\n",
    "def analyze_movie_statistics(movie_name: str = \"\") -> str:\n",
    "    \"\"\"\n",
    "    Analyze statistics for a specific movie or provide general Rotten Tomatoes dataset statistics.\n",
    "    Returns ratings, review counts, critic information, and other numerical insights.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if movie_name:\n",
    "            # Search for specific movie in the merged dataset\n",
    "            movie_data = merged_df[\n",
    "                merged_df['title_clean'].str.contains(movie_name, case=False, na=False)\n",
    "            ]\n",
    "            \n",
    "            if movie_data.empty:\n",
    "                return f\"No statistics found for '{movie_name}' in the Rotten Tomatoes dataset.\"\n",
    "            \n",
    "            # Get movie information\n",
    "            movie_info = movie_data.iloc[0]  # Get first match for movie metadata\n",
    "            movie_reviews = movie_data  # All reviews for this movie\n",
    "            \n",
    "            stats = f\"Statistics for '{movie_info.get('title_clean', movie_name)}':\\n\"\n",
    "            stats += f\"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\\n\"\n",
    "            \n",
    "            # Movie metadata\n",
    "            if movie_info.get('genre_clean'):\n",
    "                stats += f\"ðŸŽ­ Genre: {movie_info['genre_clean']}\\n\"\n",
    "            if movie_info.get('director_clean'):\n",
    "                stats += f\"ðŸŽ¬ Director: {movie_info['director_clean']}\\n\"\n",
    "            if movie_info.get('rating'):\n",
    "                stats += f\"ðŸ·ï¸ Rating: {movie_info['rating']}\\n\"\n",
    "            if movie_info.get('runtimeMinutes'):\n",
    "                stats += f\"â±ï¸ Runtime: {movie_info['runtimeMinutes']} minutes\\n\"\n",
    "            if movie_info.get('releaseDateTheaters'):\n",
    "                stats += f\"ðŸ“… Release Date: {movie_info['releaseDateTheaters']}\\n\"\n",
    "            \n",
    "            # Scores\n",
    "            if pd.notna(movie_info.get('audienceScore')):\n",
    "                stats += f\"ðŸ‘¥ Audience Score: {movie_info['audienceScore']}%\\n\"\n",
    "            if pd.notna(movie_info.get('tomatoMeter')):\n",
    "                stats += f\"ðŸ… Tomatometer: {movie_info['tomatoMeter']}%\\n\"\n",
    "            \n",
    "            # Review statistics\n",
    "            stats += f\"\\nðŸ“Š Review Analysis:\\n\"\n",
    "            stats += f\"â€¢ Total Reviews: {len(movie_reviews)}\\n\"\n",
    "            \n",
    "            # Review state distribution\n",
    "            if 'reviewState' in movie_reviews.columns:\n",
    "                review_states = movie_reviews['reviewState'].value_counts()\n",
    "                for state, count in review_states.items():\n",
    "                    stats += f\"â€¢ {state.title()}: {count} reviews\\n\"\n",
    "            \n",
    "            return stats\n",
    "        else:\n",
    "            # General dataset statistics\n",
    "            stats = f\"ðŸ… Rotten Tomatoes Dataset Statistics:\\n\"\n",
    "            stats += f\"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\\n\"\n",
    "            stats += f\"ðŸ“Š Overview:\\n\"\n",
    "            stats += f\"â€¢ Total Movies: {len(movies_df):,}\\n\"\n",
    "            stats += f\"â€¢ Total Reviews: {len(reviews_df):,}\\n\"\n",
    "            stats += f\"â€¢ Reviews in Current Sample: {len(merged_df):,}\\n\"\n",
    "            stats += f\"â€¢ Average Reviews per Movie: {len(reviews_df)/len(movies_df):.1f}\\n\"\n",
    "            \n",
    "            return stats\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"Error analyzing statistics: {str(e)}\"\n",
    "\n",
    "# Tool 3: External Movie Search\n",
    "@tool\n",
    "def search_external_movie_info(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Search external sites for reviews, ratings, or recent news about a movie.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        search_string = f'movie {query} reviews ratings'\n",
    "\n",
    "        if has_tavily:\n",
    "            result = external_search_tool.invoke({\"query\": search_string})\n",
    "            snippets = []\n",
    "            for item in result[:3]:\n",
    "                if isinstance(item, dict):\n",
    "                    url = item.get(\"url\", \"\")\n",
    "                    content = (item.get(\"content\", \"\") or \"\").strip()\n",
    "                    snippets.append(f\"Source: {url}\\n{content[:200]}â€¦\")\n",
    "            return \"\\n\\n\".join(snippets) if snippets else \"No results found.\"\n",
    "        else:\n",
    "            return external_search_tool.run(search_string)\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"External search error: {e}\"\n",
    "\n",
    "# Create the agent's toolbox\n",
    "agent_tools = [\n",
    "    search_movie_reviews,\n",
    "    analyze_movie_statistics, \n",
    "    search_external_movie_info\n",
    "]\n",
    "\n",
    "print(f\"âœ… Created {len(agent_tools)} specialized tools:\")\n",
    "for tool in agent_tools:\n",
    "    print(f\"  - {tool.name}\")\n",
    "\n",
    "print(\"\\nâœ… All agent tools ready!\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 5: Setup Enhanced Agent with Tool Selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– Building enhanced agent with tool selection...\n",
      "âœ… Enhanced agent nodes defined!\n"
     ]
    }
   ],
   "source": [
    "# Enhanced Agent State with Tool Selection\n",
    "from typing_extensions import TypedDict, Annotated\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[BaseMessage], add_messages]\n",
    "    question: str\n",
    "    tool_calls: list\n",
    "    final_answer: str\n",
    "\n",
    "# Enhanced Agent with tool selection capabilities\n",
    "print(\"ðŸ¤– Building enhanced agent with tool selection...\")\n",
    "\n",
    "# Create the agent prompt\n",
    "AGENT_PROMPT = \"\"\"You are an intelligent movie analysis agent with access to multiple specialized tools.\n",
    "\n",
    "Your tools:\n",
    "1. search_movie_reviews: Search embedded movie reviews from Rotten Tomatoes\n",
    "2. analyze_movie_statistics: Get numerical statistics about movies and datasets  \n",
    "3. search_external_movie_info: Search external sources when local data is insufficient\n",
    "\n",
    "Guidelines:\n",
    "- Start with local review data (search_movie_reviews) for most questions\n",
    "- Use statistics tools for numerical analysis\n",
    "- Only use external search when local data is clearly insufficient\n",
    "- Always explain your reasoning and cite sources\n",
    "- Provide comprehensive, insightful answers\n",
    "\n",
    "Current question: {question}\n",
    "\"\"\"\n",
    "\n",
    "# Create enhanced chat model with tool binding\n",
    "agent_model = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\", \n",
    "    temperature=0.1,\n",
    "    max_tokens=1000\n",
    ").bind_tools(agent_tools)\n",
    "\n",
    "def agent_reasoning_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"Agent reasoning and tool selection\"\"\"\n",
    "    question = state[\"question\"]\n",
    "    messages = state.get(\"messages\", [])\n",
    "    \n",
    "    # Create the prompt with current question\n",
    "    prompt_message = HumanMessage(content=AGENT_PROMPT.format(question=question))\n",
    "    \n",
    "    # Get agent response with potential tool calls\n",
    "    response = agent_model.invoke([prompt_message] + messages)\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [response],\n",
    "        \"tool_calls\": response.tool_calls if hasattr(response, 'tool_calls') and response.tool_calls else []\n",
    "    }\n",
    "\n",
    "def tool_execution_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"Execute selected tools\"\"\"\n",
    "    tool_calls = state.get(\"tool_calls\", [])\n",
    "    messages = []\n",
    "    \n",
    "    for tool_call in tool_calls:\n",
    "        tool_name = tool_call[\"name\"]\n",
    "        tool_args = tool_call[\"args\"]\n",
    "        \n",
    "        # Find and execute the tool\n",
    "        for tool in agent_tools:\n",
    "            if tool.name == tool_name:\n",
    "                try:\n",
    "                    result = tool.invoke(tool_args)\n",
    "                    # Create tool message\n",
    "                    tool_message = ToolMessage(\n",
    "                        content=str(result),\n",
    "                        tool_call_id=tool_call[\"id\"]\n",
    "                    )\n",
    "                    messages.append(tool_message)\n",
    "                except Exception as e:\n",
    "                    error_message = ToolMessage(\n",
    "                        content=f\"Error executing {tool_name}: {str(e)}\",\n",
    "                        tool_call_id=tool_call[\"id\"]\n",
    "                    )\n",
    "                    messages.append(error_message)\n",
    "                break\n",
    "    \n",
    "    return {\"messages\": messages}\n",
    "\n",
    "def final_response_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"Generate final response based on tool results\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    # Create final prompt\n",
    "    final_prompt = f\"\"\"\n",
    "    Based on the tool results above, provide a comprehensive answer to the question: {question}\n",
    "    \n",
    "    Make sure to:\n",
    "    - Synthesize information from multiple sources\n",
    "    - Cite specific data points and sources\n",
    "    - Provide insights beyond just raw data\n",
    "    - Be conversational but informative\n",
    "    \"\"\"\n",
    "    \n",
    "    final_response = chat_model.invoke(messages + [HumanMessage(content=final_prompt)])\n",
    "    \n",
    "    return {\n",
    "        \"final_answer\": final_response.content,\n",
    "        \"messages\": [final_response]\n",
    "    }\n",
    "\n",
    "print(\"âœ… Enhanced agent nodes defined!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”— Building agent workflow...\n",
      "âœ… Enhanced agent with tool selection ready!\n",
      "ðŸŽ¯ LangSmith project: Movie-Reviews-RAG-05e3c697\n",
      "ðŸš€ Enhanced agent ready for movie analysis!\n"
     ]
    }
   ],
   "source": [
    "# Build the enhanced agent graph\n",
    "print(\"ðŸ”— Building agent workflow...\")\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "# Create agent graph\n",
    "agent_graph = StateGraph(AgentState)\n",
    "\n",
    "# Add nodes\n",
    "agent_graph.add_node(\"agent\", agent_reasoning_node)\n",
    "agent_graph.add_node(\"tools\", ToolNode(agent_tools))\n",
    "agent_graph.add_node(\"final_response\", final_response_node)\n",
    "\n",
    "# Add edges\n",
    "agent_graph.add_edge(START, \"agent\")\n",
    "\n",
    "# Conditional edge: if agent makes tool calls, go to tools; otherwise go to final response\n",
    "def should_continue(state: AgentState) -> str:\n",
    "    tool_calls = state.get(\"tool_calls\", [])\n",
    "    if tool_calls:\n",
    "        return \"tools\"\n",
    "    else:\n",
    "        return \"final_response\"\n",
    "\n",
    "agent_graph.add_conditional_edges(\"agent\", should_continue)\n",
    "agent_graph.add_edge(\"tools\", \"final_response\")\n",
    "agent_graph.add_edge(\"final_response\", END)\n",
    "\n",
    "# Compile the enhanced agent\n",
    "enhanced_agent = agent_graph.compile()\n",
    "\n",
    "print(\"âœ… Enhanced agent with tool selection ready!\")\n",
    "\n",
    "# Generate unique project ID for this session\n",
    "unique_id = uuid4().hex[:8]\n",
    "project_name = f\"Movie-Reviews-RAG-{unique_id}\"\n",
    "\n",
    "# Configure LangSmith if available\n",
    "if os.getenv(\"LANGSMITH_API_KEY\"):\n",
    "    os.environ[\"LANGSMITH_PROJECT\"] = project_name\n",
    "    print(f\"ðŸŽ¯ LangSmith project: {project_name}\")\n",
    "\n",
    "print(\"ðŸš€ Enhanced agent ready for movie analysis!\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 6: Setup Different Retrievers (Optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Setup different retrievers for dynamic switching\n",
    "# print(\"Setting up different retrievers...\")\n",
    "\n",
    "# # 1. Naive Retriever (Embedding-based)\n",
    "# def create_naive_retriever():\n",
    "#     vectorstore = Qdrant.from_documents(\n",
    "#         chunks,\n",
    "#         embedding_model,\n",
    "#         location=\":memory:\",\n",
    "#         collection_name=\"MovieReviews_Naive\"\n",
    "#     )\n",
    "#     return vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "# naive_retriever = create_naive_retriever()\n",
    "# print(\"âœ… 1. Naive retriever ready\")\n",
    "\n",
    "# # 2. BM25 Retriever (Keyword-based)\n",
    "# def create_bm25_retriever():\n",
    "#     bm25 = BM25Retriever.from_documents(chunks)\n",
    "#     bm25.k = 5\n",
    "#     return bm25\n",
    "\n",
    "# bm25_retriever = create_bm25_retriever()\n",
    "# print(\"âœ… 2. BM25 retriever ready\")\n",
    "\n",
    "# # 3. Multi-Query Retriever\n",
    "# def create_multi_query_retriever():\n",
    "#     base_retriever = create_naive_retriever()\n",
    "#     return MultiQueryRetriever.from_llm(\n",
    "#         retriever=base_retriever, \n",
    "#         llm=chat_model\n",
    "#     )\n",
    "\n",
    "# multi_query_retriever = create_multi_query_retriever()\n",
    "# print(\"âœ… 3. Multi-query retriever ready\")\n",
    "\n",
    "# # 4. Parent Document Retriever\n",
    "# def create_parent_document_retriever():\n",
    "#     # Create smaller chunks for parent document retrieval\n",
    "#     child_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n",
    "    \n",
    "#     # Create new QdrantClient and collection for parent docs\n",
    "#     client = QdrantClient(location=\":memory:\")\n",
    "#     client.create_collection(\n",
    "#         collection_name=\"movie_reviews_parent_docs\",\n",
    "#         vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE)\n",
    "#     )\n",
    "    \n",
    "#     parent_document_vectorstore = QdrantVectorStore(\n",
    "#         collection_name=\"movie_reviews_parent_docs\", \n",
    "#         embedding=embedding_model, \n",
    "#         client=client\n",
    "#     )\n",
    "    \n",
    "#     store = InMemoryStore()\n",
    "#     parent_retriever = ParentDocumentRetriever(\n",
    "#         vectorstore=parent_document_vectorstore,\n",
    "#         docstore=store,\n",
    "#         child_splitter=child_splitter,\n",
    "#     )\n",
    "    \n",
    "#     parent_retriever.add_documents(chunks, ids=None)\n",
    "#     return parent_retriever\n",
    "\n",
    "# parent_document_retriever = create_parent_document_retriever()\n",
    "# print(\"âœ… 4. Parent document retriever ready\")\n",
    "\n",
    "# # 5. Contextual Compression Retriever (with Cohere reranking)\n",
    "# def create_compression_retriever():\n",
    "#     base_retriever = create_naive_retriever()\n",
    "#     compressor = CohereRerank(model=\"rerank-v3.5\")\n",
    "#     return ContextualCompressionRetriever(\n",
    "#         base_compressor=compressor, \n",
    "#         base_retriever=base_retriever\n",
    "#     )\n",
    "\n",
    "# compression_retriever = create_compression_retriever()\n",
    "# print(\"âœ… 5. Contextual compression retriever ready\")\n",
    "\n",
    "# # 6. Ensemble Retriever (combines multiple approaches)\n",
    "# def create_ensemble_retriever():\n",
    "#     # Use fresh instances to avoid conflicts\n",
    "#     naive = create_naive_retriever()\n",
    "#     bm25 = create_bm25_retriever()\n",
    "#     compression = create_compression_retriever()\n",
    "    \n",
    "#     retrievers = [bm25, naive, compression]\n",
    "#     weights = [0.4, 0.4, 0.2]  # Slightly favor BM25 and naive\n",
    "    \n",
    "#     return EnsembleRetriever(\n",
    "#         retrievers=retrievers, \n",
    "#         weights=weights\n",
    "#     )\n",
    "\n",
    "# ensemble_retriever = create_ensemble_retriever()\n",
    "# print(\"âœ… 6. Ensemble retriever ready\")\n",
    "\n",
    "# print(f\"\\nâœ… All retrievers ready! You can switch between them by updating 'base_retriever'.\")\n",
    "# print(\"Available retrievers: naive_retriever, bm25_retriever, multi_query_retriever, parent_document_retriever, compression_retriever, ensemble_retriever\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 7: Query Function for Frontend Integration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Enhanced agent ready for movie analysis!\n",
      "âœ… LangSmith tracing configured!\n",
      "\n",
      "ðŸŽ¬ Ready to use! Try: query_enhanced_agent_with_tracing('What are the best rated movies?')\n"
     ]
    }
   ],
   "source": [
    "# Create a query function for the enhanced agent with tracing\n",
    "def query_enhanced_agent_with_tracing(question: str, run_name: str = None) -> str:\n",
    "    \"\"\"Query the enhanced agent with LangSmith tracing\"\"\"\n",
    "    \n",
    "    # Generate run name if not provided\n",
    "    if not run_name:\n",
    "        run_name = f\"movie_query_{int(time.time())}\"\n",
    "    \n",
    "    # Add tags for better organization\n",
    "    tags = [\"movie-reviews\", \"rag-agent\", \"multi-tool\"]\n",
    "    \n",
    "    try:\n",
    "        # Execute with tracing metadata\n",
    "        start_time = time.time()\n",
    "        \n",
    "        result = enhanced_agent.invoke(\n",
    "            {\n",
    "                \"question\": question,\n",
    "                \"messages\": [],\n",
    "                \"tool_calls\": [],\n",
    "                \"final_answer\": \"\"\n",
    "            },\n",
    "            config={\n",
    "                \"tags\": tags,\n",
    "                \"metadata\": {\n",
    "                    \"query_type\": \"movie_analysis\",\n",
    "                    \"session_id\": unique_id,\n",
    "                    \"run_name\": run_name\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "        \n",
    "        # Return just the answer for simple frontend integration\n",
    "        return result.get(\"final_answer\", \"No answer generated\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "print(\"ðŸš€ Enhanced agent ready for movie analysis!\")\n",
    "print(\"âœ… LangSmith tracing configured!\")\n",
    "print(\"\\nðŸŽ¬ Ready to use! Try: query_enhanced_agent_with_tracing('What are the best rated movies?')\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Usage Examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¬ Testing the Movie RAG System\n",
      "==================================================\n",
      "Question: What are some highly rated movies in the database?\n",
      "\n",
      "Answer:\n",
      "Based on the extensive dataset from Rotten Tomatoes, which includes a total of 143,258 movies and over 1.4 million reviews, we can identify some highly rated films that have garnered significant acclaim from both critics and audiences.\n",
      "\n",
      "### Highly Rated Movies\n",
      "\n",
      "1. **\"The Godfather\" (1972)** - Often hailed as one of the greatest films of all time, \"The Godfather\" boasts a high approval rating on Rotten Tomatoes, typically around 97%. Its compelling narrative and iconic performances have solidified its status in cinematic history.\n",
      "\n",
      "2. **\"Parasite\" (2019)** - This South Korean film made waves globally, winning the Academy Award for Best Picture. It holds a remarkable 98% rating on Rotten Tomatoes, praised for its sharp social commentary and masterful storytelling.\n",
      "\n",
      "3. **\"Schindler's List\" (1993)** - Steven Spielberg's poignant historical drama is another standout, frequently rated above 90%. Its emotional depth and historical significance resonate with viewers and critics alike.\n",
      "\n",
      "4. **\"The Shawshank Redemption\" (1994)** - With a Rotten Tomatoes score often hovering around 91%, this film has become a beloved classic, celebrated for its themes of hope and friendship.\n",
      "\n",
      "5. **\"Black Panther\" (2018)** - As a cultural milestone in superhero cinema, \"Black Panther\" received a 96% rating, recognized for its groundbreaking representation and impactful storytelling.\n",
      "\n",
      "### Insights Beyond Raw Data\n",
      "\n",
      "While the ratings provide a quantitative measure of a film's reception, they also reflect broader trends in cinema. For instance, the rise of diverse storytelling, as seen with films like \"Parasite\" and \"Black Panther,\" indicates a shift towards more inclusive narratives that resonate with a global audience. \n",
      "\n",
      "Moreover, the average of 10.1 reviews per movie in the dataset suggests that even lesser-known films can achieve high ratings if they resonate well with critics and audiences. This highlights the importance of critical reception in shaping a film's legacy.\n",
      "\n",
      "In conclusion, the Rotten Tomatoes dataset not only showcases individual films but also illustrates evolving cinematic trends and audience preferences. Whether you're a casual viewer or a film aficionado, exploring these highly rated movies can provide a rich tapestry of storytelling and artistic expression.\n",
      "\n",
      "==================================================\n",
      "ðŸŽ¯ System ready for frontend integration!\n",
      "ðŸ”§ To switch retrievers, update the 'base_retriever' variable\n",
      "ðŸ“Š Check LangSmith for detailed tracing and analytics\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "print(\"ðŸŽ¬ Testing the Movie RAG System\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test query\n",
    "test_question = \"What are some highly rated movies in the database?\"\n",
    "print(f\"Question: {test_question}\")\n",
    "print(\"\\nAnswer:\")\n",
    "answer = query_enhanced_agent_with_tracing(test_question)\n",
    "print(answer)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ðŸŽ¯ System ready for frontend integration!\")\n",
    "print(\"ðŸ”§ To switch retrievers, update the 'base_retriever' variable\")\n",
    "print(\"ðŸ“Š Check LangSmith for detailed tracing and analytics\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Retriever Switching\n",
    "\n",
    "You can switch between different retrievers by updating the `base_retriever` variable:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Uncomment the lines above to switch retrievers\n",
      "Current retriever: VectorStoreRetriever\n"
     ]
    }
   ],
   "source": [
    "# Example: Switch to BM25 retriever\n",
    "# base_retriever = bm25_retriever\n",
    "# print(\"Switched to BM25 retriever\")\n",
    "\n",
    "# Example: Switch to Ensemble retriever\n",
    "# base_retriever = ensemble_retriever\n",
    "# print(\"Switched to Ensemble retriever\")\n",
    "\n",
    "# Example: Switch back to Naive retriever\n",
    "# base_retriever = naive_retriever\n",
    "# print(\"Switched to Naive retriever\")\n",
    "\n",
    "print(\"ðŸ’¡ Uncomment the lines above to switch retrievers\")\n",
    "print(f\"Current retriever: {type(base_retriever).__name__}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Movie Reviews RAG",
   "language": "python",
   "name": "movie-reviews-rag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
