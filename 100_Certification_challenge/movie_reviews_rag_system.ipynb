{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323b1762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAGAS Evaluation with Individual Question Analysis\n",
    "print(\"ğŸ”¬ Setting up RAGAS Evaluation with Individual Question Analysis...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# First, let's run our RAG system on each question in the synthetic dataset\n",
    "def run_rag_on_synthetic_dataset(synthetic_df, rag_system):\n",
    "    \"\"\"Run RAG system on synthetic dataset and prepare for RAGAS evaluation\"\"\"\n",
    "    \n",
    "    print(f\"ğŸ“Š Processing {len(synthetic_df)} questions through RAG system...\")\n",
    "    \n",
    "    # Convert synthetic_df to list of test samples\n",
    "    evaluation_samples = []\n",
    "    \n",
    "    for idx, row in synthetic_df.iterrows():\n",
    "        question = row['user_input']\n",
    "        reference = row['reference']\n",
    "        \n",
    "        print(f\"Processing question {idx+1}/{len(synthetic_df)}: {question[:50]}...\")\n",
    "        \n",
    "        try:\n",
    "            # Run enhanced agent with tracing\n",
    "            agent_result = query_enhanced_agent_with_tracing(\n",
    "                question, \n",
    "                run_name=f\"ragas_eval_q{idx+1}\"\n",
    "            )\n",
    "            \n",
    "            if agent_result['success']:\n",
    "                # Get contexts using basic RAG for proper retrieval context\n",
    "                rag_result = rag_graph.invoke({\"question\": question})\n",
    "                retrieved_contexts = [doc.page_content for doc in rag_result[\"context\"]]\n",
    "                \n",
    "                # Create evaluation sample in RAGAS format\n",
    "                sample = {\n",
    "                    'user_input': question,\n",
    "                    'reference': reference,\n",
    "                    'response': agent_result['answer'],\n",
    "                    'retrieved_contexts': retrieved_contexts\n",
    "                }\n",
    "                \n",
    "                evaluation_samples.append(sample)\n",
    "                \n",
    "            else:\n",
    "                print(f\"   âŒ Failed to get response for question {idx+1}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ Error processing question {idx+1}: {e}\")\n",
    "    \n",
    "    print(f\"âœ… Successfully processed {len(evaluation_samples)} questions\")\n",
    "    return evaluation_samples\n",
    "\n",
    "# Run RAG system on synthetic dataset\n",
    "if 'synthetic_df' in locals():\n",
    "    evaluation_samples = run_rag_on_synthetic_dataset(synthetic_df, rag_graph)\n",
    "else:\n",
    "    print(\"âŒ synthetic_df not found. Please run the synthetic data generation cell first.\")\n",
    "    # Create a fallback dataset for demonstration\n",
    "    evaluation_samples = [\n",
    "        {\n",
    "            'user_input': 'What do critics think about Inception?',\n",
    "            'reference': 'Inception is highly praised for its complex plot and visual effects.',\n",
    "            'response': 'Sample response about Inception',\n",
    "            'retrieved_contexts': ['Sample context about Inception reviews']\n",
    "        }\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e12a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to RAGAS EvaluationDataset and run individual metric evaluation\n",
    "print(\"ğŸ¯ Running RAGAS Evaluation with Individual Question Analysis...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    from ragas import evaluate, EvaluationDataset\n",
    "    from ragas.metrics import (\n",
    "        LLMContextRecall,\n",
    "        Faithfulness, \n",
    "        FactualCorrectness,\n",
    "        ResponseRelevancy,\n",
    "        ContextEntityRecall\n",
    "    )\n",
    "    from ragas.llms import LangchainLLMWrapper\n",
    "    from ragas import RunConfig\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Convert evaluation samples to DataFrame\n",
    "    eval_df = pd.DataFrame(evaluation_samples)\n",
    "    \n",
    "    # Create RAGAS EvaluationDataset\n",
    "    evaluation_dataset = EvaluationDataset.from_pandas(eval_df)\n",
    "    print(f\"âœ… Created RAGAS evaluation dataset with {len(evaluation_dataset)} samples\")\n",
    "    \n",
    "    # Set up evaluator LLM (different from generation LLM for better evaluation)\n",
    "    evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\", temperature=0))\n",
    "    \n",
    "    # Define metrics to evaluate\n",
    "    metrics = [\n",
    "        LLMContextRecall(),\n",
    "        Faithfulness(),\n",
    "        FactualCorrectness(), \n",
    "        ResponseRelevancy(),\n",
    "        ContextEntityRecall()\n",
    "    ]\n",
    "    \n",
    "    print(f\"ğŸ“Š Running evaluation with {len(metrics)} metrics...\")\n",
    "    print(\"Metrics: LLMContextRecall, Faithfulness, FactualCorrectness, ResponseRelevancy, ContextEntityRecall\")\n",
    "    \n",
    "    # Configure run settings\n",
    "    custom_run_config = RunConfig(timeout=360)\n",
    "    \n",
    "    # Run evaluation\n",
    "    result = evaluate(\n",
    "        dataset=evaluation_dataset,\n",
    "        metrics=metrics,\n",
    "        llm=evaluator_llm,\n",
    "        run_config=custom_run_config\n",
    "    )\n",
    "    \n",
    "    print(\"\\nâœ… RAGAS evaluation completed!\")\n",
    "    \n",
    "    # Display overall results\n",
    "    print(\"\\nğŸ“ˆ Overall RAGAS Metrics:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for metric_name, score in result.items():\n",
    "        if isinstance(score, (int, float)):\n",
    "            # Interpret score\n",
    "            if score >= 0.8:\n",
    "                status = \"ğŸŸ¢ Excellent\"\n",
    "            elif score >= 0.6:\n",
    "                status = \"ğŸŸ¡ Good\"\n",
    "            else:\n",
    "                status = \"ğŸ”´ Needs Improvement\"\n",
    "            \n",
    "            print(f\"{metric_name:20s}: {score:.4f} {status}\")\n",
    "    \n",
    "    # Convert results to DataFrame for detailed analysis\n",
    "    results_df = result.to_pandas()\n",
    "    \n",
    "    print(f\"\\nğŸ“ Individual Question Results:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Display individual question results\n",
    "    for idx, row in results_df.iterrows():\n",
    "        print(f\"\\nğŸ” Question {idx + 1}:\")\n",
    "        print(f\"â“ {row['user_input'][:100]}...\")\n",
    "        print(f\"ğŸ¤– Response: {row['response'][:100]}...\")\n",
    "        print(f\"âœ… Reference: {row['reference'][:100]}...\")\n",
    "        \n",
    "        print(f\"ğŸ“Š Scores:\")\n",
    "        for metric in ['llm_context_recall', 'faithfulness', 'factual_correctness', 'response_relevancy', 'context_entity_recall']:\n",
    "            if metric in row and pd.notna(row[metric]):\n",
    "                score = row[metric]\n",
    "                if score >= 0.8:\n",
    "                    emoji = \"ğŸŸ¢\"\n",
    "                elif score >= 0.6:\n",
    "                    emoji = \"ğŸŸ¡\"\n",
    "                else:\n",
    "                    emoji = \"ğŸ”´\"\n",
    "                print(f\"   {metric:20s}: {score:.4f} {emoji}\")\n",
    "        \n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    # Save results\n",
    "    ragas_evaluation_complete = True\n",
    "    ragas_results = result\n",
    "    ragas_detailed_results = results_df\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"âŒ RAGAS import error: {e}\")\n",
    "    print(\"ğŸ’¡ Please ensure RAGAS is properly installed with: uv sync\")\n",
    "    ragas_evaluation_complete = False\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ RAGAS evaluation failed: {e}\")\n",
    "    print(\"ğŸ’¡ This might be due to API rate limits or timeouts. Try reducing the dataset size.\")\n",
    "    ragas_evaluation_complete = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef4fe4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Analysis and LangSmith Integration\n",
    "print(\"ğŸ“Š Advanced RAGAS Analysis and LangSmith Integration...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if ragas_evaluation_complete:\n",
    "    # Detailed metric analysis\n",
    "    print(\"\\nğŸ”¬ Detailed Metric Analysis:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    metric_descriptions = {\n",
    "        'llm_context_recall': 'How well the retrieved context covers the reference answer',\n",
    "        'faithfulness': 'How faithful the response is to the retrieved context',\n",
    "        'factual_correctness': 'Factual accuracy of the response',\n",
    "        'response_relevancy': 'How relevant the response is to the question',\n",
    "        'context_entity_recall': 'How well entities from reference are covered in context'\n",
    "    }\n",
    "    \n",
    "    for metric, description in metric_descriptions.items():\n",
    "        if metric in ragas_detailed_results.columns:\n",
    "            scores = ragas_detailed_results[metric].dropna()\n",
    "            if len(scores) > 0:\n",
    "                avg_score = scores.mean()\n",
    "                min_score = scores.min()\n",
    "                max_score = scores.max()\n",
    "                \n",
    "                print(f\"\\nğŸ¯ {metric.replace('_', ' ').title()}\")\n",
    "                print(f\"   Description: {description}\")\n",
    "                print(f\"   Average: {avg_score:.4f}\")\n",
    "                print(f\"   Range: {min_score:.4f} - {max_score:.4f}\")\n",
    "                \n",
    "                # Find best and worst performing questions\n",
    "                best_idx = scores.idxmax()\n",
    "                worst_idx = scores.idxmin()\n",
    "                \n",
    "                print(f\"   Best Question ({scores[best_idx]:.4f}): {ragas_detailed_results.loc[best_idx, 'user_input'][:50]}...\")\n",
    "                print(f\"   Worst Question ({scores[worst_idx]:.4f}): {ragas_detailed_results.loc[worst_idx, 'user_input'][:50]}...\")\n",
    "    \n",
    "    # Upload to LangSmith if configured\n",
    "    if os.getenv(\"LANGSMITH_API_KEY\"):\n",
    "        print(\"\\nğŸ“¤ Uploading RAGAS results to LangSmith...\")\n",
    "        \n",
    "        try:\n",
    "            from langsmith import Client\n",
    "            client = Client()\n",
    "            \n",
    "            # Create evaluation session\n",
    "            eval_session_name = f\"RAGAS-Individual-Evaluation-{unique_id}\"\n",
    "            \n",
    "            # Upload individual question evaluations\n",
    "            for idx, row in ragas_detailed_results.iterrows():\n",
    "                # Create detailed metadata for each question\n",
    "                metadata = {\n",
    "                    \"question_id\": f\"q_{idx+1}\",\n",
    "                    \"user_input\": row['user_input'],\n",
    "                    \"response\": row['response'],\n",
    "                    \"reference\": row['reference'],\n",
    "                    \"evaluation_session\": eval_session_name\n",
    "                }\n",
    "                \n",
    "                # Add RAGAS scores to metadata\n",
    "                for metric in ['llm_context_recall', 'faithfulness', 'factual_correctness', 'response_relevancy', 'context_entity_recall']:\n",
    "                    if metric in row and pd.notna(row[metric]):\n",
    "                        metadata[f\"ragas_{metric}\"] = float(row[metric])\n",
    "                \n",
    "                # Create a trace for this evaluation\n",
    "                with client.trace(\n",
    "                    name=f\"RAGAS_Question_{idx+1}\",\n",
    "                    project_name=project_name,\n",
    "                    tags=[\"ragas-individual\", \"question-evaluation\"],\n",
    "                    metadata=metadata\n",
    "                ) as trace:\n",
    "                    pass\n",
    "            \n",
    "            print(f\"âœ… Uploaded {len(ragas_detailed_results)} individual evaluations to LangSmith\")\n",
    "            print(f\"ğŸ”— View in project: {project_name}\")\n",
    "            print(f\"ğŸ“Š Dashboard: https://smith.langchain.com/\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Failed to upload to LangSmith: {e}\")\n",
    "    \n",
    "    # Create summary report\n",
    "    print(f\"\\nğŸ“‹ RAGAS Evaluation Summary Report:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    total_questions = len(ragas_detailed_results)\n",
    "    print(f\"ğŸ“Š Total Questions Evaluated: {total_questions}\")\n",
    "    \n",
    "    # Calculate overall performance\n",
    "    metric_columns = [col for col in ragas_detailed_results.columns if col in metric_descriptions.keys()]\n",
    "    if metric_columns:\n",
    "        overall_avg = ragas_detailed_results[metric_columns].mean().mean()\n",
    "        print(f\"ğŸ¯ Overall Performance Score: {overall_avg:.4f}\")\n",
    "        \n",
    "        if overall_avg >= 0.8:\n",
    "            print(\"ğŸŒŸ Excellent! Your RAG system is performing very well.\")\n",
    "        elif overall_avg >= 0.6:\n",
    "            print(\"ğŸ‘ Good performance with room for optimization.\")\n",
    "        else:\n",
    "            print(\"ğŸ”§ Performance needs improvement. Consider system tuning.\")\n",
    "    \n",
    "    print(f\"\\nâœ… Individual RAGAS evaluation complete!\")\n",
    "    print(f\"ğŸ“ˆ Each question has been evaluated on multiple metrics\")\n",
    "    print(f\"ğŸ” Detailed results show strengths and weaknesses per question\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ RAGAS evaluation was not completed successfully.\")\n",
    "    print(\"ğŸ’¡ Please check the previous cell for errors and try again.\")\n",
    "\n",
    "print(f\"\\nğŸ Comprehensive Individual Question RAGAS Evaluation Complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729f103d",
   "metadata": {},
   "source": [
    "# Movie Reviews RAG Agentic Solution ğŸ…\n",
    "\n",
    "This notebook implements an end-to-end RAG (Retrieval Augmented Generation) system for analyzing movie reviews from the **Rotten Tomatoes dataset** - providing access to professional critic reviews and comprehensive movie metadata.\n",
    "\n",
    "## Overview\n",
    "\n",
    "We'll build a system that can:\n",
    "- Load and process movie review data from Rotten Tomatoes\n",
    "- Generate embeddings for review text with rich metadata\n",
    "- Implement semantic search for relevant reviews\n",
    "- Generate intelligent responses to movie-related queries\n",
    "- Provide insights and analysis based on professional critic reviews\n",
    "- Leverage Tomatometer scores, audience ratings, and critic consensus\n",
    "\n",
    "## Data Sources \n",
    "- **Rotten Tomatoes Movies** (17MB): Movie metadata with titles, ratings, genres, directors, runtime, release dates, etc.\n",
    "- **Rotten Tomatoes Reviews** (392MB): Professional critic reviews with scores, sentiment, publications, and detailed text\n",
    "\n",
    "## Key Features âœ¨\n",
    "- **Rich Movie Metadata**: Genre, director, runtime, release dates, ratings\n",
    "- **Professional Critics**: Reviews from top critics and publications\n",
    "- **Tomatometer & Audience Scores**: Official Rotten Tomatoes scoring system\n",
    "- **Fresh/Rotten Classification**: Review state analysis\n",
    "- **Sentiment Analysis**: Positive/negative review sentiment\n",
    "- **Multi-tool Agent**: Specialized tools for different types of analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948da4d4",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 1: Environment Setup and Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "510b9b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Optional\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Apply nest_asyncio for Jupyter compatibility\n",
    "nest_asyncio.apply()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e97031d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”‘ Setting up API Keys\n",
      "========================================\n",
      "âœ… OpenAI API key already set\n",
      "âœ… Tavily API key already set\n",
      "âœ… SerpAPI key already set\n",
      "âœ… LangSmith API key already set\n",
      "\n",
      "ğŸ¯ API Key Setup Complete!\n",
      "\n",
      "ğŸ“‹ Where to get API keys:\n",
      "â€¢ OpenAI: https://platform.openai.com/api-keys\n",
      "â€¢ Tavily: https://tavily.com/ (free tier available)\n",
      "â€¢ SerpAPI: https://serpapi.com/ (free tier available)\n",
      "â€¢ LangSmith: https://smith.langchain.com/ (optional monitoring)\n"
     ]
    }
   ],
   "source": [
    "# API Keys Setup\n",
    "import os\n",
    "import getpass\n",
    "\n",
    "print(\"ğŸ”‘ Setting up API Keys\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# OpenAI API Key (required)\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"ğŸ¤– Enter your OpenAI API Key: \")\n",
    "    print(\"âœ… OpenAI API key set\")\n",
    "else:\n",
    "    print(\"âœ… OpenAI API key already set\")\n",
    "\n",
    "# Tavily API Key (recommended for external search)\n",
    "if not os.getenv(\"TAVILY_API_KEY\"):\n",
    "    tavily_key = getpass.getpass(\"ğŸ” Enter your Tavily API Key (or press Enter to skip): \")\n",
    "    if tavily_key.strip():\n",
    "        os.environ[\"TAVILY_API_KEY\"] = tavily_key\n",
    "        print(\"âœ… Tavily API key set\")\n",
    "    else:\n",
    "        print(\"âš ï¸ Tavily API key skipped - external search will be limited\")\n",
    "else:\n",
    "    print(\"âœ… Tavily API key already set\")\n",
    "\n",
    "# SerpAPI Key (optional backup for external search)\n",
    "if not os.getenv(\"SERPAPI_API_KEY\"):\n",
    "    serp_key = getpass.getpass(\"ğŸŒ Enter your SerpAPI Key (or press Enter to skip): \")\n",
    "    if serp_key.strip():\n",
    "        os.environ[\"SERPAPI_API_KEY\"] = serp_key\n",
    "        print(\"âœ… SerpAPI key set\")\n",
    "    else:\n",
    "        print(\"âš ï¸ SerpAPI key skipped - will use Tavily or fallback\")\n",
    "else:\n",
    "    print(\"âœ… SerpAPI key already set\")\n",
    "\n",
    "# LangSmith API Key (optional for monitoring)\n",
    "if not os.getenv(\"LANGSMITH_API_KEY\"):\n",
    "    langsmith_key = getpass.getpass(\"ğŸ“Š Enter your LangSmith API Key (or press Enter to skip): \")\n",
    "    if langsmith_key.strip():\n",
    "        os.environ[\"LANGSMITH_API_KEY\"] = langsmith_key\n",
    "        os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "        os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "        os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "        os.environ[\"LANGSMITH_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "        print(\"âœ… LangSmith API key set and tracing enabled\")\n",
    "    else:\n",
    "        os.environ[\"LANGSMITH_TRACING\"] = \"false\"\n",
    "        os.environ[\"LANGCHAIN_TRACING_V2\"] = \"false\"\n",
    "        print(\"âš ï¸ LangSmith skipped - no monitoring/tracing\")\n",
    "else:\n",
    "    print(\"âœ… LangSmith API key already set\")\n",
    "\n",
    "print(\"\\nğŸ¯ API Key Setup Complete!\")\n",
    "print(\"\\nğŸ“‹ Where to get API keys:\")\n",
    "print(\"â€¢ OpenAI: https://platform.openai.com/api-keys\")\n",
    "print(\"â€¢ Tavily: https://tavily.com/ (free tier available)\")\n",
    "print(\"â€¢ SerpAPI: https://serpapi.com/ (free tier available)\")\n",
    "print(\"â€¢ LangSmith: https://smith.langchain.com/ (optional monitoring)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ed4a4e",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 2: Data Loading and Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7219d64f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ… Loading Rotten Tomatoes movie review datasets...\n",
      "Loading Rotten Tomatoes movies metadata...\n",
      "  Trying encoding: utf-8\n",
      "  âœ… Success with utf-8\n",
      "Movies dataset: 143258 movies\n",
      "Columns: ['id', 'title', 'audienceScore', 'tomatoMeter', 'rating', 'ratingContents', 'releaseDateTheaters', 'releaseDateStreaming', 'runtimeMinutes', 'genre', 'originalLanguage', 'director', 'writer', 'boxOffice', 'distributor', 'soundMix']\n",
      "\n",
      "Loading Rotten Tomatoes reviews...\n",
      "  Trying encoding: utf-8\n",
      "  âœ… Success with utf-8\n",
      "Reviews dataset: 1444963 reviews\n",
      "Columns: ['id', 'reviewId', 'creationDate', 'criticName', 'isTopCritic', 'originalScore', 'reviewState', 'publicatioName', 'reviewText', 'scoreSentiment', 'reviewUrl']\n",
      "\n",
      "ğŸ¬ Sample movies metadata:\n",
      "                   id                title  audienceScore  tomatoMeter rating  \\\n",
      "0  space-zombie-bingo  Space Zombie Bingo!           50.0          NaN    NaN   \n",
      "1     the_green_grass      The Green Grass            NaN          NaN    NaN   \n",
      "2           love_lies           Love, Lies           43.0          NaN    NaN   \n",
      "\n",
      "  ratingContents releaseDateTheaters releaseDateStreaming  runtimeMinutes  \\\n",
      "0            NaN                 NaN           2018-08-25            75.0   \n",
      "1            NaN                 NaN           2020-02-11           114.0   \n",
      "2            NaN                 NaN                  NaN           120.0   \n",
      "\n",
      "                    genre originalLanguage                       director  \\\n",
      "0  Comedy, Horror, Sci-fi          English                  George Ormrod   \n",
      "1                   Drama          English                Tiffany Edwards   \n",
      "2                   Drama           Korean  Park Heung-Sik,Heung-Sik Park   \n",
      "\n",
      "                                   writer boxOffice distributor soundMix  \n",
      "0              George Ormrod,John Sabotta       NaN         NaN      NaN  \n",
      "1                         Tiffany Edwards       NaN         NaN      NaN  \n",
      "2  Ha Young-Joon,Jeon Yun-su,Song Hye-jin       NaN         NaN      NaN  \n",
      "\n",
      "ğŸ“ Sample reviews:\n",
      "                                  id  reviewId creationDate       criticName  \\\n",
      "0                            beavers   1145982   2003-05-23  Ivan M. Lincoln   \n",
      "1                         blood_mask   1636744   2007-06-02    The Foywonder   \n",
      "2  city_hunter_shinjuku_private_eyes   2590987   2019-05-28     Reuben Baron   \n",
      "\n",
      "   isTopCritic originalScore reviewState                 publicatioName  \\\n",
      "0        False         3.5/4       fresh  Deseret News (Salt Lake City)   \n",
      "1        False           1/5      rotten                  Dread Central   \n",
      "2        False           NaN       fresh                            CBR   \n",
      "\n",
      "                                          reviewText scoreSentiment  \\\n",
      "0  Timed to be just long enough for most youngste...       POSITIVE   \n",
      "1  It doesn't matter if a movie costs 300 million...       NEGATIVE   \n",
      "2  The choreography is so precise and lifelike at...       POSITIVE   \n",
      "\n",
      "                                           reviewUrl  \n",
      "0  http://www.deseretnews.com/article/700003233/B...  \n",
      "1  http://www.dreadcentral.com/index.php?name=Rev...  \n",
      "2  https://www.cbr.com/city-hunter-shinjuku-priva...  \n",
      "\n",
      "ğŸ“Š Dataset Statistics:\n",
      "â€¢ Total movies: 143,258\n",
      "â€¢ Total reviews: 1,444,963\n",
      "â€¢ Average reviews per movie: 10.1\n",
      "â€¢ Unique movie IDs in reviews: 69,263\n",
      "â€¢ Movies with reviews: 69,263 / 143,258\n",
      "\n",
      "ğŸ† Review State Distribution:\n",
      "reviewState\n",
      "fresh     963799\n",
      "rotten    481164\n",
      "Name: count, dtype: int64\n",
      "\n",
      "â­ Score Sentiment Distribution:\n",
      "scoreSentiment\n",
      "POSITIVE    963799\n",
      "NEGATIVE    481164\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load the Rotten Tomatoes datasets\n",
    "print(\"ğŸ… Loading Rotten Tomatoes movie review datasets...\")\n",
    "\n",
    "# Robust CSV loading function with error handling\n",
    "def load_csv_robust(filepath):\n",
    "    \"\"\"Load CSV with robust error handling for malformed data\"\"\"\n",
    "    encodings = ['utf-8', 'latin1', 'cp1252', 'iso-8859-1']\n",
    "    \n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            print(f\"  Trying encoding: {encoding}\")\n",
    "            # Try with error handling for malformed lines\n",
    "            df = pd.read_csv(\n",
    "                filepath, \n",
    "                encoding=encoding,\n",
    "                on_bad_lines='skip',  # Skip bad lines instead of failing\n",
    "                engine='python',      # Use Python engine for better error handling\n",
    "                quoting=1,           # Quote all fields\n",
    "                skipinitialspace=True\n",
    "            )\n",
    "            print(f\"  âœ… Success with {encoding}\")\n",
    "            return df\n",
    "        except UnicodeDecodeError:\n",
    "            print(f\"  âŒ Failed with {encoding}\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ Failed with {encoding}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # If all encodings fail, try with minimal options\n",
    "    print(\"  Trying with basic fallback...\")\n",
    "    try:\n",
    "        df = pd.read_csv(filepath, encoding='latin1', on_bad_lines='skip', engine='python')\n",
    "        print(\"  âœ… Success with fallback method\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Could not read {filepath}: {str(e)}\")\n",
    "\n",
    "# Load Rotten Tomatoes movies metadata\n",
    "print(\"Loading Rotten Tomatoes movies metadata...\")\n",
    "movies_df = load_csv_robust(\"data/rotten_tomatoes_movies.csv\")\n",
    "print(f\"Movies dataset: {len(movies_df)} movies\")\n",
    "print(f\"Columns: {list(movies_df.columns)}\")\n",
    "\n",
    "# Load Rotten Tomatoes reviews\n",
    "print(\"\\nLoading Rotten Tomatoes reviews...\")\n",
    "reviews_df = load_csv_robust(\"data/rotten_tomatoes_movie_reviews.csv\")\n",
    "print(f\"Reviews dataset: {len(reviews_df)} reviews\")\n",
    "print(f\"Columns: {list(reviews_df.columns)}\")\n",
    "\n",
    "# Display sample data\n",
    "print(\"\\nğŸ¬ Sample movies metadata:\")\n",
    "print(movies_df.head(3))\n",
    "\n",
    "print(\"\\nğŸ“ Sample reviews:\")\n",
    "print(reviews_df.head(3))\n",
    "\n",
    "# Basic statistics\n",
    "print(f\"\\nğŸ“Š Dataset Statistics:\")\n",
    "print(f\"â€¢ Total movies: {len(movies_df):,}\")\n",
    "print(f\"â€¢ Total reviews: {len(reviews_df):,}\")\n",
    "print(f\"â€¢ Average reviews per movie: {len(reviews_df)/len(movies_df):.1f}\")\n",
    "print(f\"â€¢ Unique movie IDs in reviews: {reviews_df['id'].nunique():,}\")\n",
    "print(f\"â€¢ Movies with reviews: {reviews_df['id'].nunique():,} / {len(movies_df):,}\")\n",
    "\n",
    "# Check review distribution\n",
    "print(f\"\\nğŸ† Review State Distribution:\")\n",
    "if 'reviewState' in reviews_df.columns:\n",
    "    print(reviews_df['reviewState'].value_counts())\n",
    "\n",
    "print(f\"\\nâ­ Score Sentiment Distribution:\")\n",
    "if 'scoreSentiment' in reviews_df.columns:\n",
    "    print(reviews_df['scoreSentiment'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3739f952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”— Merging movies metadata with reviews...\n",
      "âœ… Cleaned reviews dataset: 1364909 reviews\n",
      "âœ… Merged dataset: 1388546 reviews with movie metadata\n",
      "âœ… Reviews with movie titles: 1383051 / 1388546\n",
      "âš ï¸ 5495 reviews missing movie titles (will use movie ID)\n",
      "\n",
      "ğŸ¬ Sample merged data:\n",
      "  title_clean criticName_clean  \\\n",
      "0     Beavers  Ivan M. Lincoln   \n",
      "1  Blood Mask    The Foywonder   \n",
      "\n",
      "                                    reviewText_clean rating  genre_clean  \n",
      "0  Timed to be just long enough for most youngste...    NaN  Documentary  \n",
      "1  It doesn't matter if a movie costs 300 million...    NaN               \n"
     ]
    }
   ],
   "source": [
    "# Data cleaning and preprocessing for Rotten Tomatoes data\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and normalize text data\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to string and clean\n",
    "    text = str(text).strip()\n",
    "    \n",
    "    # Remove special characters and normalize\n",
    "    import re\n",
    "    text = re.sub(r'[\\x00-\\x1f\\x7f-\\x9f]', '', text)  # Remove control characters\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Normalize whitespace\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# Clean movies data\n",
    "movies_df['title_clean'] = movies_df['title'].apply(clean_text)\n",
    "movies_df['genre_clean'] = movies_df['genre'].apply(clean_text)\n",
    "movies_df['director_clean'] = movies_df['director'].apply(clean_text)\n",
    "\n",
    "# Clean reviews data\n",
    "reviews_df['reviewText_clean'] = reviews_df['reviewText'].apply(clean_text)\n",
    "reviews_df['criticName_clean'] = reviews_df['criticName'].apply(clean_text)\n",
    "reviews_df['publicatioName_clean'] = reviews_df['publicatioName'].apply(clean_text)\n",
    "\n",
    "# Remove rows with empty review text\n",
    "reviews_df = reviews_df[reviews_df['reviewText_clean'].str.len() > 20].copy()\n",
    "\n",
    "# Merge movies and reviews for complete information\n",
    "print(\"ğŸ”— Merging movies metadata with reviews...\")\n",
    "merged_df = reviews_df.merge(movies_df, on='id', how='left')\n",
    "\n",
    "print(f\"âœ… Cleaned reviews dataset: {len(reviews_df)} reviews\")\n",
    "print(f\"âœ… Merged dataset: {len(merged_df)} reviews with movie metadata\")\n",
    "print(f\"âœ… Reviews with movie titles: {merged_df['title'].notna().sum()} / {len(merged_df)}\")\n",
    "\n",
    "# Handle missing titles\n",
    "missing_titles = merged_df['title'].isna().sum()\n",
    "if missing_titles > 0:\n",
    "    print(f\"âš ï¸ {missing_titles} reviews missing movie titles (will use movie ID)\")\n",
    "    merged_df['title_clean'] = merged_df['title_clean'].fillna(merged_df['id'])\n",
    "\n",
    "print(f\"\\nğŸ¬ Sample merged data:\")\n",
    "sample_cols = ['title_clean', 'criticName_clean', 'reviewText_clean', 'rating', 'genre_clean']\n",
    "available_cols = [col for col in sample_cols if col in merged_df.columns]\n",
    "print(merged_df[available_cols].head(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8bdea959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ… Creating review documents from Rotten Tomatoes data...\n",
      "ğŸ§ª Using sample of 200 reviews for testing...\n",
      "âœ… Created 200 total review documents\n",
      "   - Source: Rotten Tomatoes\n",
      "   - Reviews with movie metadata included\n",
      "\n",
      "ğŸ“„ Sample document:\n",
      "Movie: Beavers\n",
      "Genre: Documentary\n",
      "Director: Stephen Low\n",
      "Rating: nan\n",
      "Release Date: nan\n",
      "Critic: Ivan M. Lincoln\n",
      "Publication: Deseret News (Salt Lake City)\n",
      "Score: 3.5/4\n",
      "Review State: fresh\n",
      "Sentiment: POSITIVE\n",
      "Review: Timed to be just long enough for most youngsters' brief attention spans -- and it's pa...\n",
      "\n",
      "ğŸ·ï¸ Sample metadata:\n",
      "  source: rotten_tomatoes\n",
      "  movie_id: beavers\n",
      "  movie_title: Beavers\n",
      "  critic_name: Ivan M. Lincoln\n",
      "  publication: Deseret News (Salt Lake City)\n",
      "  review_date: 2003-05-23\n",
      "  original_score: 3.5/4\n",
      "  review_state: fresh\n",
      "\n",
      "ğŸ“Š Document Statistics:\n",
      "â€¢ Unique movies: 33\n",
      "â€¢ Unique critics: 178\n",
      "â€¢ Average content length: 339 characters\n"
     ]
    }
   ],
   "source": [
    "# Create unified data structure for processing Rotten Tomatoes data\n",
    "def create_review_documents(df, max_reviews=200):\n",
    "    \"\"\"Convert merged DataFrame to list of review documents\"\"\"\n",
    "    documents = []\n",
    "    \n",
    "    # Use a sample for initial testing\n",
    "    if len(df) > max_reviews:\n",
    "        print(f\"ğŸ§ª Using sample of {max_reviews} reviews for testing...\")\n",
    "        df_sample = df.head(max_reviews)\n",
    "    else:\n",
    "        df_sample = df\n",
    "    \n",
    "    for idx, row in df_sample.iterrows():\n",
    "        # Create comprehensive metadata\n",
    "        metadata = {\n",
    "            'source': 'rotten_tomatoes',\n",
    "            'movie_id': row.get('id', ''),\n",
    "            'movie_title': row.get('title_clean', row.get('id', 'Unknown')),\n",
    "            'critic_name': row.get('criticName_clean', 'Anonymous'),\n",
    "            'publication': row.get('publicatioName_clean', 'Unknown'),\n",
    "            'review_date': row.get('creationDate', 'Unknown'),\n",
    "            'original_score': row.get('originalScore', ''),\n",
    "            'review_state': row.get('reviewState', ''),\n",
    "            'sentiment': row.get('scoreSentiment', ''),\n",
    "            'is_top_critic': row.get('isTopCritic', False),\n",
    "            'genre': row.get('genre_clean', ''),\n",
    "            'director': row.get('director_clean', ''),\n",
    "            'rating': row.get('rating', ''),\n",
    "            'audience_score': row.get('audienceScore', ''),\n",
    "            'tomato_meter': row.get('tomatoMeter', ''),\n",
    "            'release_date': row.get('releaseDateTheaters', ''),\n",
    "            'runtime': row.get('runtimeMinutes', ''),\n",
    "            'index': idx\n",
    "        }\n",
    "        \n",
    "        # Create rich content for embedding\n",
    "        content = f\"Movie: {row.get('title_clean', row.get('id', 'Unknown'))}\\n\"\n",
    "        \n",
    "        # Add movie metadata\n",
    "        if row.get('genre_clean'):\n",
    "            content += f\"Genre: {row.get('genre_clean')}\\n\"\n",
    "        if row.get('director_clean'):\n",
    "            content += f\"Director: {row.get('director_clean')}\\n\"\n",
    "        if row.get('rating'):\n",
    "            content += f\"Rating: {row.get('rating')}\\n\"\n",
    "        if row.get('releaseDateTheaters'):\n",
    "            content += f\"Release Date: {row.get('releaseDateTheaters')}\\n\"\n",
    "        \n",
    "        # Add review information\n",
    "        content += f\"Critic: {row.get('criticName_clean', 'Anonymous')}\\n\"\n",
    "        if row.get('publicatioName_clean'):\n",
    "            content += f\"Publication: {row.get('publicatioName_clean')}\\n\"\n",
    "        if row.get('originalScore'):\n",
    "            content += f\"Score: {row.get('originalScore')}\\n\"\n",
    "        if row.get('reviewState'):\n",
    "            content += f\"Review State: {row.get('reviewState')}\\n\"\n",
    "        if row.get('scoreSentiment'):\n",
    "            content += f\"Sentiment: {row.get('scoreSentiment')}\\n\"\n",
    "        \n",
    "        # Add the main review text\n",
    "        review_text = row.get('reviewText_clean', '')\n",
    "        if review_text:\n",
    "            content += f\"Review: {review_text}\"\n",
    "        \n",
    "        documents.append({\n",
    "            'content': content,\n",
    "            'metadata': metadata\n",
    "        })\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# Create documents from merged Rotten Tomatoes data\n",
    "print(\"ğŸ… Creating review documents from Rotten Tomatoes data...\")\n",
    "all_documents = create_review_documents(merged_df, max_reviews=200)\n",
    "\n",
    "print(f\"âœ… Created {len(all_documents)} total review documents\")\n",
    "print(f\"   - Source: Rotten Tomatoes\")\n",
    "print(f\"   - Reviews with movie metadata included\")\n",
    "\n",
    "# Show sample document\n",
    "print(\"\\nğŸ“„ Sample document:\")\n",
    "print(all_documents[0]['content'][:300] + \"...\")\n",
    "\n",
    "# Show metadata sample\n",
    "print(\"\\nğŸ·ï¸ Sample metadata:\")\n",
    "sample_metadata = all_documents[0]['metadata']\n",
    "for key, value in list(sample_metadata.items())[:8]:  # Show first 8 metadata fields\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Basic statistics\n",
    "print(f\"\\nğŸ“Š Document Statistics:\")\n",
    "unique_movies = len(set([doc['metadata']['movie_title'] for doc in all_documents]))\n",
    "unique_critics = len(set([doc['metadata']['critic_name'] for doc in all_documents]))\n",
    "print(f\"â€¢ Unique movies: {unique_movies}\")\n",
    "print(f\"â€¢ Unique critics: {unique_critics}\")\n",
    "print(f\"â€¢ Average content length: {np.mean([len(doc['content']) for doc in all_documents]):.0f} characters\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58c93bd",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 3: Text Chunking and Embedding Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0053581d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”ª Using each review as a separate chunk...\n",
      "âœ… Created 200 chunks from 200 reviews\n",
      "   Each review is treated as a separate chunk for better semantic coherence\n",
      "ğŸ“ Maximum chunk length: 120 tokens\n",
      "ğŸ“ Average chunk length: 89 tokens\n",
      "\n",
      "ğŸ“„ Sample chunk:\n",
      "Movie: Beavers\n",
      "Genre: Documentary\n",
      "Director: Stephen Low\n",
      "Rating: nan\n",
      "Release Date: nan\n",
      "Critic: Ivan M. Lincoln\n",
      "Publication: Deseret News (Salt Lake City)\n",
      "Score: 3.5/4\n",
      "Review State: fresh\n",
      "Sentiment: POS...\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Token counting function\n",
    "def tiktoken_len(text):\n",
    "    tokens = tiktoken.encoding_for_model(\"gpt-4o\").encode(text)\n",
    "    return len(tokens)\n",
    "\n",
    "# Convert our documents to LangChain Document format - each review is already a chunk\n",
    "print(\"ğŸ”ª Using each review as a separate chunk...\")\n",
    "chunks = []\n",
    "for doc in all_documents:\n",
    "    langchain_doc = Document(\n",
    "        page_content=doc['content'],\n",
    "        metadata=doc['metadata']\n",
    "    )\n",
    "    chunks.append(langchain_doc)\n",
    "\n",
    "print(f\"âœ… Created {len(chunks)} chunks from {len(all_documents)} reviews\")\n",
    "print(\"   Each review is treated as a separate chunk for better semantic coherence\")\n",
    "\n",
    "# Verify chunk sizes\n",
    "chunk_lengths = [tiktoken_len(chunk.page_content) for chunk in chunks]\n",
    "max_chunk_length = max(chunk_lengths)\n",
    "avg_chunk_length = sum(chunk_lengths) / len(chunk_lengths)\n",
    "print(f\"ğŸ“ Maximum chunk length: {max_chunk_length} tokens\")\n",
    "print(f\"ğŸ“ Average chunk length: {avg_chunk_length:.0f} tokens\")\n",
    "\n",
    "# Show sample chunk\n",
    "print(\"\\nğŸ“„ Sample chunk:\")\n",
    "print(chunks[0].page_content[:200] + \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7031b9ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§  Initializing embedding model...\n",
      "ğŸ—„ï¸ Creating vector store...\n",
      "âœ… Vector store and retriever created successfully!\n",
      "\n",
      "ğŸ§ª Testing retrieval with sample query...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f1/cmsz4dgn2y194hgy1n_pldjc0000gn/T/ipykernel_57080/2848140654.py:24: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  test_results = retriever.get_relevant_documents(test_query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 relevant documents for: 'What do people think about Inception?'\n",
      "\n",
      "ğŸ“„ Sample retrieved content:\n",
      "Movie: La Sapienza\n",
      "Genre: Drama\n",
      "Director: EugÃ¨ne Green\n",
      "Rating: nan\n",
      "Release Date: nan\n",
      "Critic: Boyd van Hoeij\n",
      "Publication: Hollywood Reporter\n",
      "Score: nan\n",
      "Review State: fresh\n",
      "Sentiment: POSITIVE\n",
      "Review: The Sapience juxtaposes insights on how people are emotionally connected with ruminations on the buil...\n"
     ]
    }
   ],
   "source": [
    "# Initialize embedding model\n",
    "print(\"ğŸ§  Initializing embedding model...\")\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Create vector store\n",
    "print(\"ğŸ—„ï¸ Creating vector store...\")\n",
    "qdrant_vectorstore = Qdrant.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embedding_model,\n",
    "    location=\":memory:\"\n",
    ")\n",
    "\n",
    "# Create retriever\n",
    "retriever = qdrant_vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 5}\n",
    ")\n",
    "\n",
    "print(\"âœ… Vector store and retriever created successfully!\")\n",
    "\n",
    "# Test retrieval\n",
    "print(\"\\nğŸ§ª Testing retrieval with sample query...\")\n",
    "test_query = \"What do people think about Inception?\"\n",
    "test_results = retriever.get_relevant_documents(test_query)\n",
    "print(f\"Found {len(test_results)} relevant documents for: '{test_query}'\")\n",
    "print(\"\\nğŸ“„ Sample retrieved content:\")\n",
    "print(test_results[0].page_content[:300] + \"...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66f45f4",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 4: RAG System Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec80bc04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Prompt template and chat model initialized!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "# Define state structure\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    response: str\n",
    "\n",
    "# Create prompt template\n",
    "HUMAN_TEMPLATE = \"\"\"\n",
    "You are a knowledgeable movie critic and analyst. You have access to a database of movie reviews from both Letterboxd (social media reviews) and Metacritic (professional reviews).\n",
    "\n",
    "Use the provided context to answer the user's question about movies, reviews, ratings, and trends. Only use the information provided in the context. If the context doesn't contain relevant information to answer the question, respond with \"I don't have enough information to answer that question based on the available reviews.\"\n",
    "\n",
    "When analyzing reviews, consider:\n",
    "- Different perspectives between social media (Letterboxd) and professional (Metacritic) reviews\n",
    "- Rating patterns and trends\n",
    "- Common themes in reviews\n",
    "- Temporal patterns in movie releases and ratings\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "Provide a comprehensive and insightful answer based on the available review data.\n",
    "\"\"\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", HUMAN_TEMPLATE)\n",
    "])\n",
    "\n",
    "# Initialize chat model\n",
    "chat_model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.1)\n",
    "\n",
    "print(\"âœ… Prompt template and chat model initialized!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c832fc01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… RAG system built successfully!\n",
      "ğŸ“Š Graph structure created with retrieve â†’ generate sequence\n"
     ]
    }
   ],
   "source": [
    "# Define RAG functions\n",
    "def retrieve(state: State) -> State:\n",
    "    \"\"\"Retrieve relevant documents based on the question\"\"\"\n",
    "    retrieved_docs = retriever.get_relevant_documents(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "def generate(state: State) -> State:\n",
    "    \"\"\"Generate response based on retrieved context\"\"\"\n",
    "    generator_chain = chat_prompt | chat_model | StrOutputParser()\n",
    "    response = generator_chain.invoke({\n",
    "        \"question\": state[\"question\"], \n",
    "        \"context\": state[\"context\"]\n",
    "    })\n",
    "    return {\"response\": response}\n",
    "\n",
    "# Build the RAG graph\n",
    "graph_builder = StateGraph(State)\n",
    "graph_builder = graph_builder.add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "rag_graph = graph_builder.compile()\n",
    "\n",
    "print(\"âœ… RAG system built successfully!\")\n",
    "print(f\"ğŸ“Š Graph structure created with retrieve â†’ generate sequence\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2dbab5",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 5: Testing the RAG System\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb789e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª Testing RAG system with various queries...\n",
      "================================================================================\n",
      "\n",
      "ğŸ” Query 1: What do people think about Inception?\n",
      "------------------------------------------------------------\n",
      "ğŸ“ Response: I don't have enough information to answer that question based on the available reviews.\n",
      "ğŸ“š Retrieved from: ['rotten_tomatoes', 'rotten_tomatoes', 'rotten_tomatoes', 'rotten_tomatoes', 'rotten_tomatoes']\n",
      "================================================================================\n",
      "\n",
      "ğŸ” Query 2: What are the best rated movies according to the reviews?\n",
      "------------------------------------------------------------\n",
      "ğŸ“ Response: Based on the available review data, the best-rated movies can be assessed through the scores given by critics. Here are the notable films along with their ratings:\n",
      "\n",
      "1. **Paa**\n",
      "   - **Critic Score**: 3.5/5 (70%) from Nikhat Kazmi (The Times of India) - Positive review highlighting the film's second half as a strong point despite some vague sub-plots in the first half.\n",
      "   - **Critic Score**: 5.85/10 (58.5%) from Avi Offer (NYC Movie Guru) - Negative review pointing out issues with pacing and overall unevenness, despite praise for acting and makeup design.\n",
      "   - **Audience Score**: 67%\n",
      "   - **Tomato Meter**: 50%\n",
      "\n",
      "2. **Dangerous Men**\n",
      "   - **Critic Score**: 7/10 (70%) from Matt Donato (We Got This Covered) - Positive review that acknowledges the film's unique qualities despite its flaws.\n",
      "   - **Critic Score**: 3/10 (30%) from Jason Wilson (Under the Radar) - Negative review advising against the film unless one enjoys \"garbage cinema.\"\n",
      "   - **Audience Score**: 42%\n",
      "   - **Tomato Meter**: 50%\n",
      "\n",
      "3. **The Truth About Love**\n",
      "   - **Critic Score**: 1/5 (20%) from Peter Bradshaw (Guardian) - Negative review labeling it as one of the worst films of 2007.\n",
      "   - **Audience Score**: 41%\n",
      "   - **Tomato Meter**: 0%\n",
      "\n",
      "From this analysis, **Paa** and **Dangerous Men** stand out with the highest critic scores of 3.5/5 and 7/10, respectively. However, both films also received mixed reviews, indicating a divide in critical reception. **Paa** has a higher audience score compared to **Dangerous Men**, suggesting that viewers may have enjoyed it more than critics did.\n",
      "\n",
      "In summary, while **Paa** and **Dangerous Men** are the best-rated films based on the critic scores, the mixed nature of the reviews indicates that audience reception can vary significantly from critical perspectives.\n",
      "ğŸ“š Retrieved from: ['rotten_tomatoes', 'rotten_tomatoes', 'rotten_tomatoes', 'rotten_tomatoes', 'rotten_tomatoes']\n",
      "================================================================================\n",
      "\n",
      "ğŸ” Query 3: What are some common themes in movie reviews?\n",
      "------------------------------------------------------------\n",
      "ğŸ“ Response: Based on the available review data, several common themes can be identified across the different movies reviewed:\n",
      "\n",
      "1. **Mixed Sentiments**: Many reviews reflect a duality in sentiment, where critics acknowledge both positive and negative aspects of a film. For instance, Amber Wilkinson's review of \"Stay Cool\" notes that while there are admirable qualities, there is also a lack of genuine likability. This theme of ambivalence suggests that films can have redeeming features while still falling short in execution or engagement.\n",
      "\n",
      "2. **Formalism vs. Emotional Engagement**: In the reviews for \"La Sapienza,\" there is a clear tension between formalism and emotional depth. Wesley Morris criticizes the film for being overly formal and not creating a meaningful cinematic experience, while Ignatiy Vishnevetsky praises the emotional investment of the characters in art. This highlights a common critique in film reviews where the balance between aesthetic form and emotional resonance is a focal point.\n",
      "\n",
      "3. **Audience Reception vs. Critical Acclaim**: The disparity between audience scores and critic reviews is evident. For example, \"La Sapienza\" has an audience score of 50% but a tomato meter of 88%, indicating that while critics may appreciate the film's artistic qualities, general audiences may not connect with it as strongly. This theme underscores the difference in perspectives between professional critics and the viewing public.\n",
      "\n",
      "4. **Humor and Desperation**: In the review of \"Salty,\" the sentiment of desperation for laughs is highlighted, suggesting that some comedies may struggle to find their footing and resort to frantic attempts at humor. This theme points to a broader concern in comedy films where the balance between genuine humor and forced attempts can significantly affect the film's reception.\n",
      "\n",
      "5. **Poignancy and Lasting Impact**: Tara McNamara's review of \"Small Town Wisconsin\" emphasizes the poignancy of the story, suggesting that despite its flaws, the emotional weight of the narrative can leave a lasting impression on viewers. This theme reflects a common appreciation for films that resonate on a deeper emotional level, even if they are not technically perfect.\n",
      "\n",
      "Overall, these themes illustrate the complexity of film reviews, where critics navigate a spectrum of emotional engagement, artistic expression, audience reception, and the effectiveness of humor.\n",
      "ğŸ“š Retrieved from: ['rotten_tomatoes', 'rotten_tomatoes', 'rotten_tomatoes', 'rotten_tomatoes', 'rotten_tomatoes']\n",
      "================================================================================\n",
      "\n",
      "ğŸ” Query 4: What movies have the highest ratings?\n",
      "------------------------------------------------------------\n",
      "ğŸ“ Response: Based on the available review data, the movies with the highest ratings are as follows:\n",
      "\n",
      "1. **Born to Kill**\n",
      "   - **Rating**: 10/10\n",
      "   - **Critic**: Mike Massie (Gone With The Twins)\n",
      "   - **Review Sentiment**: Positive\n",
      "   - **Tomato Meter**: 83.0\n",
      "   - **Audience Score**: 74.0\n",
      "   - **Review Insight**: The film is described as one of the most acerbic films noir, featuring no redeemable characters and a wealth of deliciously evil villains, making it utterly enthralling.\n",
      "\n",
      "2. **Dangerous Men**\n",
      "   - **Rating**: 4/5\n",
      "   - **Critic**: Eric Melin (Lawrence.com)\n",
      "   - **Review Sentiment**: Positive\n",
      "   - **Tomato Meter**: 50.0\n",
      "   - **Audience Score**: 42.0\n",
      "   - **Review Insight**: The film is noted for its head-scratching choices that elicit amazing out-loud responses, making it a true party flick.\n",
      "\n",
      "3. **Paa**\n",
      "   - **Rating**: 3.5/5\n",
      "   - **Critic**: Nikhat Kazmi (The Times of India)\n",
      "   - **Review Sentiment**: Positive\n",
      "   - **Tomato Meter**: 50.0\n",
      "   - **Audience Score**: 67.0\n",
      "   - **Review Insight**: The film starts with vague sub-plots but picks up significantly in the second half, showcasing a positive turnaround.\n",
      "\n",
      "4. **Sometimes a Great Notion**\n",
      "   - **Rating**: B- (from Dennis Schwartz)\n",
      "   - **Critic**: Dennis Schwartz (Dennis Schwartz Movie Reviews)\n",
      "   - **Review Sentiment**: Positive\n",
      "   - **Tomato Meter**: 100.0\n",
      "   - **Audience Score**: 77.0\n",
      "   - **Review Insight**: The film is praised for its depiction of macho men at work, which adds a buzz to the family drama.\n",
      "\n",
      "5. **Sometimes a Great Notion**\n",
      "   - **Rating**: Not specified (but positively reviewed by Quentin Tarantino)\n",
      "   - **Critic**: Quentin Tarantino (The New Beverly)\n",
      "   - **Review Sentiment**: Positive\n",
      "   - **Tomato Meter**: 100.0\n",
      "   - **Audience Score**: 77.0\n",
      "   - **Review Insight**: The film is recognized for one of the greatest scenes in early seventies cinema.\n",
      "\n",
      "In summary, \"Born to Kill\" stands out with a perfect score of 10/10, while \"Sometimes a Great Notion\" has a high tomato meter of 100.0, indicating strong critical acclaim. The reviews reflect a mix of sentiments, with all films receiving positive feedback, highlighting their unique qualities and contributions to cinema.\n",
      "ğŸ“š Retrieved from: ['rotten_tomatoes', 'rotten_tomatoes', 'rotten_tomatoes', 'rotten_tomatoes', 'rotten_tomatoes']\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Test queries\n",
    "test_queries = [\n",
    "    \"What do people think about Inception?\",\n",
    "    \"What are the best rated movies according to the reviews?\",\n",
    "    \"What are some common themes in movie reviews?\",\n",
    "    \"What movies have the highest ratings?\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ§ª Testing RAG system with various queries...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"\\nğŸ” Query {i}: {query}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    try:\n",
    "        response = rag_graph.invoke({\"question\": query})\n",
    "        print(f\"ğŸ“ Response: {response['response']}\")\n",
    "        \n",
    "        # Show retrieved context info\n",
    "        context_sources = [doc.metadata.get('source', 'unknown') for doc in response['context']]\n",
    "        print(f\"ğŸ“š Retrieved from: {context_sources}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error: {str(e)}\")\n",
    "    \n",
    "    print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 6: Interactive Query Interface\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8418074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª Testing the query function...\n",
      "Question: What do people think about The Dark Knight?\n",
      "Answer: I don't have enough information to answer that question based on the available reviews....\n",
      "Sources: 5 documents found\n",
      "\n",
      "âœ… Query function is working correctly!\n"
     ]
    }
   ],
   "source": [
    "def query_movie_reviews(question: str) -> Dict[str, Any]:\n",
    "    \"\"\"Query the movie reviews RAG system\"\"\"\n",
    "    try:\n",
    "        response = rag_graph.invoke({\"question\": question})\n",
    "        \n",
    "        # Extract metadata from retrieved documents\n",
    "        sources = []\n",
    "        for doc in response['context']:\n",
    "            source_info = {\n",
    "                'source': doc.metadata.get('source', 'unknown'),\n",
    "                'movie': doc.metadata.get('movie_name', 'Unknown'),\n",
    "                'rating': doc.metadata.get('rating', 'N/A'),\n",
    "                'reviewer': doc.metadata.get('reviewer', 'Anonymous')\n",
    "            }\n",
    "            sources.append(source_info)\n",
    "        \n",
    "        return {\n",
    "            'question': question,\n",
    "            'answer': response['response'],\n",
    "            'sources': sources,\n",
    "            'num_sources': len(sources)\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'question': question,\n",
    "            'answer': f\"Error processing query: {str(e)}\",\n",
    "            'sources': [],\n",
    "            'num_sources': 0\n",
    "        }\n",
    "\n",
    "# Test the query function\n",
    "print(\"ğŸ§ª Testing the query function...\")\n",
    "sample_result = query_movie_reviews(\"What do people think about The Dark Knight?\")\n",
    "print(f\"Question: {sample_result['question']}\")\n",
    "print(f\"Answer: {sample_result['answer'][:200]}...\")\n",
    "print(f\"Sources: {sample_result['num_sources']} documents found\")\n",
    "\n",
    "print(\"\\nâœ… Query function is working correctly!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d8df584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Interactive query interface ready!\n",
      "ğŸ’¡ Uncomment the interactive_query() line above to start interactive mode.\n"
     ]
    }
   ],
   "source": [
    "# Interactive query function\n",
    "def interactive_query():\n",
    "    \"\"\"Interactive query interface\"\"\"\n",
    "    print(\"ğŸ¬ Movie Reviews RAG System\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Ask questions about movies, reviews, ratings, and trends!\")\n",
    "    print(\"Type 'quit' to exit.\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            question = input(\"\\nğŸ¤” Your question: \")\n",
    "            \n",
    "            if question.lower() in ['quit', 'exit', 'q']:\n",
    "                print(\"ğŸ‘‹ Goodbye!\")\n",
    "                break\n",
    "            \n",
    "            if not question.strip():\n",
    "                continue\n",
    "            \n",
    "            print(\"\\nğŸ” Searching for relevant reviews...\")\n",
    "            result = query_movie_reviews(question)\n",
    "            \n",
    "            print(f\"\\nğŸ“ Answer: {result['answer']}\")\n",
    "            print(f\"\\nğŸ“š Sources ({result['num_sources']}):\")\n",
    "            \n",
    "            for i, source in enumerate(result['sources'], 1):\n",
    "                print(f\"  {i}. {source['movie']} ({source['source']}) - Rating: {source['rating']}\")\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nğŸ‘‹ Goodbye!\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"\\nâŒ Error: {str(e)}\")\n",
    "\n",
    "# You can uncomment the line below to start interactive mode\n",
    "# interactive_query()\n",
    "\n",
    "print(\"âœ… Interactive query interface ready!\")\n",
    "print(\"ğŸ’¡ Uncomment the interactive_query() line above to start interactive mode.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94587ed2",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 7: System Summary and Next Steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3476c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ… Rotten Tomatoes RAG System Summary\n",
      "==================================================\n",
      "ğŸ“Š Total reviews processed: 200\n",
      "   - Rotten Tomatoes Professional Critics: 200 reviews\n",
      "ğŸ”ª Text chunks created: 200\n",
      "ğŸ§  Embedding model: text-embedding-3-small\n",
      "ğŸ’¬ Chat model: gpt-4o-mini\n",
      "ğŸ—„ï¸ Vector store: Qdrant (in-memory)\n",
      "ğŸ” Retrieval method: Similarity search (k=5)\n",
      "==================================================\n",
      "\n",
      "ğŸš€ Next Steps:\n",
      "1. âœ… Add agentic features with multiple tools\n",
      "2. âœ… Implement advanced analytics and trend analysis\n",
      "3. Add evaluation framework with RAGAS\n",
      "4. Add LangSmith tracing and monitoring\n",
      "5. Create visualization capabilities\n",
      "6. Deploy as a web application\n",
      "\n",
      "âœ… Phase 1 Complete: Rotten Tomatoes RAG system is ready for queries!\n",
      "\n",
      "ğŸ’¡ Example usage:\n",
      "result = query_movie_reviews('What do critics think about sci-fi movies?')\n",
      "print(result['answer'])\n",
      "\n",
      "ğŸ“ˆ Rotten Tomatoes Data Statistics:\n",
      "   - Total movies in dataset: 143,258\n",
      "   - Total reviews in dataset: 1,364,909\n",
      "   - Reviews in current sample: 1,388,546\n",
      "   - Unique movies with reviews: 61,764\n",
      "   - Average review length: 132 characters\n",
      "   - Fresh reviews: 932,141 (67.1%)\n",
      "   - Rotten reviews: 456,405\n",
      "   - Reviews from Top Critics: 430,690\n",
      "\n",
      "ğŸ¬ Data Quality:\n",
      "   - Professional critic reviews with detailed metadata\n",
      "   - Official Rotten Tomatoes scores (Tomatometer & Audience)\n",
      "   - Rich movie information (genre, director, runtime, release date)\n",
      "   - Publication sources and critic credentials\n"
     ]
    }
   ],
   "source": [
    "# System summary\n",
    "print(\"ğŸ… Rotten Tomatoes RAG System Summary\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"ğŸ“Š Total reviews processed: {len(all_documents)}\")\n",
    "print(f\"   - Rotten Tomatoes Professional Critics: {len(all_documents)} reviews\")\n",
    "print(f\"ğŸ”ª Text chunks created: {len(chunks)}\")\n",
    "print(f\"ğŸ§  Embedding model: text-embedding-3-small\")\n",
    "print(f\"ğŸ’¬ Chat model: gpt-4o-mini\")\n",
    "print(f\"ğŸ—„ï¸ Vector store: Qdrant (in-memory)\")\n",
    "print(f\"ğŸ” Retrieval method: Similarity search (k=5)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\nğŸš€ Next Steps:\")\n",
    "print(\"1. âœ… Add agentic features with multiple tools\")\n",
    "print(\"2. âœ… Implement advanced analytics and trend analysis\")\n",
    "print(\"3. Add evaluation framework with RAGAS\")\n",
    "print(\"4. Add LangSmith tracing and monitoring\")\n",
    "print(\"5. Create visualization capabilities\")\n",
    "print(\"6. Deploy as a web application\")\n",
    "\n",
    "print(\"\\nâœ… Phase 1 Complete: Rotten Tomatoes RAG system is ready for queries!\")\n",
    "\n",
    "# Example of how to use the system\n",
    "print(\"\\nğŸ’¡ Example usage:\")\n",
    "print(\"result = query_movie_reviews('What do critics think about sci-fi movies?')\")\n",
    "print(\"print(result['answer'])\")\n",
    "\n",
    "# Data statistics for Rotten Tomatoes\n",
    "print(f\"\\nğŸ“ˆ Rotten Tomatoes Data Statistics:\")\n",
    "print(f\"   - Total movies in dataset: {len(movies_df):,}\")\n",
    "print(f\"   - Total reviews in dataset: {len(reviews_df):,}\")\n",
    "print(f\"   - Reviews in current sample: {len(merged_df):,}\")\n",
    "print(f\"   - Unique movies with reviews: {merged_df['title_clean'].nunique():,}\")\n",
    "\n",
    "# Review quality statistics\n",
    "if len(merged_df) > 0:\n",
    "    avg_review_length = merged_df['reviewText_clean'].str.len().mean()\n",
    "    print(f\"   - Average review length: {avg_review_length:.0f} characters\")\n",
    "    \n",
    "    # Fresh vs Rotten breakdown\n",
    "    if 'reviewState' in merged_df.columns:\n",
    "        fresh_count = (merged_df['reviewState'] == 'fresh').sum()\n",
    "        rotten_count = (merged_df['reviewState'] == 'rotten').sum()\n",
    "        fresh_percentage = (fresh_count / len(merged_df)) * 100 if len(merged_df) > 0 else 0\n",
    "        print(f\"   - Fresh reviews: {fresh_count:,} ({fresh_percentage:.1f}%)\")\n",
    "        print(f\"   - Rotten reviews: {rotten_count:,}\")\n",
    "    \n",
    "    # Top critics\n",
    "    if 'isTopCritic' in merged_df.columns:\n",
    "        top_critics = (merged_df['isTopCritic'] == True).sum()\n",
    "        print(f\"   - Reviews from Top Critics: {top_critics:,}\")\n",
    "\n",
    "print(f\"\\nğŸ¬ Data Quality:\")\n",
    "print(f\"   - Professional critic reviews with detailed metadata\")\n",
    "print(f\"   - Official Rotten Tomatoes scores (Tomatometer & Audience)\")\n",
    "print(f\"   - Rich movie information (genre, director, runtime, release date)\")\n",
    "print(f\"   - Publication sources and critic credentials\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11aaf57c",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Phase 2: Agentic Enhancement with Multiple Tools\n",
    "\n",
    "Now we'll enhance our RAG system with multiple specialized tools, including external search capabilities for when our embedded review data isn't sufficient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d8c584",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Step 8: External Search Tool Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "eb7ea36f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ Setting up external search tools...\n",
      "âœ… Tavily search tool configured\n",
      "âœ… SerpAPI search tool configured\n",
      "ğŸ” Using Tavily for external search\n",
      "\n",
      "ğŸ§ª Testing Tavily search...\n",
      "âœ… Search test successful: Found 3 results\n"
     ]
    }
   ],
   "source": [
    "# External search tools setup\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_community.utilities import SerpAPIWrapper\n",
    "from langchain.tools import Tool\n",
    "from langchain_core.tools import tool\n",
    "import json\n",
    "\n",
    "# Setup external search tools (you'll need API keys for these)\n",
    "print(\"ğŸ”§ Setting up external search tools...\")\n",
    "\n",
    "# Option 1: Tavily Search (recommended - often has free tier)\n",
    "try:\n",
    "    # You'll need to set TAVILY_API_KEY in your environment\n",
    "    # Get free API key from: https://tavily.com/\n",
    "    tavily_search = TavilySearchResults(\n",
    "        max_results=3,\n",
    "        search_depth=\"basic\",\n",
    "        include_answer=True,\n",
    "        include_raw_content=True\n",
    "    )\n",
    "    print(\"âœ… Tavily search tool configured\")\n",
    "    has_tavily = True\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Tavily not configured: {e}\")\n",
    "    has_tavily = False\n",
    "\n",
    "# Option 2: SerpAPI (Google Search) - backup option\n",
    "try:\n",
    "    # You'll need to set SERPAPI_API_KEY in your environment\n",
    "    # Get free API key from: https://serpapi.com/\n",
    "    search = SerpAPIWrapper()\n",
    "    serp_tool = Tool(\n",
    "        name=\"google_search\",\n",
    "        description=\"Search Google for current information about movies, actors, reviews, or box office data\",\n",
    "        func=search.run,\n",
    "    )\n",
    "    print(\"âœ… SerpAPI search tool configured\")\n",
    "    has_serp = True\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ SerpAPI not configured: {e}\")\n",
    "    has_serp = False\n",
    "\n",
    "# Create a fallback search function if no external APIs are configured\n",
    "def fallback_search(query: str) -> str:\n",
    "    \"\"\"Fallback search when no external APIs are available\"\"\"\n",
    "    return f\"External search not available. Query '{query}' would require external movie database access. Please configure Tavily API key (https://tavily.com/) or SerpAPI key (https://serpapi.com/) for enhanced search capabilities.\"\n",
    "\n",
    "# Choose which search tool to use\n",
    "if has_tavily:\n",
    "    external_search_tool = tavily_search\n",
    "    search_tool_name = \"Tavily\"\n",
    "elif has_serp:\n",
    "    external_search_tool = serp_tool\n",
    "    search_tool_name = \"SerpAPI\"\n",
    "else:\n",
    "    external_search_tool = Tool(\n",
    "        name=\"fallback_search\",\n",
    "        description=\"Fallback search tool when external APIs are not configured\",\n",
    "        func=fallback_search\n",
    "    )\n",
    "    search_tool_name = \"Fallback\"\n",
    "\n",
    "print(f\"ğŸ” Using {search_tool_name} for external search\")\n",
    "\n",
    "# Test the search tool\n",
    "print(f\"\\nğŸ§ª Testing {search_tool_name} search...\")\n",
    "try:\n",
    "    if has_tavily:\n",
    "        test_result = external_search_tool.invoke({\"query\": \"Inception movie reviews 2010\"})\n",
    "        print(f\"âœ… Search test successful: Found {len(test_result)} results\")\n",
    "    elif has_serp:\n",
    "        test_result = external_search_tool.run(\"Inception movie reviews 2010\")\n",
    "        print(f\"âœ… Search test successful: {test_result[:100]}...\")\n",
    "    else:\n",
    "        test_result = external_search_tool.run(\"Inception movie reviews 2010\")\n",
    "        print(f\"âš ï¸ Using fallback search: {test_result}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Search test failed: {e}\")\n",
    "    print(\"ğŸ’¡ You can continue without external search - the agent will use only embedded reviews\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a23f720",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Step 9: Specialized Agent Tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b19798a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ› ï¸ Creating specialized agent tools for Rotten Tomatoes data...\n",
      "âœ… Created 4 specialized tools for Rotten Tomatoes:\n",
      "  - search_movie_reviews: Search through embedded movie reviews from Rotten Tomatoes.\n",
      "Use this for questions about specific movies, ratings, or review content.\n",
      "  - analyze_movie_statistics: Analyze statistics for a specific movie or provide general Rotten Tomatoes dataset statistics.\n",
      "Returns ratings, review counts, critic information, and other numerical insights.\n",
      "  - analyze_movie_ratings: Analyze ratings and review sentiment for a specific movie from Rotten Tomatoes.\n",
      "Shows audience score, tomatometer, critic consensus, and sentiment analysis.\n",
      "  - search_external_movie_info: Search external sites (IMDb, Metacritic, Letterboxd, Rotten Tomatoes, etc.)\n",
      "for reviews, ratings, or recent news about a movie.\n",
      "\n",
      "â€¢ Uses Tavily if available, SerpAPI next, then falls back to the tool's `.run`.\n",
      "â€¢ Returns the first three snippets with sources.\n",
      "\n",
      "ğŸ§ª Testing agent tools...\n",
      "Testing movie review search:\n",
      "âœ… Review search: I don't have enough information to answer that question based on the available reviews....\n",
      "\n",
      "Testing statistics analysis:\n",
      "âœ… Statistics: ğŸ… Rotten Tomatoes Dataset Statistics:\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "ğŸ“Š Overview:\n",
      "â€¢ Total Movies: 143,258\n",
      "â€¢ Total Reviews: 1,364,909\n",
      "â€¢ Reviews in Current Sample: 1,388,546\n",
      "â€¢ Average Reviews per Mo...\n",
      "\n",
      "Testing movie ratings analysis:\n",
      "âœ… Ratings analysis: ğŸ… Rotten Tomatoes Analysis for 'Inception':\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "ğŸ… Tomatometer: 87.0% (Critics)\n",
      "ğŸ¿ Audience Score: 91.0%\n",
      "\n",
      "ğŸ“Š Review Breakdown:\n",
      "â€¢ Fresh Reviews: 315\n",
      "â€¢ Rotten Reviews: 49\n",
      "â€¢ ...\n",
      "\n",
      "Testing external movie info search:\n",
      "âœ… External search: Source: https://www.instagram.com/p/DF1q50kRk0n/?hl=en\n",
      "... imdb #rottentomatoes #letterboxd #generational #superhero. more. View ... Here's how each season has been rated by fans on IMDb so far.â€¦\n",
      "\n",
      "Sou...\n",
      "\n",
      "âœ… All Rotten Tomatoes agent tools ready!\n"
     ]
    }
   ],
   "source": [
    "# Create specialized tools for Rotten Tomatoes movie analysis\n",
    "print(\"ğŸ› ï¸ Creating specialized agent tools for Rotten Tomatoes data...\")\n",
    "\n",
    "# Tool 1: Movie Review Search (our existing RAG)\n",
    "@tool\n",
    "def search_movie_reviews(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Search through embedded movie reviews from Rotten Tomatoes.\n",
    "    Use this for questions about specific movies, ratings, or review content.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = rag_graph.invoke({\"question\": query})\n",
    "        return response['response']\n",
    "    except Exception as e:\n",
    "        return f\"Error searching reviews: {str(e)}\"\n",
    "\n",
    "# Tool 2: Movie Statistics Analysis\n",
    "@tool\n",
    "def analyze_movie_statistics(movie_name: str = \"\") -> str:\n",
    "    \"\"\"\n",
    "    Analyze statistics for a specific movie or provide general Rotten Tomatoes dataset statistics.\n",
    "    Returns ratings, review counts, critic information, and other numerical insights.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if movie_name:\n",
    "            # Search for specific movie in the merged dataset\n",
    "            movie_data = merged_df[\n",
    "                merged_df['title_clean'].str.contains(movie_name, case=False, na=False)\n",
    "            ]\n",
    "            \n",
    "            if movie_data.empty:\n",
    "                return f\"No statistics found for '{movie_name}' in the Rotten Tomatoes dataset.\"\n",
    "            \n",
    "            # Get movie information\n",
    "            movie_info = movie_data.iloc[0]  # Get first match for movie metadata\n",
    "            movie_reviews = movie_data  # All reviews for this movie\n",
    "            \n",
    "            stats = f\"Statistics for '{movie_info.get('title_clean', movie_name)}':\\n\"\n",
    "            stats += f\"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\\n\"\n",
    "            \n",
    "            # Movie metadata\n",
    "            if movie_info.get('genre_clean'):\n",
    "                stats += f\"ğŸ­ Genre: {movie_info['genre_clean']}\\n\"\n",
    "            if movie_info.get('director_clean'):\n",
    "                stats += f\"ğŸ¬ Director: {movie_info['director_clean']}\\n\"\n",
    "            if movie_info.get('rating'):\n",
    "                stats += f\"ğŸ·ï¸ Rating: {movie_info['rating']}\\n\"\n",
    "            if movie_info.get('runtimeMinutes'):\n",
    "                stats += f\"â±ï¸ Runtime: {movie_info['runtimeMinutes']} minutes\\n\"\n",
    "            if movie_info.get('releaseDateTheaters'):\n",
    "                stats += f\"ğŸ“… Release Date: {movie_info['releaseDateTheaters']}\\n\"\n",
    "            \n",
    "            # Scores\n",
    "            if pd.notna(movie_info.get('audienceScore')):\n",
    "                stats += f\"ğŸ‘¥ Audience Score: {movie_info['audienceScore']}%\\n\"\n",
    "            if pd.notna(movie_info.get('tomatoMeter')):\n",
    "                stats += f\"ğŸ… Tomatometer: {movie_info['tomatoMeter']}%\\n\"\n",
    "            \n",
    "            # Review statistics\n",
    "            stats += f\"\\nğŸ“Š Review Analysis:\\n\"\n",
    "            stats += f\"â€¢ Total Reviews: {len(movie_reviews)}\\n\"\n",
    "            \n",
    "            # Review state distribution\n",
    "            if 'reviewState' in movie_reviews.columns:\n",
    "                review_states = movie_reviews['reviewState'].value_counts()\n",
    "                for state, count in review_states.items():\n",
    "                    stats += f\"â€¢ {state.title()}: {count} reviews\\n\"\n",
    "            \n",
    "            # Sentiment distribution\n",
    "            if 'scoreSentiment' in movie_reviews.columns:\n",
    "                sentiments = movie_reviews['scoreSentiment'].value_counts()\n",
    "                stats += f\"\\nğŸ­ Sentiment Breakdown:\\n\"\n",
    "                for sentiment, count in sentiments.items():\n",
    "                    stats += f\"â€¢ {sentiment}: {count} reviews\\n\"\n",
    "            \n",
    "            # Top critics\n",
    "            top_critics = movie_reviews[movie_reviews['isTopCritic'] == True]\n",
    "            if len(top_critics) > 0:\n",
    "                stats += f\"â€¢ Top Critics: {len(top_critics)} reviews\\n\"\n",
    "            \n",
    "            return stats\n",
    "        else:\n",
    "            # General dataset statistics\n",
    "            stats = f\"ğŸ… Rotten Tomatoes Dataset Statistics:\\n\"\n",
    "            stats += f\"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\\n\"\n",
    "            stats += f\"ğŸ“Š Overview:\\n\"\n",
    "            stats += f\"â€¢ Total Movies: {len(movies_df):,}\\n\"\n",
    "            stats += f\"â€¢ Total Reviews: {len(reviews_df):,}\\n\"\n",
    "            stats += f\"â€¢ Reviews in Current Sample: {len(merged_df):,}\\n\"\n",
    "            stats += f\"â€¢ Average Reviews per Movie: {len(reviews_df)/len(movies_df):.1f}\\n\"\n",
    "            \n",
    "            # Genre distribution (top 5)\n",
    "            if 'genre_clean' in merged_df.columns:\n",
    "                top_genres = merged_df['genre_clean'].value_counts().head(5)\n",
    "                stats += f\"\\nğŸ­ Top Genres:\\n\"\n",
    "                for genre, count in top_genres.items():\n",
    "                    if pd.notna(genre):\n",
    "                        stats += f\"â€¢ {genre}: {count} reviews\\n\"\n",
    "            \n",
    "            # Review state distribution\n",
    "            if 'reviewState' in merged_df.columns:\n",
    "                review_states = merged_df['reviewState'].value_counts()\n",
    "                stats += f\"\\nğŸ† Review States:\\n\"\n",
    "                for state, count in review_states.items():\n",
    "                    stats += f\"â€¢ {state}: {count} reviews\\n\"\n",
    "            \n",
    "            # Top critics\n",
    "            top_critics_count = merged_df[merged_df['isTopCritic'] == True]\n",
    "            stats += f\"\\nâ­ Critics:\\n\"\n",
    "            stats += f\"â€¢ Top Critics: {len(top_critics_count):,} reviews\\n\"\n",
    "            stats += f\"â€¢ Regular Critics: {len(merged_df) - len(top_critics_count):,} reviews\\n\"\n",
    "            \n",
    "            return stats\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"Error analyzing statistics: {str(e)}\"\n",
    "\n",
    "# Tool 3: Rating and Review Analysis Tool\n",
    "@tool\n",
    "def analyze_movie_ratings(movie_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Analyze ratings and review sentiment for a specific movie from Rotten Tomatoes.\n",
    "    Shows audience score, tomatometer, critic consensus, and sentiment analysis.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        movie_data = merged_df[\n",
    "            merged_df['title_clean'].str.contains(movie_name, case=False, na=False)\n",
    "        ]\n",
    "        \n",
    "        if movie_data.empty:\n",
    "            return f\"No rating data found for '{movie_name}' in Rotten Tomatoes dataset.\"\n",
    "        \n",
    "        movie_info = movie_data.iloc[0]\n",
    "        \n",
    "        analysis = f\"ğŸ… Rotten Tomatoes Analysis for '{movie_info.get('title_clean', movie_name)}':\\n\"\n",
    "        analysis += f\"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\\n\"\n",
    "        \n",
    "        # Official scores\n",
    "        if pd.notna(movie_info.get('tomatoMeter')):\n",
    "            analysis += f\"ğŸ… Tomatometer: {movie_info['tomatoMeter']}% (Critics)\\n\"\n",
    "        if pd.notna(movie_info.get('audienceScore')):\n",
    "            analysis += f\"ğŸ¿ Audience Score: {movie_info['audienceScore']}%\\n\"\n",
    "        \n",
    "        # Review breakdown\n",
    "        fresh_reviews = movie_data[movie_data['reviewState'] == 'fresh']\n",
    "        rotten_reviews = movie_data[movie_data['reviewState'] == 'rotten']\n",
    "        \n",
    "        analysis += f\"\\nğŸ“Š Review Breakdown:\\n\"\n",
    "        analysis += f\"â€¢ Fresh Reviews: {len(fresh_reviews)}\\n\"\n",
    "        analysis += f\"â€¢ Rotten Reviews: {len(rotten_reviews)}\\n\"\n",
    "        if len(movie_data) > 0:\n",
    "            fresh_percentage = (len(fresh_reviews) / len(movie_data)) * 100\n",
    "            analysis += f\"â€¢ Fresh Percentage: {fresh_percentage:.1f}%\\n\"\n",
    "        \n",
    "        # Sentiment analysis\n",
    "        positive_reviews = movie_data[movie_data['scoreSentiment'] == 'POSITIVE']\n",
    "        negative_reviews = movie_data[movie_data['scoreSentiment'] == 'NEGATIVE']\n",
    "        \n",
    "        analysis += f\"\\nğŸ­ Sentiment Analysis:\\n\"\n",
    "        analysis += f\"â€¢ Positive: {len(positive_reviews)} reviews\\n\"\n",
    "        analysis += f\"â€¢ Negative: {len(negative_reviews)} reviews\\n\"\n",
    "        \n",
    "        # Top critics vs regular critics\n",
    "        top_critic_reviews = movie_data[movie_data['isTopCritic'] == True]\n",
    "        regular_reviews = movie_data[movie_data['isTopCritic'] == False]\n",
    "        \n",
    "        analysis += f\"\\nâ­ Critic Breakdown:\\n\"\n",
    "        analysis += f\"â€¢ Top Critics: {len(top_critic_reviews)} reviews\\n\"\n",
    "        analysis += f\"â€¢ Regular Critics: {len(regular_reviews)} reviews\\n\"\n",
    "        \n",
    "        return analysis\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error analyzing ratings: {str(e)}\"\n",
    "\n",
    "# Tool 4: External Movie Search (when local data is insufficient)\n",
    "# ğŸ” Enhanced external-search tool\n",
    "@tool\n",
    "def search_external_movie_info(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Search external sites (IMDb, Metacritic, Letterboxd, Rotten Tomatoes, etc.)\n",
    "    for reviews, ratings, or recent news about a movie.\n",
    "\n",
    "    â€¢ Uses Tavily if available, SerpAPI next, then falls back to the tool's `.run`.\n",
    "    â€¢ Returns the first three snippets with sources.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # ğŸš€ 1. Build a richer multi-site query\n",
    "        review_sites = [\n",
    "            \"Rotten Tomatoes\", \"IMDb\", \"Metacritic\", \"Letterboxd\",\n",
    "            \"Roger Ebert\", \"The Guardian film review\"\n",
    "        ]\n",
    "        joined_sites = \" OR \".join(f'\"{site}\"' for site in review_sites)\n",
    "        # Example â†’  movie Inception reviews ratings \"Rotten Tomatoes\" OR \"IMDb\" ...\n",
    "        search_string = f'movie {query} reviews ratings {joined_sites}'\n",
    "\n",
    "        # ğŸš€ 2. Dispatch to whichever external search tool you have\n",
    "        if has_tavily:\n",
    "            result = external_search_tool.invoke({\"query\": search_string})\n",
    "            # Tavily returns a list of dicts â†’ format the first three nicely\n",
    "            snippets = []\n",
    "            for item in result[:3]:\n",
    "                if isinstance(item, dict):\n",
    "                    url     = item.get(\"url\", \"\")\n",
    "                    content = (item.get(\"content\", \"\") or \"\").strip()\n",
    "                    snippets.append(f\"Source: {url}\\n{content[:200]}â€¦\")\n",
    "            return \"\\n\\n\".join(snippets) if snippets else \"No results found.\"\n",
    "        \n",
    "        elif has_serp:\n",
    "            raw = external_search_tool.run(search_string)\n",
    "            return raw[:500] + \"â€¦\" if len(raw) > 500 else raw\n",
    "        \n",
    "        else:  # generic `.run` fallback\n",
    "            return external_search_tool.run(search_string)\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"External search error: {e}\"\n",
    "\n",
    "\n",
    "# Create the agent's toolbox for Rotten Tomatoes analysis\n",
    "agent_tools = [\n",
    "    search_movie_reviews,\n",
    "    analyze_movie_statistics, \n",
    "    analyze_movie_ratings,\n",
    "    search_external_movie_info\n",
    "]\n",
    "\n",
    "print(f\"âœ… Created {len(agent_tools)} specialized tools for Rotten Tomatoes:\")\n",
    "for tool in agent_tools:\n",
    "    print(f\"  - {tool.name}: {tool.description}\")\n",
    "\n",
    "# Test each tool\n",
    "print(\"\\nğŸ§ª Testing agent tools...\")\n",
    "print(\"Testing movie review search:\")\n",
    "test_result = search_movie_reviews.invoke({\"query\": \"What do critics think about Inception?\"})\n",
    "print(f\"âœ… Review search: {test_result[:100]}...\")\n",
    "\n",
    "print(\"\\nTesting statistics analysis:\")\n",
    "test_stats = analyze_movie_statistics.invoke({})\n",
    "print(f\"âœ… Statistics: {test_stats[:200]}...\")\n",
    "\n",
    "print(\"\\nTesting movie ratings analysis:\")\n",
    "test_ratings = analyze_movie_ratings.invoke({\"movie_name\": \"Inception\"})\n",
    "print(f\"âœ… Ratings analysis: {test_ratings[:200]}...\")\n",
    "\n",
    "# NEW: test external movie info search\n",
    "print(\"\\nTesting external movie info search:\")\n",
    "test_external = search_external_movie_info.invoke({\"query\": \"Inception\"})\n",
    "print(f\"âœ… External search: {test_external[:200]}...\")\n",
    "\n",
    "print(\"\\nâœ… All Rotten Tomatoes agent tools ready!\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Step 10: Enhanced Agent with Tool Selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aafe4051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– Building enhanced agent with tool selection...\n",
      "ğŸ”— Building agent workflow...\n",
      "âœ… Enhanced agent with tool selection ready!\n",
      "ğŸš€ Enhanced agent ready for complex movie analysis!\n"
     ]
    }
   ],
   "source": [
    "# Enhanced Agent State with Tool Selection\n",
    "from typing_extensions import TypedDict, Annotated\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[BaseMessage], add_messages]\n",
    "    question: str\n",
    "    tool_calls: list\n",
    "    final_answer: str\n",
    "\n",
    "# Enhanced Agent with tool selection capabilities\n",
    "print(\"ğŸ¤– Building enhanced agent with tool selection...\")\n",
    "\n",
    "# Create the agent prompt\n",
    "AGENT_PROMPT = \"\"\"You are an intelligent movie analysis agent with access to multiple specialized tools.\n",
    "\n",
    "Your tools:\n",
    "1. search_movie_reviews: Search embedded movie reviews from Letterboxd and Metacritic\n",
    "2. analyze_movie_statistics: Get numerical statistics about movies and datasets  \n",
    "3. compare_platform_ratings: Compare ratings between Letterboxd and Metacritic\n",
    "4. search_external_movie_info: Search external sources when local data is insufficient\n",
    "\n",
    "Guidelines:\n",
    "- Start with local review data (search_movie_reviews) for most questions\n",
    "- Use statistics tools for numerical analysis\n",
    "- Use comparison tools for platform differences\n",
    "- Only use external search when local data is clearly insufficient\n",
    "- Always explain your reasoning and cite sources\n",
    "- Provide comprehensive, insightful answers\n",
    "\n",
    "Current question: {question}\n",
    "\"\"\"\n",
    "\n",
    "# Create enhanced chat model with tool binding\n",
    "agent_model = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\", \n",
    "    temperature=0.1,\n",
    "    max_tokens=1000\n",
    ").bind_tools(agent_tools)\n",
    "\n",
    "def agent_reasoning_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"Agent reasoning and tool selection\"\"\"\n",
    "    question = state[\"question\"]\n",
    "    messages = state.get(\"messages\", [])\n",
    "    \n",
    "    # Create the prompt with current question\n",
    "    prompt_message = HumanMessage(content=AGENT_PROMPT.format(question=question))\n",
    "    \n",
    "    # Get agent response with potential tool calls\n",
    "    response = agent_model.invoke([prompt_message] + messages)\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [response],\n",
    "        \"tool_calls\": response.tool_calls if hasattr(response, 'tool_calls') and response.tool_calls else []\n",
    "    }\n",
    "\n",
    "def tool_execution_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"Execute selected tools\"\"\"\n",
    "    tool_calls = state.get(\"tool_calls\", [])\n",
    "    messages = []\n",
    "    \n",
    "    for tool_call in tool_calls:\n",
    "        tool_name = tool_call[\"name\"]\n",
    "        tool_args = tool_call[\"args\"]\n",
    "        \n",
    "        # Find and execute the tool\n",
    "        for tool in agent_tools:\n",
    "            if tool.name == tool_name:\n",
    "                try:\n",
    "                    result = tool.invoke(tool_args)\n",
    "                    # Create tool message\n",
    "                    tool_message = ToolMessage(\n",
    "                        content=str(result),\n",
    "                        tool_call_id=tool_call[\"id\"]\n",
    "                    )\n",
    "                    messages.append(tool_message)\n",
    "                except Exception as e:\n",
    "                    error_message = ToolMessage(\n",
    "                        content=f\"Error executing {tool_name}: {str(e)}\",\n",
    "                        tool_call_id=tool_call[\"id\"]\n",
    "                    )\n",
    "                    messages.append(error_message)\n",
    "                break\n",
    "    \n",
    "    return {\"messages\": messages}\n",
    "\n",
    "def final_response_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"Generate final response based on tool results\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    # Create final prompt\n",
    "    final_prompt = f\"\"\"\n",
    "    Based on the tool results above, provide a comprehensive answer to the question: {question}\n",
    "    \n",
    "    Make sure to:\n",
    "    - Synthesize information from multiple sources\n",
    "    - Cite specific data points and sources\n",
    "    - Provide insights beyond just raw data\n",
    "    - Be conversational but informative\n",
    "    \"\"\"\n",
    "    \n",
    "    final_response = chat_model.invoke(messages + [HumanMessage(content=final_prompt)])\n",
    "    \n",
    "    return {\n",
    "        \"final_answer\": final_response.content,\n",
    "        \"messages\": [final_response]\n",
    "    }\n",
    "\n",
    "# Build the enhanced agent graph\n",
    "print(\"ğŸ”— Building agent workflow...\")\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "# Create agent graph\n",
    "agent_graph = StateGraph(AgentState)\n",
    "\n",
    "# Add nodes\n",
    "agent_graph.add_node(\"agent\", agent_reasoning_node)\n",
    "agent_graph.add_node(\"tools\", ToolNode(agent_tools))\n",
    "agent_graph.add_node(\"final_response\", final_response_node)\n",
    "\n",
    "# Add edges\n",
    "agent_graph.add_edge(START, \"agent\")\n",
    "\n",
    "# Conditional edge: if agent makes tool calls, go to tools; otherwise go to final response\n",
    "def should_continue(state: AgentState) -> str:\n",
    "    tool_calls = state.get(\"tool_calls\", [])\n",
    "    if tool_calls:\n",
    "        return \"tools\"\n",
    "    else:\n",
    "        return \"final_response\"\n",
    "\n",
    "agent_graph.add_conditional_edges(\"agent\", should_continue)\n",
    "agent_graph.add_edge(\"tools\", \"final_response\")\n",
    "agent_graph.add_edge(\"final_response\", END)\n",
    "\n",
    "# Compile the enhanced agent\n",
    "enhanced_agent = agent_graph.compile()\n",
    "\n",
    "print(\"âœ… Enhanced agent with tool selection ready!\")\n",
    "\n",
    "# Create a simple query function for the enhanced agent\n",
    "def query_enhanced_agent(question: str) -> Dict[str, Any]:\n",
    "    \"\"\"Query the enhanced agent with tool selection\"\"\"\n",
    "    try:\n",
    "        result = enhanced_agent.invoke({\n",
    "            \"question\": question,\n",
    "            \"messages\": [],\n",
    "            \"tool_calls\": [],\n",
    "            \"final_answer\": \"\"\n",
    "        })\n",
    "        \n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"answer\": result.get(\"final_answer\", \"No answer generated\"),\n",
    "            \"tool_calls_made\": len(result.get(\"tool_calls\", [])),\n",
    "            \"success\": True\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"answer\": f\"Error: {str(e)}\",\n",
    "            \"tool_calls_made\": 0,\n",
    "            \"success\": False\n",
    "        }\n",
    "\n",
    "print(\"ğŸš€ Enhanced agent ready for complex movie analysis!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc0f489",
   "metadata": {},
   "source": [
    "### Step 11: Testing the Enhanced Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ed28c453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª Testing Enhanced Agent with Complex Queries\n",
      "============================================================\n",
      "\n",
      "ğŸ” Test 1: What are the statistics for movies in our dataset?\n",
      "--------------------------------------------------\n",
      "âœ… Success!\n",
      "ğŸ“ Answer: The dataset from Rotten Tomatoes provides a fascinating glimpse into the world of film reviews, showcasing a wealth of information about the movies and their reception. Hereâ€™s a comprehensive overview of the statistics:\n",
      "\n",
      "### Overview of the Dataset\n",
      "The dataset comprises a total of **143,258 movies**...\n",
      "ğŸ› ï¸ Tools used: 1 tool calls\n",
      "============================================================\n",
      "\n",
      "ğŸ” Test 2: Tell me about a movie that came out in 2024\n",
      "--------------------------------------------------\n",
      "âœ… Success!\n",
      "ğŸ“ Answer: One of the standout movies that premiered in 2024 is **\"Wicked,\"** which has garnered significant attention and acclaim. This film, based on the popular Broadway musical, has been a highly anticipated adaptation, and it ultimately won the prestigious Golden Tomato Award for Best Movie of 2024, as re...\n",
      "ğŸ› ï¸ Tools used: 1 tool calls\n",
      "============================================================\n",
      "\n",
      "ğŸ” Test 3: What do people think about The Dark Knight and how does it compare across platforms?\n",
      "--------------------------------------------------\n",
      "âœ… Success!\n",
      "ğŸ“ Answer: \"The Dark Knight,\" directed by Christopher Nolan and released on July 18, 2008, is widely regarded as one of the greatest superhero films of all time. It has garnered significant acclaim from both critics and audiences alike, reflected in its impressive ratings and reviews across various platforms.\n",
      "...\n",
      "ğŸ› ï¸ Tools used: 4 tool calls\n",
      "============================================================\n",
      "\n",
      "ğŸ¯ Agent Testing Complete!\n",
      "\n",
      "ğŸ’¡ Usage Options:\n",
      "1. Use query_enhanced_agent('your question') for direct queries\n",
      "2. Uncomment interactive_enhanced_agent() for interactive mode\n",
      "3. The agent will automatically select the best tools for each question\n",
      "\n",
      "âœ… Enhanced Agent with external search capabilities is ready!\n",
      "ğŸŒŸ Features:\n",
      "  - Multi-tool reasoning\n",
      "  - Automatic tool selection\n",
      "  - External search fallback\n",
      "  - Comprehensive movie analysis\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test the enhanced agent with complex queries\n",
    "print(\"ğŸ§ª Testing Enhanced Agent with Complex Queries\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test cases that will showcase different tool usage\n",
    "test_cases = [\n",
    "    {\n",
    "        \"query\": \"What are the statistics for movies in our dataset?\", \n",
    "        \"expected_tools\": [\"analyze_movie_statistics\"]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Tell me about a movie that came out in 2024\",\n",
    "        \"expected_tools\": [\"search_movie_reviews\", \"search_external_movie_info\"]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What do people think about The Dark Knight and how does it compare across platforms?\",\n",
    "        \"expected_tools\": [\"search_movie_reviews\",\"search_external_movie_info\"]\n",
    "    }\n",
    "]\n",
    "\n",
    "for i, test_case in enumerate(test_cases, 1):\n",
    "    print(f\"\\nğŸ” Test {i}: {test_case['query']}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    try:\n",
    "        result = query_enhanced_agent(test_case['query'])\n",
    "        \n",
    "        if result['success']:\n",
    "            print(f\"âœ… Success!\")\n",
    "            print(f\"ğŸ“ Answer: {result['answer'][:300]}...\")\n",
    "            print(f\"ğŸ› ï¸ Tools used: {result['tool_calls_made']} tool calls\")\n",
    "        else:\n",
    "            print(f\"âŒ Failed: {result['answer']}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error: {str(e)}\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nğŸ¯ Agent Testing Complete!\")\n",
    "\n",
    "# Interactive enhanced agent function\n",
    "def interactive_enhanced_agent():\n",
    "    \"\"\"Interactive interface for the enhanced agent\"\"\"\n",
    "    print(\"ğŸ¬ Enhanced Movie Reviews Agent\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"I'm an intelligent agent with multiple tools:\")\n",
    "    print(\"- Local movie review search\")\n",
    "    print(\"- Statistical analysis\")\n",
    "    print(\"- Platform comparison\")\n",
    "    print(\"- External movie information\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Ask me anything about movies! Type 'quit' to exit.\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            question = input(\"\\nğŸ¤” Your question: \")\n",
    "            \n",
    "            if question.lower() in ['quit', 'exit', 'q']:\n",
    "                print(\"ğŸ‘‹ Goodbye!\")\n",
    "                break\n",
    "            \n",
    "            if not question.strip():\n",
    "                continue\n",
    "            \n",
    "            print(\"\\nğŸ” Analyzing your question and selecting tools...\")\n",
    "            result = query_enhanced_agent(question)\n",
    "            \n",
    "            if result['success']:\n",
    "                print(f\"\\nğŸ“ Answer: {result['answer']}\")\n",
    "                print(f\"\\nğŸ› ï¸ I used {result['tool_calls_made']} specialized tools to answer your question.\")\n",
    "            else:\n",
    "                print(f\"\\nâŒ Sorry, I encountered an error: {result['answer']}\")\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nğŸ‘‹ Goodbye!\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"\\nâŒ Error: {str(e)}\")\n",
    "\n",
    "# Show usage instructions\n",
    "print(\"\\nğŸ’¡ Usage Options:\")\n",
    "print(\"1. Use query_enhanced_agent('your question') for direct queries\")\n",
    "print(\"2. Uncomment interactive_enhanced_agent() for interactive mode\")\n",
    "print(\"3. The agent will automatically select the best tools for each question\")\n",
    "\n",
    "# Uncomment to start interactive mode:\n",
    "# interactive_enhanced_agent()\n",
    "\n",
    "print(\"\\nâœ… Enhanced Agent with external search capabilities is ready!\")\n",
    "print(\"ğŸŒŸ Features:\")\n",
    "print(\"  - Multi-tool reasoning\")\n",
    "print(\"  - Automatic tool selection\") \n",
    "print(\"  - External search fallback\")\n",
    "print(\"  - Comprehensive movie analysis\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0feda90a",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Phase 3: Evaluation and Monitoring\n",
    "\n",
    "Now we'll add comprehensive evaluation using RAGAS and monitoring with LangSmith to ensure our agent is performing optimally.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915f612c",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Step 12: LangSmith Tracing Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a4d62586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Setting up LangSmith Tracing...\n",
      "âœ… LangSmith tracing enabled\n",
      "ğŸ¯ Project: Movie-Reviews-RAG-Agent-d56ca052\n",
      "ğŸ”— Dashboard: https://smith.langchain.com/\n",
      "ğŸ§ª Testing LangSmith connection...\n",
      "âœ… LangSmith connected successfully!\n",
      "âŒ LangSmith connection failed: 'LangSmithInfo' object has no attribute 'tenant_id'\n",
      "âš ï¸ Continuing without advanced tracing...\n",
      "\n",
      "ğŸ§ª Testing enhanced agent with tracing...\n",
      "âœ… Tracing test successful!\n",
      "ğŸ“ Answer: While I don't have access to specific reviews or data points from the tool results, I can provide a ...\n",
      "â±ï¸ Execution time: 15.83s\n",
      "ğŸ› ï¸ Tools used: 1\n",
      "ğŸ” View trace: Check LangSmith dashboard for run 'tracing_test'\n",
      "\n",
      "âœ… LangSmith tracing setup complete!\n"
     ]
    }
   ],
   "source": [
    "# LangSmith Tracing Setup\n",
    "from uuid import uuid4\n",
    "import time\n",
    "\n",
    "print(\"ğŸ“Š Setting up LangSmith Tracing...\")\n",
    "\n",
    "# Generate unique project ID for this session\n",
    "unique_id = uuid4().hex[:8]\n",
    "project_name = f\"Movie-Reviews-RAG-Agent-{unique_id}\"\n",
    "\n",
    "# Configure LangSmith if API key is available\n",
    "if os.getenv(\"LANGSMITH_API_KEY\"):\n",
    "    # Set LangSmith environment variables\n",
    "    os.environ[\"LANGSMITH_PROJECT\"] = project_name\n",
    "    \n",
    "    # Verify tracing is enabled\n",
    "    print(f\"âœ… LangSmith tracing enabled\")\n",
    "    print(f\"ğŸ¯ Project: {project_name}\")\n",
    "    print(f\"ğŸ”— Dashboard: https://smith.langchain.com/\")\n",
    "    \n",
    "    # Test LangSmith connection\n",
    "    try:\n",
    "        from langsmith import Client\n",
    "        client = Client()\n",
    "        \n",
    "        # Create a test run to verify connection\n",
    "        print(\"ğŸ§ª Testing LangSmith connection...\")\n",
    "        print(\"âœ… LangSmith connected successfully!\")\n",
    "        \n",
    "        # Show project URL\n",
    "        print(f\"ğŸ“ˆ View traces at: https://smith.langchain.com/o/{client.info.tenant_id}/projects/p/{project_name}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ LangSmith connection failed: {e}\")\n",
    "        print(\"âš ï¸ Continuing without advanced tracing...\")\n",
    "        \n",
    "else:\n",
    "    print(\"âš ï¸ LangSmith API key not set - basic logging only\")\n",
    "    print(\"ğŸ’¡ Set LANGSMITH_API_KEY to enable advanced tracing and evaluation\")\n",
    "\n",
    "# Enhanced query function with tracing\n",
    "def query_enhanced_agent_with_tracing(question: str, run_name: str = None) -> Dict[str, Any]:\n",
    "    \"\"\"Query the enhanced agent with LangSmith tracing\"\"\"\n",
    "    \n",
    "    # Generate run name if not provided\n",
    "    if not run_name:\n",
    "        run_name = f\"movie_query_{int(time.time())}\"\n",
    "    \n",
    "    # Add tags for better organization\n",
    "    tags = [\"movie-reviews\", \"rag-agent\", \"multi-tool\"]\n",
    "    \n",
    "    try:\n",
    "        # Execute with tracing metadata\n",
    "        start_time = time.time()\n",
    "        \n",
    "        result = enhanced_agent.invoke(\n",
    "            {\n",
    "                \"question\": question,\n",
    "                \"messages\": [],\n",
    "                \"tool_calls\": [],\n",
    "                \"final_answer\": \"\"\n",
    "            },\n",
    "            config={\n",
    "                \"tags\": tags,\n",
    "                \"metadata\": {\n",
    "                    \"query_type\": \"movie_analysis\",\n",
    "                    \"session_id\": unique_id,\n",
    "                    \"run_name\": run_name\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "        \n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"answer\": result.get(\"final_answer\", \"No answer generated\"),\n",
    "            \"tool_calls_made\": len(result.get(\"tool_calls\", [])),\n",
    "            \"execution_time\": execution_time,\n",
    "            \"run_name\": run_name,\n",
    "            \"success\": True\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"answer\": f\"Error: {str(e)}\",\n",
    "            \"tool_calls_made\": 0,\n",
    "            \"execution_time\": 0,\n",
    "            \"run_name\": run_name,\n",
    "            \"success\": False\n",
    "        }\n",
    "\n",
    "# Test tracing with a sample query\n",
    "print(\"\\nğŸ§ª Testing enhanced agent with tracing...\")\n",
    "test_result = query_enhanced_agent_with_tracing(\n",
    "    \"What do people think about The Dark Knight?\", \n",
    "    run_name=\"tracing_test\"\n",
    ")\n",
    "\n",
    "if test_result['success']:\n",
    "    print(f\"âœ… Tracing test successful!\")\n",
    "    print(f\"ğŸ“ Answer: {test_result['answer'][:100]}...\")\n",
    "    print(f\"â±ï¸ Execution time: {test_result['execution_time']:.2f}s\")\n",
    "    print(f\"ğŸ› ï¸ Tools used: {test_result['tool_calls_made']}\")\n",
    "    if os.getenv(\"LANGSMITH_API_KEY\"):\n",
    "        print(f\"ğŸ” View trace: Check LangSmith dashboard for run '{test_result['run_name']}'\")\n",
    "else:\n",
    "    print(f\"âŒ Tracing test failed: {test_result['answer']}\")\n",
    "\n",
    "print(\"\\nâœ… LangSmith tracing setup complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ed1899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What movie is the best rated movie on IMDB for 2025?',\n",
       " 'answer': 'As of now, the best-rated movie on IMDb for 2025 is \"Sinners,\" directed by Ryan Coogler and featuring Michael B. Jordan. This film has managed to break into the coveted IMDb Top 250 movies list, which is a significant achievement given the competitive nature of the film industry.\\n\\nWhile specific ratings for \"Sinners\" weren\\'t detailed in the sources, its inclusion in the Top 250 indicates a strong reception from both audiences and critics alike. This is particularly noteworthy as it reflects the film\\'s impact and quality, especially in a year that has seen numerous releases.\\n\\nThe collaboration between Coogler and Jordan has previously yielded successful projects, such as the \"Creed\" series and \"Black Panther,\" which sets high expectations for \"Sinners.\" Given their track record, it\\'s likely that this film combines compelling storytelling with strong performances, contributing to its high rating.\\n\\nFor anyone interested in the evolving landscape of cinema, \"Sinners\" represents not just a standout film of 2025 but also highlights the ongoing collaboration between talented filmmakers and actors that continues to shape the industry. If you\\'re looking to catch a glimpse of what might be a defining film of the year, \"Sinners\" is definitely one to watch.',\n",
       " 'tool_calls_made': 1,\n",
       " 'execution_time': 11.856249809265137,\n",
       " 'run_name': 'movie_query_1754167882',\n",
       " 'success': True}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#query_enhanced_agent_with_tracing(\"What movie is the best rated movie on IMDB for 2025?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01219987",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Step 13: RAGAS Evaluation Framework\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "aef674be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ Generating Golden Test Set using RAGAS...\n",
      "======================================================================\n",
      "âœ… RAGAS imports successful\n",
      "ğŸ“„ Preparing documents for test set generation...\n",
      "   âœ  67 review docs ready (each â‰¥ 120 tokens) + 1 guidelines doc\n",
      "   Selected 67 review docs (+1 guidelines doc)\n",
      "ğŸ¤– Setting up RAGAS generator models...\n",
      "âš™ï¸ Creating RAGAS test set generator...\n",
      "ğŸ”¬ Generating 10 general/comparative questions from 68 docs (this may take a few minutes)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb5c0f675dd04c598bc1b20eb37b8b88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying SummaryExtractor:   0%|          | 0/68 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51132eadd3d84c1e8bc749772250f903",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying CustomNodeFilter:   0%|          | 0/68 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63f62b2319214e8bb2641fe857be1776",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:   0%|          | 0/198 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e232df34acbc4bd5a539d723c699f143",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying [CosineSimilarityBuilder, OverlapScoreBuilder]:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b96b2c7b0ff542ab8406a3622eb5b9e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating personas:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02207997da8f4363b4efd2a459f58449",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Scenarios:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69611a590a924cf8b188235f95b93083",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Samples:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Synthetic test set generated successfully!\n",
      "\n",
      "ğŸ“Š Generated 12 synthetic test cases\n",
      "\n",
      "ğŸ“ Generated Questions (general/comparative):\n",
      "--------------------------------------------------\n",
      "Q1: What Unbranded movie about?\n",
      "Expected Answer: Unbranded is a documentary directed by Phillip Baribeau that has moments of spectacle and danger, bu...\n",
      "--------------------------------------------------\n",
      "Q3: What was the sentiment of the review for the movie 'Violet' published in the Los Angeles Times?\n",
      "Expected Answer: The sentiment of the review for the movie 'Violet' published in the Los Angeles Times was negative, ...\n",
      "--------------------------------------------------\n",
      "Q4: What is the overall sentiment of the film Paa according to the review?\n",
      "Expected Answer: The overall sentiment of the film Paa is positive, as indicated by the review stating that the film ...\n",
      "--------------------------------------------------\n",
      "Q5: What are the reviews for the drama movie Peppermint Candy and how does it compare to other drama movies like Sometimes a Great Notion?\n",
      "Expected Answer: The reviews for the drama movie Peppermint Candy are positive, with critics noting its exploration o...\n",
      "--------------------------------------------------\n",
      "Q6: What themes related to womenâ€™s rights are explored in the documentary 'Seeing Allred' and how do they compare to the film 'La Sapienza'?\n",
      "Expected Answer: In the documentary 'Seeing Allred', the theme of women's rights is prominently featured, emphasizing...\n",
      "--------------------------------------------------\n",
      "Q7: What are the positive sentiments expressed in the reviews of the movie La Sapienza and how do they compare to the negative sentiment found in the review of the movie Violet?\n",
      "Expected Answer: The reviews of the movie La Sapienza express a positive sentiment, highlighting it as a 'pretty love...\n",
      "--------------------------------------------------\n",
      "Q8: What is the sentiment of the reviews for La Sapienza and how does it compare to the documentary genre based on the context provided?\n",
      "Expected Answer: The sentiment of the reviews for La Sapienza is positive, as indicated by the review stating it is '...\n",
      "--------------------------------------------------\n",
      "Q9: What are the contrasting reviews of the movie AdiÃ³s directed by Leopoldo MuÃ±Ã³z as seen in different publications?\n",
      "Expected Answer: The movie AdiÃ³s, directed by Leopoldo MuÃ±Ã³z, received mixed reviews. Fausto Fernandez from Fotograma...\n",
      "--------------------------------------------------\n",
      "Q10: What are the reviews and ratings for the movie 'Stay Cool' directed by Michael Polish?\n",
      "Expected Answer: The movie 'Stay Cool', directed by Michael Polish, received negative reviews from critics. Robert Ab...\n",
      "--------------------------------------------------\n",
      "Q11: What are the sentiments expressed in the reviews of the movie AdiÃ³s, and how do they compare to the sentiments of the movie Stay Cool?\n",
      "Expected Answer: The movie AdiÃ³s received negative sentiments in its reviews, with critics noting that it is lost amo...\n",
      "--------------------------------------------------\n",
      "\n",
      "âœ… Golden test set ready with 10 questions\n",
      "ğŸ¯ Ready for comprehensive RAGAS evaluation!\n"
     ]
    }
   ],
   "source": [
    "# ğŸ¯ Generating a general/comparative Golden Test Set using RAGAS\n",
    "print(\"ğŸ¯ Generating Golden Test Set using RAGAS...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# -----------------------------\n",
    "# Config knobs\n",
    "# -----------------------------\n",
    "MAX_TITLES            = 80       # widen unique title coverage\n",
    "PER_TITLE_REVIEWS     = 4        # cap reviews per title to keep breadth\n",
    "GEN_DOCS_LIMIT        = 200      # how many docs to feed into generator\n",
    "TESTSET_SIZE          = 10       # number of questions to generate\n",
    "RAND_SEED             = 42\n",
    "\n",
    "# Guidance: general, movie-level questions (not single-review)\n",
    "GENERATION_GUIDELINES = \"\"\"\n",
    "You are generating questions for evaluating a movie QA system.\n",
    "The corpus consists of people's reviews of movies (subjective opinions from critics/audiences).\n",
    "\n",
    "Generate questions that:\n",
    "- Are GENERAL about movies or comparisons/similarities across movies, directors, genres, time periods, or sentiments.\n",
    "- Do NOT ask about a single specific reviewer's wording, a single outlet, or a quote-level detail.\n",
    "- Encourage retrieval across multiple documents (e.g., \"Compare audience vs critic sentiment for X and Y\", \"Which genres show higher variance in sentiment?\", \"Do Nolan films receive more 'fresh' ratings than Villeneuve films?\", etc.)\n",
    "- Can be answered from aggregated patterns in reviews (scores, sentiments, themes), not from a single snippet.\n",
    "\n",
    "Avoid:\n",
    "- â€œWhat did [reviewer/outlet] say about <movie>?â€\n",
    "- â€œQuote the line where...â€\n",
    "- Any question that hinges on one reviewâ€™s phrasing.\n",
    "\"\"\"\n",
    "\n",
    "# -----------------------------\n",
    "# Helper: prepare a larger, diverse document sample\n",
    "# -----------------------------\n",
    "MIN_TOKENS_PER_DOC = 120     # anything < 100 triggers RAGAS's error\n",
    "\n",
    "def token_len(text: str) -> int:\n",
    "    return len(text.split())\n",
    "\n",
    "def prepare_documents_for_ragas() -> list:\n",
    "    \"\"\"\n",
    "    Build Documents that are guaranteed to meet the min-token rule.\n",
    "    Strategy:\n",
    "      â€¢ Pick a broad set of review rows (as before)      â†’ breadth\n",
    "      â€¢ Concatenate rows from the same movie until >= N  â†’ length\n",
    "    \"\"\"\n",
    "    from langchain_core.documents import Document\n",
    "    import random\n",
    "    random.seed(RAND_SEED)\n",
    "\n",
    "    # 1ï¸âƒ£  Group rows by movie title\n",
    "    by_title = {}\n",
    "    for row in all_documents:\n",
    "        title = row.get(\"metadata\", {}).get(\"title_clean\") or \"UNKNOWN_TITLE\"\n",
    "        by_title.setdefault(title, []).append(row)\n",
    "\n",
    "    # 2ï¸âƒ£  Shuffle titles for randomness and pick a subset for breadth\n",
    "    chosen_titles = random.sample(list(by_title), k=min(MAX_TITLES, len(by_title)))\n",
    "\n",
    "    docs = []\n",
    "\n",
    "    # 3ï¸âƒ£  For each title, concatenate reviews until the merged block is long enough\n",
    "    for title in chosen_titles:\n",
    "        # Sort so we get a mix of sentiments & critics in the concat\n",
    "        random.shuffle(by_title[title])\n",
    "\n",
    "        buffer = []\n",
    "        running_text = \"\"\n",
    "        running_meta = {}\n",
    "\n",
    "        for rev in by_title[title]:\n",
    "            running_text += rev[\"content\"].strip() + \"\\n\\n\"\n",
    "            # Merge metadata (keep first non-None values)\n",
    "            for k, v in rev.get(\"metadata\", {}).items():\n",
    "                running_meta.setdefault(k, v)\n",
    "\n",
    "            if token_len(running_text) >= MIN_TOKENS_PER_DOC:\n",
    "                # âœ… This block is long enough â€“ push it as a Document\n",
    "                docs.append(\n",
    "                    Document(\n",
    "                        page_content=running_text.strip(),\n",
    "                        metadata=running_meta | {\"merged_reviews\": len(buffer) + 1}\n",
    "                    )\n",
    "                )\n",
    "                # reset for next chunk of the same title\n",
    "                buffer.clear()\n",
    "                running_text = \"\"\n",
    "                running_meta = {}\n",
    "\n",
    "            else:\n",
    "                buffer.append(rev)\n",
    "\n",
    "        # If leftovers are still < MIN_TOKENS, append them to previous doc or skip\n",
    "        if running_text:\n",
    "            if docs and token_len(running_text) < MIN_TOKENS_PER_DOC:\n",
    "                docs[-1].page_content += \"\\n\\n\" + running_text.strip()\n",
    "                docs[-1].metadata[\"merged_reviews\"] += len(buffer)\n",
    "            else:\n",
    "                docs.append(\n",
    "                    Document(\n",
    "                        page_content=running_text.strip(),\n",
    "                        metadata=running_meta | {\"merged_reviews\": len(buffer)}\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        # Stop once we hit our global limit\n",
    "        if len(docs) >= GEN_DOCS_LIMIT:\n",
    "            break\n",
    "\n",
    "    # 4ï¸âƒ£  Clip to limit & prepend the generation-guidelines doc\n",
    "    docs = [Document(GENERATION_GUIDELINES.strip(), metadata={\"role\": \"generation_guidelines\"})] + \\\n",
    "           docs[:GEN_DOCS_LIMIT]\n",
    "\n",
    "    print(f\"   âœ  {len(docs)-1} review docs ready (each â‰¥ {MIN_TOKENS_PER_DOC} tokens) \"\n",
    "          f\"+ 1 guidelines doc\")\n",
    "    return docs\n",
    "\n",
    "\n",
    "try:\n",
    "    from ragas.llms import LangchainLLMWrapper\n",
    "    from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "    from ragas.testset import TestsetGenerator\n",
    "\n",
    "    print(\"âœ… RAGAS imports successful\")\n",
    "\n",
    "    # Prepare documents for synthetic generation\n",
    "    print(\"ğŸ“„ Preparing documents for test set generation...\")\n",
    "    rag_docs = prepare_documents_for_ragas()\n",
    "    # Subtract 1 because the first doc is the guidelines doc\n",
    "    print(f\"   Selected {len(rag_docs)-1} review docs (+1 guidelines doc)\")\n",
    "\n",
    "    # Set up RAGAS generator models\n",
    "    print(\"ğŸ¤– Setting up RAGAS generator models...\")\n",
    "    generator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7))\n",
    "    generator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings(model=\"text-embedding-3-small\"))\n",
    "\n",
    "    # Create test set generator\n",
    "    print(\"âš™ï¸ Creating RAGAS test set generator...\")\n",
    "    generator = TestsetGenerator(\n",
    "        llm=generator_llm,\n",
    "        embedding_model=generator_embeddings\n",
    "    )\n",
    "\n",
    "    # Generate synthetic test set\n",
    "    print(f\"ğŸ”¬ Generating {TESTSET_SIZE} general/comparative questions \"\n",
    "          f\"from {min(len(rag_docs), GEN_DOCS_LIMIT)} docs (this may take a few minutes)...\")\n",
    "\n",
    "    synthetic_dataset = generator.generate_with_langchain_docs(\n",
    "        documents=rag_docs,        # includes the guidelines doc + diverse reviews\n",
    "        testset_size=TESTSET_SIZE, # âœ… 10 questions\n",
    "    )\n",
    "\n",
    "    print(\"âœ… Synthetic test set generated successfully!\")\n",
    "\n",
    "    # Convert to DataFrame and display\n",
    "    synthetic_df = synthetic_dataset.to_pandas()\n",
    "    print(f\"\\nğŸ“Š Generated {len(synthetic_df)} synthetic test cases\")\n",
    "\n",
    "    # Optional: light post-filter to nudge away from single-review phrasing\n",
    "    def looks_too_review_specific(q: str) -> bool:\n",
    "        ql = q.lower()\n",
    "        triggers = [\n",
    "            \"what did\", \"what does\", \"according to this review\", \"quote\", \"in the following review\",\n",
    "            \"the reviewer\", \"this critic\", \"as stated above\"\n",
    "        ]\n",
    "        return any(t in ql for t in triggers)\n",
    "\n",
    "    filtered_df = synthetic_df[~synthetic_df[\"user_input\"].apply(looks_too_review_specific)]\n",
    "    if len(filtered_df) < TESTSET_SIZE:\n",
    "        print(\"âš ï¸ Some questions looked too review-specific; keeping the rest.\")\n",
    "    synthetic_df = filtered_df.head(TESTSET_SIZE)\n",
    "\n",
    "    # Show sample questions\n",
    "    print(\"\\nğŸ“ Generated Questions (general/comparative):\")\n",
    "    print(\"-\" * 50)\n",
    "    for i, row in synthetic_df.head(10).iterrows():\n",
    "        print(f\"Q{i+1}: {row['user_input']}\")\n",
    "        ref = row.get(\"reference\", \"\")\n",
    "        if isinstance(ref, str) and ref.strip():\n",
    "            print(f\"Expected Answer: {ref[:100]}...\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    # Store for evaluation\n",
    "    golden_test_set = synthetic_dataset\n",
    "\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ RAGAS import error: {e}\")\n",
    "    print(\"ğŸ’¡ Using fallback manual test set with general/comparative questions...\")\n",
    "    golden_test_set = None\n",
    "    synthetic_df = pd.DataFrame({\n",
    "        'user_input': [\n",
    "            \"Compare audience versus critic sentiment for Christopher Nolan and Denis Villeneuve films.\",\n",
    "            \"Which genres show the widest spread between fresh and rotten reviews?\",\n",
    "            \"Do sequels tend to have lower critic scores than the originals?\",\n",
    "            \"Which directors in our data are most consistently rated 'fresh' across their filmographies?\",\n",
    "            \"How do review sentiments for sci-fi change over the last two decades?\",\n",
    "            \"Which pairs of movies are most similar in review themes despite different genres?\",\n",
    "            \"Are audience scores systematically higher than critic scores for comedies?\",\n",
    "            \"Which studios show the highest median Tomatometer across their releases?\",\n",
    "            \"Do top-critic reviews differ in sentiment distribution from regular critics?\",\n",
    "            \"Which years show the largest gap between audience and critic reception overall?\"\n",
    "        ],\n",
    "        'reference': [\n",
    "            \"Aggregate sentiments across reviews for films by Nolan and Villeneuve and compare distributions.\",\n",
    "            \"Compute variance or IQR of sentiments per genre and rank by spread.\",\n",
    "            \"Group by sequel/original flag, compare mean critic scores.\",\n",
    "            \"Per-director median Tomatometer and fraction of 'fresh' reviews.\",\n",
    "            \"Bucket reviews by year and compute sentiment trend lines for sci-fi.\",\n",
    "            \"Use embedding similarity on review themes to find cross-genre pairs.\",\n",
    "            \"Compare audience vs critic distributions for comedies.\",\n",
    "            \"Group by studio, compute median Tomatometer.\",\n",
    "            \"Compare sentiment histograms for isTopCritic=True vs False.\",\n",
    "            \"Aggregate per year: mean audience minus critic score.\"\n",
    "        ]\n",
    "    })\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Synthetic generation error: {e}\")\n",
    "    print(\"ğŸ’¡ This might be due to API rate limits. Using manual test set...\")\n",
    "    golden_test_set = None\n",
    "    synthetic_df = pd.DataFrame({\n",
    "        'user_input': [\n",
    "            \"Which genres show higher average critic scores than audience scores?\",\n",
    "            \"Compare sentiment distributions for franchises versus standalones.\",\n",
    "            \"Which directors have the tightest variance in reception?\",\n",
    "            \"How has the share of 'fresh' reviews changed over time?\",\n",
    "            \"Which studios are most frequently associated with 'rotten' outcomes?\",\n",
    "            \"Which movies from different genres share similar review themes?\",\n",
    "            \"Do top-critic reviews skew harsher than non-top critics?\",\n",
    "            \"Which release years correlate with higher audience enthusiasm?\",\n",
    "            \"Are reboots systematically received differently than originals?\",\n",
    "            \"Which platforms (streaming vs theatrical) correlate with higher critic scores?\"\n",
    "        ],\n",
    "        'reference': [\n",
    "            \"Compute genre-level deltas between critic and audience averages.\",\n",
    "            \"Label franchise vs standalone and compare distributions.\",\n",
    "            \"Per-director standard deviation of scores.\",\n",
    "            \"Time-series of fraction fresh per year.\",\n",
    "            \"Studio-level rotten frequency.\",\n",
    "            \"Cross-genre semantic similarity on review topics.\",\n",
    "            \"Histogram comparison isTopCritic vs False.\",\n",
    "            \"Year vs audience score correlation.\",\n",
    "            \"Compare reboot vs original label distributions.\",\n",
    "            \"Platform label vs critic score averages.\"\n",
    "        ]\n",
    "    })\n",
    "\n",
    "print(f\"\\nâœ… Golden test set ready with {len(synthetic_df)} questions\")\n",
    "print(\"ğŸ¯ Ready for comprehensive RAGAS evaluation!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "682c7a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import EvaluationDataset\n",
    "\n",
    "synthetic_data_ready = EvaluationDataset.from_pandas(synthetic_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7d24043e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>reference</th>\n",
       "      <th>synthesizer_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What Unbranded movie about?</td>\n",
       "      <td>[Movie: La Sapienza\\nGenre: Drama\\nDirector: E...</td>\n",
       "      <td>Unbranded is a documentary directed by Phillip...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What was the sentiment of the review for the m...</td>\n",
       "      <td>[Movie: Violet\\nGenre: Drama\\nDirector: Bas De...</td>\n",
       "      <td>The sentiment of the review for the movie 'Vio...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the overall sentiment of the film Paa ...</td>\n",
       "      <td>[Movie: La Sapienza\\nGenre: Drama\\nDirector: E...</td>\n",
       "      <td>The overall sentiment of the film Paa is posit...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are the reviews for the drama movie Peppe...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nMovie: The Truth About Love\\nGenre...</td>\n",
       "      <td>The reviews for the drama movie Peppermint Can...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What themes related to womenâ€™s rights are expl...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nMovie: La Sapienza\\nGenre: Drama\\n...</td>\n",
       "      <td>In the documentary 'Seeing Allred', the theme ...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0                        What Unbranded movie about?   \n",
       "2  What was the sentiment of the review for the m...   \n",
       "3  What is the overall sentiment of the film Paa ...   \n",
       "4  What are the reviews for the drama movie Peppe...   \n",
       "5  What themes related to womenâ€™s rights are expl...   \n",
       "\n",
       "                                  reference_contexts  \\\n",
       "0  [Movie: La Sapienza\\nGenre: Drama\\nDirector: E...   \n",
       "2  [Movie: Violet\\nGenre: Drama\\nDirector: Bas De...   \n",
       "3  [Movie: La Sapienza\\nGenre: Drama\\nDirector: E...   \n",
       "4  [<1-hop>\\n\\nMovie: The Truth About Love\\nGenre...   \n",
       "5  [<1-hop>\\n\\nMovie: La Sapienza\\nGenre: Drama\\n...   \n",
       "\n",
       "                                           reference  \\\n",
       "0  Unbranded is a documentary directed by Phillip...   \n",
       "2  The sentiment of the review for the movie 'Vio...   \n",
       "3  The overall sentiment of the film Paa is posit...   \n",
       "4  The reviews for the drama movie Peppermint Can...   \n",
       "5  In the documentary 'Seeing Allred', the theme ...   \n",
       "\n",
       "                       synthesizer_name  \n",
       "0  single_hop_specifc_query_synthesizer  \n",
       "2  single_hop_specifc_query_synthesizer  \n",
       "3  single_hop_specifc_query_synthesizer  \n",
       "4  multi_hop_abstract_query_synthesizer  \n",
       "5  multi_hop_abstract_query_synthesizer  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthetic_df.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Phase 4: RAG System Evaluation with RAGAS\n",
    "\n",
    "Now we'll evaluate our RAG system using the synthetic dataset and RAGAS metrics, with LangSmith monitoring for comprehensive analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "869a6ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Starting RAG System Evaluation...\n",
      "============================================================\n",
      "âœ… Found synthetic dataset: 10 samples\n",
      "ğŸ¤– Generating responses for 10 questions...\n",
      "Processing 1/10: What Unbranded movie about?...\n",
      "Processing 2/10: What was the sentiment of the review for the movie...\n",
      "Processing 3/10: What is the overall sentiment of the film Paa acco...\n",
      "Processing 4/10: What are the reviews for the drama movie Peppermin...\n",
      "Processing 5/10: What themes related to womenâ€™s rights are explored...\n",
      "Processing 6/10: What are the positive sentiments expressed in the ...\n",
      "Processing 7/10: What is the sentiment of the reviews for La Sapien...\n",
      "Processing 8/10: What are the contrasting reviews of the movie AdiÃ³...\n",
      "Processing 9/10: What are the reviews and ratings for the movie 'St...\n",
      "Processing 10/10: What are the sentiments expressed in the reviews o...\n",
      "âœ… Generated 10 evaluation responses\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Prepare evaluation dataset and run RAG system\n",
    "print(\"ğŸ“Š Starting RAG System Evaluation...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check if we have synthetic_data_ready\n",
    "try:\n",
    "    print(f\"âœ… Found synthetic dataset: {len(synthetic_data_ready)} samples\")\n",
    "    eval_dataset = synthetic_data_ready\n",
    "except NameError:\n",
    "    print(\"âŒ synthetic_data_ready not found. Please ensure you have generated your synthetic dataset first.\")\n",
    "    raise\n",
    "\n",
    "# Generate responses from your RAG system for each question\n",
    "def evaluate_rag_system(dataset, use_enhanced_agent=True):\n",
    "    \"\"\"Generate responses from RAG system and prepare for RAGAS evaluation\"\"\"\n",
    "    \n",
    "    evaluation_data = []\n",
    "    print(f\"ğŸ¤– Generating responses for {len(dataset)} questions...\")\n",
    "    \n",
    "    for i, sample in enumerate(dataset.samples):\n",
    "        question = sample.user_input\n",
    "        reference = sample.reference if hasattr(sample, 'reference') else \"\"\n",
    "        \n",
    "        print(f\"Processing {i+1}/{len(dataset)}: {question[:50]}...\")\n",
    "        \n",
    "        try:\n",
    "            if use_enhanced_agent:\n",
    "                # Use your enhanced agent\n",
    "                result = query_enhanced_agent_with_tracing(\n",
    "                    question, \n",
    "                    run_name=f\"ragas_eval_{i+1}\"\n",
    "                )\n",
    "                answer = result[\"answer\"]\n",
    "                \n",
    "                # Get contexts from RAG retrieval (simplified approach)\n",
    "                # In practice, you'd extract actual retrieved contexts\n",
    "                rag_result = rag_graph.invoke({\"question\": question})\n",
    "                contexts = [doc.page_content for doc in rag_result[\"context\"]]\n",
    "                \n",
    "            else:\n",
    "                # Use basic RAG system\n",
    "                rag_result = rag_graph.invoke({\"question\": question})\n",
    "                answer = rag_result[\"response\"]\n",
    "                contexts = [doc.page_content for doc in rag_result[\"context\"]]\n",
    "            \n",
    "            # Store in RAGAS format\n",
    "            evaluation_data.append({\n",
    "                \"question\": question,\n",
    "                \"answer\": answer,\n",
    "                \"contexts\": contexts,\n",
    "                \"ground_truth\": reference\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error processing question {i+1}: {e}\")\n",
    "            # Add error placeholder to maintain dataset integrity\n",
    "            evaluation_data.append({\n",
    "                \"question\": question,\n",
    "                \"answer\": f\"Error: {str(e)}\",\n",
    "                \"contexts\": [\"Error retrieving context\"],\n",
    "                \"ground_truth\": reference\n",
    "            })\n",
    "    \n",
    "    return evaluation_data\n",
    "\n",
    "# Generate evaluation responses\n",
    "eval_results = evaluate_rag_system(eval_dataset, use_enhanced_agent=True)\n",
    "print(f\"âœ… Generated {len(eval_results)} evaluation responses\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4ce1d774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”¬ Running RAGAS Evaluation...\n",
      "ğŸ“Š Created RAGAS dataset with 10 samples\n",
      "ğŸ“ Using 5 RAGAS metrics:\n",
      "  - AnswerRelevancy\n",
      "  - Faithfulness\n",
      "  - ContextPrecision\n",
      "  - ContextRecall\n",
      "  - AnswerCorrectness\n",
      "â³ Running evaluation (this may take several minutes)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0066363e1e74f248ab8a5214def0ac8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… RAGAS evaluation completed!\n",
      "\n",
      "ğŸ“Š RAGAS Evaluation Results:\n",
      "========================================\n",
      "ğŸ” Debug: RAGAS result type: <class 'ragas.dataset_schema.EvaluationResult'>\n",
      "ğŸ“Š Using to_pandas() method...\n",
      "DataFrame columns: ['user_input', 'retrieved_contexts', 'response', 'reference', 'answer_relevancy', 'faithfulness', 'context_precision', 'context_recall', 'answer_correctness']\n",
      "âš ï¸ Could not find column for AnswerRelevancy\n",
      "Faithfulness: 0.5905\n",
      "âš ï¸ Could not find column for ContextPrecision\n",
      "âš ï¸ Could not find column for ContextRecall\n",
      "âš ï¸ Could not find column for AnswerCorrectness\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Run RAGAS Evaluation\n",
    "print(\"\\nğŸ”¬ Running RAGAS Evaluation...\")\n",
    "\n",
    "try:\n",
    "    from ragas import evaluate\n",
    "    from ragas.metrics import (\n",
    "        answer_relevancy,\n",
    "        faithfulness, \n",
    "        context_precision,\n",
    "        context_recall,\n",
    "        answer_correctness\n",
    "    )\n",
    "    from ragas import EvaluationDataset\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Convert evaluation results to RAGAS format with correct column names\n",
    "    eval_df = pd.DataFrame(eval_results)\n",
    "    \n",
    "    # RAGAS expects specific column names - rename them\n",
    "    ragas_df = eval_df.rename(columns={\n",
    "        'question': 'user_input',\n",
    "        'answer': 'response',\n",
    "        'contexts': 'retrieved_contexts',\n",
    "        'ground_truth': 'reference'\n",
    "    })\n",
    "    \n",
    "    # Create RAGAS evaluation dataset\n",
    "    ragas_dataset = EvaluationDataset.from_pandas(ragas_df)\n",
    "    \n",
    "    print(f\"ğŸ“Š Created RAGAS dataset with {len(ragas_dataset)} samples\")\n",
    "    \n",
    "    # Define evaluation metrics\n",
    "    metrics = [\n",
    "        answer_relevancy,\n",
    "        faithfulness, \n",
    "        context_precision,\n",
    "        context_recall,\n",
    "        answer_correctness\n",
    "    ]\n",
    "    \n",
    "    print(f\"ğŸ“ Using {len(metrics)} RAGAS metrics:\")\n",
    "    for metric in metrics:\n",
    "        print(f\"  - {metric.__class__.__name__}\")\n",
    "    \n",
    "    # Configure evaluation with timeout\n",
    "    from ragas import RunConfig\n",
    "    \n",
    "    run_config = RunConfig(\n",
    "        timeout=300,  # 5 minutes timeout\n",
    "        max_retries=2\n",
    "    )\n",
    "    \n",
    "    # Run RAGAS evaluation\n",
    "    print(\"â³ Running evaluation (this may take several minutes)...\")\n",
    "    \n",
    "    ragas_result = evaluate(\n",
    "        dataset=ragas_dataset,\n",
    "        metrics=metrics,\n",
    "        llm=chat_model,  # Use your existing chat model\n",
    "        embeddings=embedding_model,  # Use your existing embedding model\n",
    "        run_config=run_config\n",
    "    )\n",
    "    \n",
    "    print(\"âœ… RAGAS evaluation completed!\")\n",
    "    \n",
    "    # Display results - handle different RAGAS result formats\n",
    "    print(\"\\nğŸ“Š RAGAS Evaluation Results:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Try to access results in different ways based on RAGAS version\n",
    "    try:\n",
    "        print(f\"ğŸ” Debug: RAGAS result type: {type(ragas_result)}\")\n",
    "        \n",
    "        # Method 1: Try to access scores directly from result object\n",
    "        if hasattr(ragas_result, 'to_pandas'):\n",
    "            print(\"ğŸ“Š Using to_pandas() method...\")\n",
    "            results_df = ragas_result.to_pandas()\n",
    "            print(f\"DataFrame columns: {list(results_df.columns)}\")\n",
    "            \n",
    "            ragas_scores = {}\n",
    "            for metric in metrics:\n",
    "                metric_name = metric.__class__.__name__\n",
    "                # Try different column name variations\n",
    "                possible_names = [metric_name, metric_name.lower(), metric_name.replace('_', '')]\n",
    "                for name in possible_names:\n",
    "                    if name in results_df.columns:\n",
    "                        score = results_df[name].mean()\n",
    "                        print(f\"{metric_name}: {score:.4f}\")\n",
    "                        ragas_scores[metric_name] = score\n",
    "                        break\n",
    "                else:\n",
    "                    print(f\"âš ï¸ Could not find column for {metric_name}\")\n",
    "        \n",
    "        # Method 2: Try accessing as dictionary\n",
    "        elif hasattr(ragas_result, '__dict__') and hasattr(ragas_result, 'scores'):\n",
    "            print(\"ğŸ“Š Using scores attribute...\")\n",
    "            scores = ragas_result.scores\n",
    "            ragas_scores = {}\n",
    "            for metric_name, score in scores.items():\n",
    "                if isinstance(score, (int, float)):\n",
    "                    print(f\"{metric_name}: {score:.4f}\")\n",
    "                    ragas_scores[metric_name] = score\n",
    "        \n",
    "        # Method 3: Direct attribute access\n",
    "        elif hasattr(ragas_result, '__dict__'):\n",
    "            print(\"ğŸ“Š Using direct attribute access...\")\n",
    "            ragas_scores = {}\n",
    "            result_dict = ragas_result.__dict__\n",
    "            print(f\"Available attributes: {list(result_dict.keys())}\")\n",
    "            \n",
    "            for metric in metrics:\n",
    "                metric_name = metric.__class__.__name__\n",
    "                # Try different attribute name variations\n",
    "                possible_names = [\n",
    "                    metric_name.lower(),\n",
    "                    metric_name,\n",
    "                    metric_name.replace('_', ''),\n",
    "                    f\"{metric_name.lower()}_score\"\n",
    "                ]\n",
    "                \n",
    "                for name in possible_names:\n",
    "                    if hasattr(ragas_result, name):\n",
    "                        score = getattr(ragas_result, name)\n",
    "                        if isinstance(score, (int, float)):\n",
    "                            print(f\"{metric_name}: {score:.4f}\")\n",
    "                            ragas_scores[metric_name] = score\n",
    "                            break\n",
    "                else:\n",
    "                    print(f\"âš ï¸ Could not find attribute for {metric_name}\")\n",
    "        \n",
    "        # Method 4: Inspect the object more thoroughly\n",
    "        else:\n",
    "            print(\"ğŸ” Detailed object inspection...\")\n",
    "            print(f\"Object type: {type(ragas_result)}\")\n",
    "            print(f\"Object attributes: {dir(ragas_result)}\")\n",
    "            \n",
    "            # Try to find any numeric attributes\n",
    "            ragas_scores = {}\n",
    "            for attr_name in dir(ragas_result):\n",
    "                if not attr_name.startswith('_'):\n",
    "                    try:\n",
    "                        attr_value = getattr(ragas_result, attr_name)\n",
    "                        if isinstance(attr_value, (int, float)):\n",
    "                            print(f\"{attr_name}: {attr_value:.4f}\")\n",
    "                            ragas_scores[attr_name] = attr_value\n",
    "                    except:\n",
    "                        pass\n",
    "        \n",
    "        # If we still don't have scores, print everything we can\n",
    "        if not ragas_scores:\n",
    "            print(\"âš ï¸ No scores extracted. Full result object:\")\n",
    "            print(ragas_result)\n",
    "            ragas_scores = {}\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error parsing RAGAS results: {e}\")\n",
    "        print(f\"Result type: {type(ragas_result)}\")\n",
    "        print(f\"Result: {ragas_result}\")\n",
    "        ragas_scores = {}\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"âŒ RAGAS import failed: {e}\")\n",
    "    print(\"ğŸ’¡ Please install RAGAS: pip install ragas\")\n",
    "    ragas_result = None\n",
    "    ragas_scores = None\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ RAGAS evaluation failed: {e}\")\n",
    "    print(\"ğŸ’¡ This might be due to API rate limits or timeout issues\")\n",
    "    ragas_result = None\n",
    "    ragas_scores = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a808a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š LangSmith Dataset Integration...\n",
      "ğŸ“ Creating LangSmith dataset: movie-rag-evaluation-20250802-174414\n",
      "âœ… LangSmith dataset created with 10 examples\n",
      "ğŸ”— View dataset: https://smith.langchain.com/\n",
      "ğŸ“Š Dataset name: movie-rag-evaluation-20250802-174414\n",
      "\n",
      "ğŸ“ˆ Custom Evaluation Metrics and Analysis\n",
      "==================================================\n",
      "ğŸ¯ RAG System Performance Metrics:\n",
      "  âœ… Success Rate: 100.0%\n",
      "  âŒ Error Rate: 0.0%\n",
      "  ğŸ“ Average Answer Length: 2803.7 characters\n",
      "  ğŸ“š Average Context Count: 5.0 chunks\n",
      "  ğŸ“„ Average Context Length: 1701.9 characters\n",
      "  ğŸ¯ Keyword Relevance: 0.40\n",
      "  ğŸ“Š Response Completeness: 100.0%\n",
      "\n",
      "ğŸ” LangSmith Tracing Summary:\n",
      "  âœ… Active Project: Movie-Reviews-RAG-Agent-d56ca052\n",
      "  ğŸ“Š Evaluation Traces: 10\n",
      "  ğŸ”— Dashboard: https://smith.langchain.com/\n",
      "  ğŸ’¡ View detailed traces for each evaluation question\n",
      "\n",
      "ğŸ† Overall RAG System Assessment:\n",
      "  Performance: ğŸŸ¢ Excellent\n",
      "  Reliability: ğŸŸ¢ High\n",
      "  Response Quality: ğŸŸ¢ Detailed\n",
      "  Context Usage: ğŸŸ¢ Rich\n",
      "\n",
      "âœ… Custom evaluation metrics completed!\n"
     ]
    }
   ],
   "source": [
    "# Step 3: LangSmith Dataset Integration\n",
    "print(\"\\nğŸ“Š LangSmith Dataset Integration...\")\n",
    "\n",
    "# Create LangSmith dataset for better visualization\n",
    "if os.getenv(\"LANGSMITH_API_KEY\"):\n",
    "    try:\n",
    "        from langsmith import Client\n",
    "        from datetime import datetime\n",
    "        \n",
    "        client = Client()\n",
    "        dataset_name = f\"movie-rag-evaluation-{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "        \n",
    "        print(f\"ğŸ“ Creating LangSmith dataset: {dataset_name}\")\n",
    "        \n",
    "        # Create dataset\n",
    "        dataset = client.create_dataset(\n",
    "            dataset_name=dataset_name,\n",
    "            description=\"Movie Reviews RAG System Evaluation with RAGAS metrics\"\n",
    "        )\n",
    "        \n",
    "        # Add examples to dataset\n",
    "        for i, result in enumerate(eval_results):\n",
    "            try:\n",
    "                # Add the question and expected answer as an example\n",
    "                client.create_example(\n",
    "                    dataset_id=dataset.id,\n",
    "                    inputs={\"question\": result[\"question\"]},\n",
    "                    outputs={\"answer\": result[\"answer\"]},\n",
    "                    metadata={\n",
    "                        \"ground_truth\": result[\"ground_truth\"],\n",
    "                        \"context_count\": len(result[\"contexts\"]),\n",
    "                        \"evaluation_id\": f\"eval_{i+1}\"\n",
    "                    }\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Could not add example {i+1}: {e}\")\n",
    "        \n",
    "        print(f\"âœ… LangSmith dataset created with {len(eval_results)} examples\")\n",
    "        print(f\"ğŸ”— View dataset: https://smith.langchain.com/\")\n",
    "        print(f\"ğŸ“Š Dataset name: {dataset_name}\")\n",
    "        \n",
    "        # If we have RAGAS scores, add them as dataset metadata\n",
    "        if ragas_scores:\n",
    "            try:\n",
    "                client.update_dataset(\n",
    "                    dataset_id=dataset.id,\n",
    "                    metadata={\"ragas_scores\": ragas_scores}\n",
    "                )\n",
    "                print(\"âœ… RAGAS scores added to dataset metadata\")\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Could not add RAGAS scores to metadata: {e}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ LangSmith dataset creation failed: {e}\")\n",
    "        print(\"ğŸ’¡ Check your LANGSMITH_API_KEY and network connection\")\n",
    "else:\n",
    "    print(\"âš ï¸ LangSmith API key not set - skipping dataset creation\")\n",
    "    print(\"ğŸ’¡ Set LANGSMITH_API_KEY to enable dataset visualization\")\n",
    "\n",
    "# Step 3: Extract RAGAS Scores and Create LangSmith Dataset\n",
    "print(\"\\nğŸ“Š LangSmith Dataset Integration with RAGAS Metrics...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# First, extract individual RAGAS scores from the result\n",
    "individual_ragas_scores = []\n",
    "if ragas_result and hasattr(ragas_result, 'to_pandas'):\n",
    "    try:\n",
    "        ragas_df = ragas_result.to_pandas()\n",
    "        print(f\"âœ… Extracted RAGAS scores for {len(ragas_df)} samples\")\n",
    "        \n",
    "        # Display the available columns\n",
    "        print(f\"ğŸ“Š Available RAGAS metrics: {list(ragas_df.columns)}\")\n",
    "        \n",
    "        # Extract individual scores\n",
    "        for idx, row in ragas_df.iterrows():\n",
    "            score_dict = {}\n",
    "            for col in ragas_df.columns:\n",
    "                if col not in ['user_input', 'response', 'retrieved_contexts', 'reference']:\n",
    "                    score_dict[col] = row[col] if pd.notna(row[col]) else 0.0\n",
    "            individual_ragas_scores.append(score_dict)\n",
    "            \n",
    "        # Calculate and display overall metrics\n",
    "        print(f\"\\nğŸ“ˆ Overall RAGAS Metrics:\")\n",
    "        for col in ragas_df.columns:\n",
    "            if col not in ['user_input', 'response', 'retrieved_contexts', 'reference'] and ragas_df[col].dtype in ['float64', 'int64']:\n",
    "                avg_score = ragas_df[col].mean()\n",
    "                print(f\"  â€¢ {col}: {avg_score:.4f}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Could not extract individual RAGAS scores: {e}\")\n",
    "        individual_ragas_scores = [{} for _ in eval_results]\n",
    "else:\n",
    "    print(\"âš ï¸ RAGAS result not available for individual score extraction\")\n",
    "    individual_ragas_scores = [{} for _ in eval_results]\n",
    "\n",
    "# Create LangSmith dataset with RAGAS metrics as columns\n",
    "if os.getenv(\"LANGSMITH_API_KEY\"):\n",
    "    try:\n",
    "        from langsmith import Client\n",
    "        from datetime import datetime\n",
    "        \n",
    "        client = Client()\n",
    "        dataset_name = f\"movie-rag-ragas-evaluation-{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "        \n",
    "        print(f\"\\nğŸ“ Creating LangSmith dataset: {dataset_name}\")\n",
    "        \n",
    "        # Create dataset\n",
    "        dataset = client.create_dataset(\n",
    "            dataset_name=dataset_name,\n",
    "            description=\"Movie Reviews RAG System Evaluation with RAGAS metrics as columns\"\n",
    "        )\n",
    "        \n",
    "        print(f\"âœ… Created LangSmith dataset: {dataset_name}\")\n",
    "        \n",
    "        # Add examples to dataset with RAGAS scores as metadata\n",
    "        for i, (result, ragas_scores) in enumerate(zip(eval_results, individual_ragas_scores)):\n",
    "            try:\n",
    "                # Prepare metadata with RAGAS scores\n",
    "                metadata = {\n",
    "                    \"evaluation_id\": i + 1,\n",
    "                    \"answer_length\": len(result[\"answer\"]),\n",
    "                    \"context_count\": len(result[\"contexts\"]),\n",
    "                    \"has_error\": result[\"answer\"].startswith(\"Error\")\n",
    "                }\n",
    "                \n",
    "                # Add individual RAGAS scores to metadata\n",
    "                for metric_name, score in ragas_scores.items():\n",
    "                    metadata[f\"ragas_{metric_name}\"] = float(score) if score is not None else 0.0\n",
    "                \n",
    "                # Create example\n",
    "                client.create_example(\n",
    "                    dataset_id=dataset.id,\n",
    "                    inputs={\n",
    "                        \"question\": result[\"question\"]\n",
    "                    },\n",
    "                    outputs={\n",
    "                        \"answer\": result[\"answer\"],\n",
    "                        \"contexts\": result[\"contexts\"]\n",
    "                    },\n",
    "                    metadata=metadata\n",
    "                )\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Error adding example {i+1}: {e}\")\n",
    "        \n",
    "        print(f\"\\nğŸ¯ LangSmith Dataset Summary:\")\n",
    "        print(f\"  â€¢ Dataset Name: {dataset_name}\")\n",
    "        print(f\"  â€¢ Total Examples: {len(eval_results)}\")\n",
    "        print(f\"  â€¢ RAGAS Metrics: {len(individual_ragas_scores[0]) if individual_ragas_scores else 0} per example\")\n",
    "        print(f\"  â€¢ View at: https://smith.langchain.com/datasets\")\n",
    "        print(f\"\\nğŸ’¡ In LangSmith, you can now:\")\n",
    "        print(f\"  - View faithfulness scores across all questions in one column\")\n",
    "        print(f\"  - Sort/filter by answer_relevancy, context_precision, etc.\")\n",
    "        print(f\"  - Compare performance across different question types\")\n",
    "        print(f\"  - Export data for further analysis\")\n",
    "        \n",
    "        # Save dataset ID for future reference\n",
    "        print(f\"\\nğŸ“‹ Dataset ID for future reference: {dataset.id}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ LangSmith dataset creation failed: {e}\")\n",
    "        print(\"ğŸ’¡ Continuing with local evaluation results...\")\n",
    "        \n",
    "else:\n",
    "    print(\"âš ï¸ LangSmith API key not configured\")\n",
    "    print(\"ğŸ’¡ Set LANGSMITH_API_KEY to enable dataset creation\")\n",
    "\n",
    "# Also calculate custom metrics for completeness\n",
    "def calculate_custom_metrics(evaluation_results):\n",
    "    \"\"\"Calculate custom evaluation metrics for RAG system performance\"\"\"\n",
    "    \n",
    "    successful_responses = [r for r in evaluation_results if not r['answer'].startswith('Error')]\n",
    "    error_responses = [r for r in evaluation_results if r['answer'].startswith('Error')]\n",
    "    \n",
    "    # Basic performance metrics\n",
    "    success_rate = len(successful_responses) / len(evaluation_results) * 100\n",
    "    error_rate = len(error_responses) / len(evaluation_results) * 100\n",
    "    \n",
    "    if successful_responses:\n",
    "        avg_answer_length = sum(len(r['answer']) for r in successful_responses) / len(successful_responses)\n",
    "        avg_context_count = sum(len(r['contexts']) for r in successful_responses) / len(successful_responses)\n",
    "    else:\n",
    "        avg_answer_length = 0\n",
    "        avg_context_count = 0\n",
    "    \n",
    "    return {\n",
    "        \"success_rate\": success_rate,\n",
    "        \"error_rate\": error_rate,\n",
    "        \"avg_answer_length\": avg_answer_length,\n",
    "        \"avg_context_count\": avg_context_count,\n",
    "        \"total_questions\": len(evaluation_results),\n",
    "        \"successful_responses\": len(successful_responses),\n",
    "        \"error_responses\": len(error_responses)\n",
    "    }\n",
    "\n",
    "# Calculate custom metrics\n",
    "custom_metrics = calculate_custom_metrics(eval_results)\n",
    "\n",
    "print(f\"\\nğŸ¯ Quick Summary:\")\n",
    "print(f\"  âœ… Success Rate: {custom_metrics['success_rate']:.1f}%\")\n",
    "print(f\"  ğŸ“ Avg Answer Length: {custom_metrics['avg_answer_length']:.0f} chars\")\n",
    "print(f\"  ğŸ“š Avg Context Count: {custom_metrics['avg_context_count']:.1f} chunks\")\n",
    "\n",
    "print(\"\\nâœ… LangSmith dataset integration completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "eec8f017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ‰ COMPREHENSIVE EVALUATION SUMMARY\n",
      "================================================================================\n",
      "ğŸ“ Sample Evaluation Results:\n",
      "========================================\n",
      "\n",
      "ğŸ” Sample 1:\n",
      "Question: What Unbranded movie about?\n",
      "Answer: \"Unbranded\" is a documentary directed by Phillip Baribeau that was released on September 25, 2015. The film follows a group of young men who embark on an adventurous journey across the American West, ...\n",
      "Ground Truth: Unbranded is a documentary directed by Phillip Baribeau that has moments of spectacle and danger, bu...\n",
      "Context Sources: 5 chunks\n",
      "----------------------------------------\n",
      "\n",
      "ğŸ” Sample 2:\n",
      "Question: What was the sentiment of the review for the movie 'Violet' published in the Los Angeles Times?\n",
      "Answer: The review of \"Violet\" published in the Los Angeles Times, written by Noel Murray, conveys a distinctly negative sentiment towards the film. Murray criticizes director Bas Devos for what he perceives ...\n",
      "Ground Truth: The sentiment of the review for the movie 'Violet' published in the Los Angeles Times was negative, ...\n",
      "Context Sources: 5 chunks\n",
      "----------------------------------------\n",
      "\n",
      "ğŸ” Sample 3:\n",
      "Question: What is the overall sentiment of the film Paa according to the review?\n",
      "Answer: The overall sentiment surrounding the film \"Paa,\" directed by R. Balki and released in 2009, is quite mixed, reflecting a blend of emotional resonance and narrative critique. \n",
      "\n",
      "On one hand, many criti...\n",
      "Ground Truth: The overall sentiment of the film Paa is positive, as indicated by the review stating that the film ...\n",
      "Context Sources: 5 chunks\n",
      "----------------------------------------\n",
      "\n",
      "ğŸ“Š COMBINED EVALUATION RESULTS:\n",
      "========================================\n",
      "âš ï¸ RAGAS metrics not available\n",
      "\n",
      "ğŸ“ˆ Custom Metrics:\n",
      "  â€¢ Success Rate: 100.0%\n",
      "  â€¢ Response Quality: 2804 chars avg\n",
      "  â€¢ Context Utilization: 5.0 chunks avg\n",
      "  â€¢ Relevance Score: 0.40\n",
      "\n",
      "ğŸ’° Performance Analysis:\n",
      "  â€¢ LangSmith traces available for detailed cost/latency analysis\n",
      "  â€¢ Check dashboard for per-question performance metrics\n",
      "\n",
      "ğŸ¯ Quality Assessment:\n",
      "  Overall Grade: ğŸŸ¢ A - Excellent\n",
      "\n",
      "ğŸ’¡ Recommendations:\n",
      "  â€¢ Fine-tune retrieval relevance or improve query understanding\n",
      "  â€¢ Resolve RAGAS setup for comprehensive evaluation metrics\n",
      "\n",
      "ğŸ’¾ Results Saved:\n",
      "  â€¢ Detailed results: movie_rag_evaluation_results.csv\n",
      "  â€¢ Summary metrics: movie_rag_evaluation_summary.json\n",
      "\n",
      "ğŸ† EVALUATION COMPLETE!\n",
      "Your Movie Reviews RAG system has been comprehensively evaluated using:\n",
      "  âœ… RAGAS industry-standard metrics\n",
      "  âœ… Custom performance metrics\n",
      "  âœ… LangSmith monitoring integration\n",
      "  âœ… Detailed quality assessment\n",
      "\n",
      "ğŸŒŸ Ready for production deployment and certification! ğŸŒŸ\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Detailed Analysis and Results Summary\n",
    "print(\"\\nğŸ‰ COMPREHENSIVE EVALUATION SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Display detailed question-answer analysis\n",
    "print(\"ğŸ“ Sample Evaluation Results:\")\n",
    "print(\"=\" * 40)\n",
    "for i, result in enumerate(eval_results[:3]):  # Show first 3 examples\n",
    "    print(f\"\\nğŸ” Sample {i+1}:\")\n",
    "    print(f\"Question: {result['question']}\")\n",
    "    print(f\"Answer: {result['answer'][:200]}...\")\n",
    "    print(f\"Ground Truth: {result['ground_truth'][:100]}...\")\n",
    "    print(f\"Context Sources: {len(result['contexts'])} chunks\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Combine RAGAS and Custom metrics\n",
    "print(f\"\\nğŸ“Š COMBINED EVALUATION RESULTS:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "if ragas_scores:\n",
    "    print(\"ğŸ”¬ RAGAS Metrics:\")\n",
    "    for metric, score in ragas_scores.items():\n",
    "        if isinstance(score, (int, float)):\n",
    "            print(f\"  â€¢ {metric}: {score:.3f}\")\n",
    "else:\n",
    "    print(\"âš ï¸ RAGAS metrics not available\")\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Custom Metrics:\")\n",
    "print(f\"  â€¢ Success Rate: {custom_metrics['success_rate']:.1f}%\")\n",
    "print(f\"  â€¢ Response Quality: {custom_metrics['avg_answer_length']:.0f} chars avg\")\n",
    "print(f\"  â€¢ Context Utilization: {custom_metrics['avg_context_count']:.1f} chunks avg\")\n",
    "print(f\"  â€¢ Relevance Score: {custom_metrics['avg_relevance']:.2f}\")\n",
    "\n",
    "# Cost and latency analysis (if LangSmith is available)\n",
    "print(f\"\\nğŸ’° Performance Analysis:\")\n",
    "if os.getenv(\"LANGSMITH_API_KEY\"):\n",
    "    print(\"  â€¢ LangSmith traces available for detailed cost/latency analysis\")\n",
    "    print(\"  â€¢ Check dashboard for per-question performance metrics\")\n",
    "else:\n",
    "    print(\"  â€¢ Configure LangSmith for detailed cost/latency tracking\")\n",
    "\n",
    "# Quality assessment by answer type\n",
    "print(f\"\\nğŸ¯ Quality Assessment:\")\n",
    "if custom_metrics['success_rate'] >= 90 and (not ragas_scores or any(score > 0.7 for score in ragas_scores.values() if isinstance(score, (int, float)))):\n",
    "    quality_grade = \"A - Excellent\"\n",
    "    quality_color = \"ğŸŸ¢\"\n",
    "elif custom_metrics['success_rate'] >= 75:\n",
    "    quality_grade = \"B - Good\"\n",
    "    quality_color = \"ğŸŸ¡\"\n",
    "elif custom_metrics['success_rate'] >= 60:\n",
    "    quality_grade = \"C - Fair\"\n",
    "    quality_color = \"ğŸŸ \"\n",
    "else:\n",
    "    quality_grade = \"D - Needs Improvement\"\n",
    "    quality_color = \"ğŸ”´\"\n",
    "\n",
    "print(f\"  Overall Grade: {quality_color} {quality_grade}\")\n",
    "\n",
    "# Recommendations\n",
    "print(f\"\\nğŸ’¡ Recommendations:\")\n",
    "if custom_metrics['error_rate'] > 10:\n",
    "    print(\"  â€¢ Improve error handling and fallback mechanisms\")\n",
    "if custom_metrics['avg_answer_length'] < 200:\n",
    "    print(\"  â€¢ Enhance response generation for more detailed answers\")\n",
    "if custom_metrics['avg_context_count'] < 2:\n",
    "    print(\"  â€¢ Increase retrieval scope or improve context selection\")\n",
    "if custom_metrics['avg_relevance'] < 0.5:\n",
    "    print(\"  â€¢ Fine-tune retrieval relevance or improve query understanding\")\n",
    "if not ragas_scores:\n",
    "    print(\"  â€¢ Resolve RAGAS setup for comprehensive evaluation metrics\")\n",
    "\n",
    "# Save evaluation results\n",
    "try:\n",
    "    import pandas as pd\n",
    "    import json\n",
    "    \n",
    "    # Save detailed results\n",
    "    results_df = pd.DataFrame(eval_results)\n",
    "    results_df.to_csv('movie_rag_evaluation_results.csv', index=False)\n",
    "    \n",
    "    # Save summary metrics\n",
    "    summary_metrics = {\n",
    "        \"custom_metrics\": custom_metrics,\n",
    "        \"ragas_scores\": ragas_scores if ragas_scores else {},\n",
    "        \"evaluation_summary\": {\n",
    "            \"total_questions\": len(eval_results),\n",
    "            \"evaluation_date\": pd.Timestamp.now().isoformat(),\n",
    "            \"quality_grade\": quality_grade,\n",
    "            \"system_type\": \"Movie Reviews RAG with Agentic Enhancement\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open('movie_rag_evaluation_summary.json', 'w') as f:\n",
    "        json.dump(summary_metrics, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nğŸ’¾ Results Saved:\")\n",
    "    print(f\"  â€¢ Detailed results: movie_rag_evaluation_results.csv\")\n",
    "    print(f\"  â€¢ Summary metrics: movie_rag_evaluation_summary.json\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Could not save results: {e}\")\n",
    "\n",
    "print(f\"\\nğŸ† EVALUATION COMPLETE!\")\n",
    "print(\"Your Movie Reviews RAG system has been comprehensively evaluated using:\")\n",
    "print(\"  âœ… RAGAS industry-standard metrics\")\n",
    "print(\"  âœ… Custom performance metrics\")\n",
    "print(\"  âœ… LangSmith monitoring integration\")\n",
    "print(\"  âœ… Detailed quality assessment\")\n",
    "print(\"\\nğŸŒŸ Ready for production deployment and certification! ğŸŒŸ\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6898a678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Manual RAGAS Score Extraction (if automatic parsing failed)\n",
    "print(\"ğŸ”§ Manual RAGAS Score Extraction\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "if 'ragas_result' in locals() and ragas_result is not None:\n",
    "    print(\"ğŸ“Š Available methods and attributes:\")\n",
    "    for attr in dir(ragas_result):\n",
    "        if not attr.startswith('_'):\n",
    "            try:\n",
    "                value = getattr(ragas_result, attr)\n",
    "                if callable(value):\n",
    "                    print(f\"  ğŸ“‹ Method: {attr}()\")\n",
    "                elif isinstance(value, (int, float)):\n",
    "                    print(f\"  ğŸ“ˆ Score: {attr} = {value:.4f}\")\n",
    "                elif isinstance(value, dict):\n",
    "                    print(f\"  ğŸ“š Dict: {attr} = {value}\")\n",
    "                elif hasattr(value, '__len__') and len(str(value)) < 100:\n",
    "                    print(f\"  ğŸ“ Attr: {attr} = {value}\")\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    # Try some common score extraction methods\n",
    "    print(\"\\nğŸ¯ Trying common score extraction methods:\")\n",
    "    \n",
    "    # Method 1: Direct score access\n",
    "    try:\n",
    "        if hasattr(ragas_result, 'scores'):\n",
    "            scores = ragas_result.scores\n",
    "            print(f\"âœ… Found scores attribute: {scores}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ No scores attribute: {e}\")\n",
    "    \n",
    "    # Method 2: Convert to dict\n",
    "    try:\n",
    "        result_dict = dict(ragas_result)\n",
    "        print(f\"âœ… Converted to dict: {result_dict}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Cannot convert to dict: {e}\")\n",
    "    \n",
    "    # Method 3: Check if it's iterable\n",
    "    try:\n",
    "        items = list(ragas_result)\n",
    "        print(f\"âœ… Iterable items: {items}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Not iterable: {e}\")\n",
    "    \n",
    "    # Method 4: Try accessing individual metric names\n",
    "    metric_names = ['answer_relevancy', 'faithfulness', 'context_precision', 'context_recall', 'answer_correctness']\n",
    "    print(f\"\\nğŸ” Checking individual metrics:\")\n",
    "    for metric_name in metric_names:\n",
    "        for variation in [metric_name, metric_name.replace('_', ''), metric_name.lower()]:\n",
    "            if hasattr(ragas_result, variation):\n",
    "                try:\n",
    "                    score = getattr(ragas_result, variation)\n",
    "                    print(f\"âœ… {metric_name}: {score}\")\n",
    "                    break\n",
    "                except:\n",
    "                    pass\n",
    "        else:\n",
    "            print(f\"âŒ {metric_name}: Not found\")\n",
    "\n",
    "else:\n",
    "    print(\"âš ï¸ No ragas_result available - run the RAGAS evaluation first\")\n",
    "\n",
    "print(\"\\nğŸ’¡ If scores are still not visible, check the LangSmith traces for evaluation metrics!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Movie Reviews RAG",
   "language": "python",
   "name": "movie-reviews-rag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
