{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "729f103d",
   "metadata": {},
   "source": [
    "# Movie Reviews RAG Agentic Solution üçÖ\n",
    "\n",
    "This notebook implements an end-to-end RAG (Retrieval Augmented Generation) system for analyzing movie reviews from the **Rotten Tomatoes dataset** - providing access to professional critic reviews and comprehensive movie metadata.\n",
    "\n",
    "## Overview\n",
    "\n",
    "We'll build a system that can:\n",
    "- Load and process movie review data from Rotten Tomatoes\n",
    "- Generate embeddings for review text with rich metadata\n",
    "- Implement semantic search for relevant reviews\n",
    "- Generate intelligent responses to movie-related queries\n",
    "- Provide insights and analysis based on professional critic reviews\n",
    "- Leverage Tomatometer scores, audience ratings, and critic consensus\n",
    "\n",
    "## Data Sources \n",
    "- **Rotten Tomatoes Movies** (17MB): Movie metadata with titles, ratings, genres, directors, runtime, release dates, etc.\n",
    "- **Rotten Tomatoes Reviews** (392MB): Professional critic reviews with scores, sentiment, publications, and detailed text\n",
    "\n",
    "## Key Features ‚ú®\n",
    "- **Rich Movie Metadata**: Genre, director, runtime, release dates, ratings\n",
    "- **Professional Critics**: Reviews from top critics and publications\n",
    "- **Tomatometer & Audience Scores**: Official Rotten Tomatoes scoring system\n",
    "- **Fresh/Rotten Classification**: Review state analysis\n",
    "- **Sentiment Analysis**: Positive/negative review sentiment\n",
    "- **Multi-tool Agent**: Specialized tools for different types of analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948da4d4",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 1: Environment Setup and Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "510b9b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Optional\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Apply nest_asyncio for Jupyter compatibility\n",
    "nest_asyncio.apply()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e97031d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîë Setting up API Keys\n",
      "========================================\n",
      "‚úÖ OpenAI API key already set\n",
      "‚úÖ Tavily API key already set\n",
      "‚úÖ SerpAPI key already set\n",
      "‚úÖ LangSmith API key already set\n",
      "\n",
      "üéØ API Key Setup Complete!\n",
      "\n",
      "üìã Where to get API keys:\n",
      "‚Ä¢ OpenAI: https://platform.openai.com/api-keys\n",
      "‚Ä¢ Tavily: https://tavily.com/ (free tier available)\n",
      "‚Ä¢ SerpAPI: https://serpapi.com/ (free tier available)\n",
      "‚Ä¢ LangSmith: https://smith.langchain.com/ (optional monitoring)\n"
     ]
    }
   ],
   "source": [
    "# API Keys Setup\n",
    "import os\n",
    "import getpass\n",
    "\n",
    "print(\"üîë Setting up API Keys\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# OpenAI API Key (required)\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"ü§ñ Enter your OpenAI API Key: \")\n",
    "    print(\"‚úÖ OpenAI API key set\")\n",
    "else:\n",
    "    print(\"‚úÖ OpenAI API key already set\")\n",
    "\n",
    "# Tavily API Key (recommended for external search)\n",
    "if not os.getenv(\"TAVILY_API_KEY\"):\n",
    "    tavily_key = getpass.getpass(\"üîç Enter your Tavily API Key (or press Enter to skip): \")\n",
    "    if tavily_key.strip():\n",
    "        os.environ[\"TAVILY_API_KEY\"] = tavily_key\n",
    "        print(\"‚úÖ Tavily API key set\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Tavily API key skipped - external search will be limited\")\n",
    "else:\n",
    "    print(\"‚úÖ Tavily API key already set\")\n",
    "\n",
    "# SerpAPI Key (optional backup for external search)\n",
    "if not os.getenv(\"SERPAPI_API_KEY\"):\n",
    "    serp_key = getpass.getpass(\"üåê Enter your SerpAPI Key (or press Enter to skip): \")\n",
    "    if serp_key.strip():\n",
    "        os.environ[\"SERPAPI_API_KEY\"] = serp_key\n",
    "        print(\"‚úÖ SerpAPI key set\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è SerpAPI key skipped - will use Tavily or fallback\")\n",
    "else:\n",
    "    print(\"‚úÖ SerpAPI key already set\")\n",
    "\n",
    "# LangSmith API Key (optional for monitoring)\n",
    "if not os.getenv(\"LANGSMITH_API_KEY\"):\n",
    "    langsmith_key = getpass.getpass(\"üìä Enter your LangSmith API Key (or press Enter to skip): \")\n",
    "    if langsmith_key.strip():\n",
    "        os.environ[\"LANGSMITH_API_KEY\"] = langsmith_key\n",
    "        os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "        os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "        os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "        os.environ[\"LANGSMITH_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "        print(\"‚úÖ LangSmith API key set and tracing enabled\")\n",
    "    else:\n",
    "        os.environ[\"LANGSMITH_TRACING\"] = \"false\"\n",
    "        os.environ[\"LANGCHAIN_TRACING_V2\"] = \"false\"\n",
    "        print(\"‚ö†Ô∏è LangSmith skipped - no monitoring/tracing\")\n",
    "else:\n",
    "    print(\"‚úÖ LangSmith API key already set\")\n",
    "\n",
    "print(\"\\nüéØ API Key Setup Complete!\")\n",
    "print(\"\\nüìã Where to get API keys:\")\n",
    "print(\"‚Ä¢ OpenAI: https://platform.openai.com/api-keys\")\n",
    "print(\"‚Ä¢ Tavily: https://tavily.com/ (free tier available)\")\n",
    "print(\"‚Ä¢ SerpAPI: https://serpapi.com/ (free tier available)\")\n",
    "print(\"‚Ä¢ LangSmith: https://smith.langchain.com/ (optional monitoring)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ed4a4e",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 2: Data Loading and Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7219d64f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üçÖ Loading Rotten Tomatoes movie review datasets...\n",
      "Loading Rotten Tomatoes movies metadata...\n",
      "  Trying encoding: utf-8\n",
      "  ‚úÖ Success with utf-8\n",
      "Movies dataset: 143258 movies\n",
      "Columns: ['id', 'title', 'audienceScore', 'tomatoMeter', 'rating', 'ratingContents', 'releaseDateTheaters', 'releaseDateStreaming', 'runtimeMinutes', 'genre', 'originalLanguage', 'director', 'writer', 'boxOffice', 'distributor', 'soundMix']\n",
      "\n",
      "Loading Rotten Tomatoes reviews...\n",
      "  Trying encoding: utf-8\n",
      "  ‚úÖ Success with utf-8\n",
      "Reviews dataset: 1444963 reviews\n",
      "Columns: ['id', 'reviewId', 'creationDate', 'criticName', 'isTopCritic', 'originalScore', 'reviewState', 'publicatioName', 'reviewText', 'scoreSentiment', 'reviewUrl']\n",
      "\n",
      "üé¨ Sample movies metadata:\n",
      "                   id                title  audienceScore  tomatoMeter rating  \\\n",
      "0  space-zombie-bingo  Space Zombie Bingo!           50.0          NaN    NaN   \n",
      "1     the_green_grass      The Green Grass            NaN          NaN    NaN   \n",
      "2           love_lies           Love, Lies           43.0          NaN    NaN   \n",
      "\n",
      "  ratingContents releaseDateTheaters releaseDateStreaming  runtimeMinutes  \\\n",
      "0            NaN                 NaN           2018-08-25            75.0   \n",
      "1            NaN                 NaN           2020-02-11           114.0   \n",
      "2            NaN                 NaN                  NaN           120.0   \n",
      "\n",
      "                    genre originalLanguage                       director  \\\n",
      "0  Comedy, Horror, Sci-fi          English                  George Ormrod   \n",
      "1                   Drama          English                Tiffany Edwards   \n",
      "2                   Drama           Korean  Park Heung-Sik,Heung-Sik Park   \n",
      "\n",
      "                                   writer boxOffice distributor soundMix  \n",
      "0              George Ormrod,John Sabotta       NaN         NaN      NaN  \n",
      "1                         Tiffany Edwards       NaN         NaN      NaN  \n",
      "2  Ha Young-Joon,Jeon Yun-su,Song Hye-jin       NaN         NaN      NaN  \n",
      "\n",
      "üìù Sample reviews:\n",
      "                                  id  reviewId creationDate       criticName  \\\n",
      "0                            beavers   1145982   2003-05-23  Ivan M. Lincoln   \n",
      "1                         blood_mask   1636744   2007-06-02    The Foywonder   \n",
      "2  city_hunter_shinjuku_private_eyes   2590987   2019-05-28     Reuben Baron   \n",
      "\n",
      "   isTopCritic originalScore reviewState                 publicatioName  \\\n",
      "0        False         3.5/4       fresh  Deseret News (Salt Lake City)   \n",
      "1        False           1/5      rotten                  Dread Central   \n",
      "2        False           NaN       fresh                            CBR   \n",
      "\n",
      "                                          reviewText scoreSentiment  \\\n",
      "0  Timed to be just long enough for most youngste...       POSITIVE   \n",
      "1  It doesn't matter if a movie costs 300 million...       NEGATIVE   \n",
      "2  The choreography is so precise and lifelike at...       POSITIVE   \n",
      "\n",
      "                                           reviewUrl  \n",
      "0  http://www.deseretnews.com/article/700003233/B...  \n",
      "1  http://www.dreadcentral.com/index.php?name=Rev...  \n",
      "2  https://www.cbr.com/city-hunter-shinjuku-priva...  \n",
      "\n",
      "üìä Dataset Statistics:\n",
      "‚Ä¢ Total movies: 143,258\n",
      "‚Ä¢ Total reviews: 1,444,963\n",
      "‚Ä¢ Average reviews per movie: 10.1\n",
      "‚Ä¢ Unique movie IDs in reviews: 69,263\n",
      "‚Ä¢ Movies with reviews: 69,263 / 143,258\n",
      "\n",
      "üèÜ Review State Distribution:\n",
      "reviewState\n",
      "fresh     963799\n",
      "rotten    481164\n",
      "Name: count, dtype: int64\n",
      "\n",
      "‚≠ê Score Sentiment Distribution:\n",
      "scoreSentiment\n",
      "POSITIVE    963799\n",
      "NEGATIVE    481164\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load the Rotten Tomatoes datasets\n",
    "print(\"üçÖ Loading Rotten Tomatoes movie review datasets...\")\n",
    "\n",
    "# Robust CSV loading function with error handling\n",
    "def load_csv_robust(filepath):\n",
    "    \"\"\"Load CSV with robust error handling for malformed data\"\"\"\n",
    "    encodings = ['utf-8', 'latin1', 'cp1252', 'iso-8859-1']\n",
    "    \n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            print(f\"  Trying encoding: {encoding}\")\n",
    "            # Try with error handling for malformed lines\n",
    "            df = pd.read_csv(\n",
    "                filepath, \n",
    "                encoding=encoding,\n",
    "                on_bad_lines='skip',  # Skip bad lines instead of failing\n",
    "                engine='python',      # Use Python engine for better error handling\n",
    "                quoting=1,           # Quote all fields\n",
    "                skipinitialspace=True\n",
    "            )\n",
    "            print(f\"  ‚úÖ Success with {encoding}\")\n",
    "            return df\n",
    "        except UnicodeDecodeError:\n",
    "            print(f\"  ‚ùå Failed with {encoding}\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Failed with {encoding}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # If all encodings fail, try with minimal options\n",
    "    print(\"  Trying with basic fallback...\")\n",
    "    try:\n",
    "        df = pd.read_csv(filepath, encoding='latin1', on_bad_lines='skip', engine='python')\n",
    "        print(\"  ‚úÖ Success with fallback method\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Could not read {filepath}: {str(e)}\")\n",
    "\n",
    "# Load Rotten Tomatoes movies metadata\n",
    "print(\"Loading Rotten Tomatoes movies metadata...\")\n",
    "movies_df = load_csv_robust(\"data/rotten_tomatoes_movies.csv\")\n",
    "print(f\"Movies dataset: {len(movies_df)} movies\")\n",
    "print(f\"Columns: {list(movies_df.columns)}\")\n",
    "\n",
    "# Load Rotten Tomatoes reviews\n",
    "print(\"\\nLoading Rotten Tomatoes reviews...\")\n",
    "reviews_df = load_csv_robust(\"data/rotten_tomatoes_movie_reviews.csv\")\n",
    "print(f\"Reviews dataset: {len(reviews_df)} reviews\")\n",
    "print(f\"Columns: {list(reviews_df.columns)}\")\n",
    "\n",
    "# Display sample data\n",
    "print(\"\\nüé¨ Sample movies metadata:\")\n",
    "print(movies_df.head(3))\n",
    "\n",
    "print(\"\\nüìù Sample reviews:\")\n",
    "print(reviews_df.head(3))\n",
    "\n",
    "# Basic statistics\n",
    "print(f\"\\nüìä Dataset Statistics:\")\n",
    "print(f\"‚Ä¢ Total movies: {len(movies_df):,}\")\n",
    "print(f\"‚Ä¢ Total reviews: {len(reviews_df):,}\")\n",
    "print(f\"‚Ä¢ Average reviews per movie: {len(reviews_df)/len(movies_df):.1f}\")\n",
    "print(f\"‚Ä¢ Unique movie IDs in reviews: {reviews_df['id'].nunique():,}\")\n",
    "print(f\"‚Ä¢ Movies with reviews: {reviews_df['id'].nunique():,} / {len(movies_df):,}\")\n",
    "\n",
    "# Check review distribution\n",
    "print(f\"\\nüèÜ Review State Distribution:\")\n",
    "if 'reviewState' in reviews_df.columns:\n",
    "    print(reviews_df['reviewState'].value_counts())\n",
    "\n",
    "print(f\"\\n‚≠ê Score Sentiment Distribution:\")\n",
    "if 'scoreSentiment' in reviews_df.columns:\n",
    "    print(reviews_df['scoreSentiment'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3739f952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó Merging movies metadata with reviews...\n",
      "‚úÖ Cleaned reviews dataset: 1364909 reviews\n",
      "‚úÖ Merged dataset: 1388546 reviews with movie metadata\n",
      "‚úÖ Reviews with movie titles: 1383051 / 1388546\n",
      "‚ö†Ô∏è 5495 reviews missing movie titles (will use movie ID)\n",
      "\n",
      "üé¨ Sample merged data:\n",
      "  title_clean criticName_clean  \\\n",
      "0     Beavers  Ivan M. Lincoln   \n",
      "1  Blood Mask    The Foywonder   \n",
      "\n",
      "                                    reviewText_clean rating  genre_clean  \n",
      "0  Timed to be just long enough for most youngste...    NaN  Documentary  \n",
      "1  It doesn't matter if a movie costs 300 million...    NaN               \n"
     ]
    }
   ],
   "source": [
    "# Data cleaning and preprocessing for Rotten Tomatoes data\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and normalize text data\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to string and clean\n",
    "    text = str(text).strip()\n",
    "    \n",
    "    # Remove special characters and normalize\n",
    "    import re\n",
    "    text = re.sub(r'[\\x00-\\x1f\\x7f-\\x9f]', '', text)  # Remove control characters\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Normalize whitespace\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# Clean movies data\n",
    "movies_df['title_clean'] = movies_df['title'].apply(clean_text)\n",
    "movies_df['genre_clean'] = movies_df['genre'].apply(clean_text)\n",
    "movies_df['director_clean'] = movies_df['director'].apply(clean_text)\n",
    "\n",
    "# Clean reviews data\n",
    "reviews_df['reviewText_clean'] = reviews_df['reviewText'].apply(clean_text)\n",
    "reviews_df['criticName_clean'] = reviews_df['criticName'].apply(clean_text)\n",
    "reviews_df['publicatioName_clean'] = reviews_df['publicatioName'].apply(clean_text)\n",
    "\n",
    "# Remove rows with empty review text\n",
    "reviews_df = reviews_df[reviews_df['reviewText_clean'].str.len() > 20].copy()\n",
    "\n",
    "# Merge movies and reviews for complete information\n",
    "print(\"üîó Merging movies metadata with reviews...\")\n",
    "merged_df = reviews_df.merge(movies_df, on='id', how='left')\n",
    "\n",
    "print(f\"‚úÖ Cleaned reviews dataset: {len(reviews_df)} reviews\")\n",
    "print(f\"‚úÖ Merged dataset: {len(merged_df)} reviews with movie metadata\")\n",
    "print(f\"‚úÖ Reviews with movie titles: {merged_df['title'].notna().sum()} / {len(merged_df)}\")\n",
    "\n",
    "# Handle missing titles\n",
    "missing_titles = merged_df['title'].isna().sum()\n",
    "if missing_titles > 0:\n",
    "    print(f\"‚ö†Ô∏è {missing_titles} reviews missing movie titles (will use movie ID)\")\n",
    "    merged_df['title_clean'] = merged_df['title_clean'].fillna(merged_df['id'])\n",
    "\n",
    "print(f\"\\nüé¨ Sample merged data:\")\n",
    "sample_cols = ['title_clean', 'criticName_clean', 'reviewText_clean', 'rating', 'genre_clean']\n",
    "available_cols = [col for col in sample_cols if col in merged_df.columns]\n",
    "print(merged_df[available_cols].head(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8bdea959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üçÖ Creating review documents from Rotten Tomatoes data...\n",
      "üß™ Using sample of 200 reviews for testing...\n",
      "‚úÖ Created 200 total review documents\n",
      "   - Source: Rotten Tomatoes\n",
      "   - Reviews with movie metadata included\n",
      "\n",
      "üìÑ Sample document:\n",
      "Movie: Beavers\n",
      "Genre: Documentary\n",
      "Director: Stephen Low\n",
      "Rating: nan\n",
      "Release Date: nan\n",
      "Critic: Ivan M. Lincoln\n",
      "Publication: Deseret News (Salt Lake City)\n",
      "Score: 3.5/4\n",
      "Review State: fresh\n",
      "Sentiment: POSITIVE\n",
      "Review: Timed to be just long enough for most youngsters' brief attention spans -- and it's pa...\n",
      "\n",
      "üè∑Ô∏è Sample metadata:\n",
      "  source: rotten_tomatoes\n",
      "  movie_id: beavers\n",
      "  movie_title: Beavers\n",
      "  critic_name: Ivan M. Lincoln\n",
      "  publication: Deseret News (Salt Lake City)\n",
      "  review_date: 2003-05-23\n",
      "  original_score: 3.5/4\n",
      "  review_state: fresh\n",
      "\n",
      "üìä Document Statistics:\n",
      "‚Ä¢ Unique movies: 33\n",
      "‚Ä¢ Unique critics: 178\n",
      "‚Ä¢ Average content length: 339 characters\n"
     ]
    }
   ],
   "source": [
    "# Create unified data structure for processing Rotten Tomatoes data\n",
    "def create_review_documents(df, max_reviews=200):\n",
    "    \"\"\"Convert merged DataFrame to list of review documents\"\"\"\n",
    "    documents = []\n",
    "    \n",
    "    # Use a sample for initial testing\n",
    "    if len(df) > max_reviews:\n",
    "        print(f\"üß™ Using sample of {max_reviews} reviews for testing...\")\n",
    "        df_sample = df.head(max_reviews)\n",
    "    else:\n",
    "        df_sample = df\n",
    "    \n",
    "    for idx, row in df_sample.iterrows():\n",
    "        # Create comprehensive metadata\n",
    "        metadata = {\n",
    "            'source': 'rotten_tomatoes',\n",
    "            'movie_id': row.get('id', ''),\n",
    "            'movie_title': row.get('title_clean', row.get('id', 'Unknown')),\n",
    "            'critic_name': row.get('criticName_clean', 'Anonymous'),\n",
    "            'publication': row.get('publicatioName_clean', 'Unknown'),\n",
    "            'review_date': row.get('creationDate', 'Unknown'),\n",
    "            'original_score': row.get('originalScore', ''),\n",
    "            'review_state': row.get('reviewState', ''),\n",
    "            'sentiment': row.get('scoreSentiment', ''),\n",
    "            'is_top_critic': row.get('isTopCritic', False),\n",
    "            'genre': row.get('genre_clean', ''),\n",
    "            'director': row.get('director_clean', ''),\n",
    "            'rating': row.get('rating', ''),\n",
    "            'audience_score': row.get('audienceScore', ''),\n",
    "            'tomato_meter': row.get('tomatoMeter', ''),\n",
    "            'release_date': row.get('releaseDateTheaters', ''),\n",
    "            'runtime': row.get('runtimeMinutes', ''),\n",
    "            'index': idx\n",
    "        }\n",
    "        \n",
    "        # Create rich content for embedding\n",
    "        content = f\"Movie: {row.get('title_clean', row.get('id', 'Unknown'))}\\n\"\n",
    "        \n",
    "        # Add movie metadata\n",
    "        if row.get('genre_clean'):\n",
    "            content += f\"Genre: {row.get('genre_clean')}\\n\"\n",
    "        if row.get('director_clean'):\n",
    "            content += f\"Director: {row.get('director_clean')}\\n\"\n",
    "        if row.get('rating'):\n",
    "            content += f\"Rating: {row.get('rating')}\\n\"\n",
    "        if row.get('releaseDateTheaters'):\n",
    "            content += f\"Release Date: {row.get('releaseDateTheaters')}\\n\"\n",
    "        \n",
    "        # Add review information\n",
    "        content += f\"Critic: {row.get('criticName_clean', 'Anonymous')}\\n\"\n",
    "        if row.get('publicatioName_clean'):\n",
    "            content += f\"Publication: {row.get('publicatioName_clean')}\\n\"\n",
    "        if row.get('originalScore'):\n",
    "            content += f\"Score: {row.get('originalScore')}\\n\"\n",
    "        if row.get('reviewState'):\n",
    "            content += f\"Review State: {row.get('reviewState')}\\n\"\n",
    "        if row.get('scoreSentiment'):\n",
    "            content += f\"Sentiment: {row.get('scoreSentiment')}\\n\"\n",
    "        \n",
    "        # Add the main review text\n",
    "        review_text = row.get('reviewText_clean', '')\n",
    "        if review_text:\n",
    "            content += f\"Review: {review_text}\"\n",
    "        \n",
    "        documents.append({\n",
    "            'content': content,\n",
    "            'metadata': metadata\n",
    "        })\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# Create documents from merged Rotten Tomatoes data\n",
    "print(\"üçÖ Creating review documents from Rotten Tomatoes data...\")\n",
    "all_documents = create_review_documents(merged_df, max_reviews=200)\n",
    "\n",
    "print(f\"‚úÖ Created {len(all_documents)} total review documents\")\n",
    "print(f\"   - Source: Rotten Tomatoes\")\n",
    "print(f\"   - Reviews with movie metadata included\")\n",
    "\n",
    "# Show sample document\n",
    "print(\"\\nüìÑ Sample document:\")\n",
    "print(all_documents[0]['content'][:300] + \"...\")\n",
    "\n",
    "# Show metadata sample\n",
    "print(\"\\nüè∑Ô∏è Sample metadata:\")\n",
    "sample_metadata = all_documents[0]['metadata']\n",
    "for key, value in list(sample_metadata.items())[:8]:  # Show first 8 metadata fields\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Basic statistics\n",
    "print(f\"\\nüìä Document Statistics:\")\n",
    "unique_movies = len(set([doc['metadata']['movie_title'] for doc in all_documents]))\n",
    "unique_critics = len(set([doc['metadata']['critic_name'] for doc in all_documents]))\n",
    "print(f\"‚Ä¢ Unique movies: {unique_movies}\")\n",
    "print(f\"‚Ä¢ Unique critics: {unique_critics}\")\n",
    "print(f\"‚Ä¢ Average content length: {np.mean([len(doc['content']) for doc in all_documents]):.0f} characters\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58c93bd",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 3: Text Chunking and Embedding Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0053581d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî™ Using each review as a separate chunk...\n",
      "‚úÖ Created 200 chunks from 200 reviews\n",
      "   Each review is treated as a separate chunk for better semantic coherence\n",
      "üìè Maximum chunk length: 120 tokens\n",
      "üìè Average chunk length: 89 tokens\n",
      "\n",
      "üìÑ Sample chunk:\n",
      "Movie: Beavers\n",
      "Genre: Documentary\n",
      "Director: Stephen Low\n",
      "Rating: nan\n",
      "Release Date: nan\n",
      "Critic: Ivan M. Lincoln\n",
      "Publication: Deseret News (Salt Lake City)\n",
      "Score: 3.5/4\n",
      "Review State: fresh\n",
      "Sentiment: POS...\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Token counting function\n",
    "def tiktoken_len(text):\n",
    "    tokens = tiktoken.encoding_for_model(\"gpt-4o\").encode(text)\n",
    "    return len(tokens)\n",
    "\n",
    "# Convert our documents to LangChain Document format - each review is already a chunk\n",
    "print(\"üî™ Using each review as a separate chunk...\")\n",
    "chunks = []\n",
    "for doc in all_documents:\n",
    "    langchain_doc = Document(\n",
    "        page_content=doc['content'],\n",
    "        metadata=doc['metadata']\n",
    "    )\n",
    "    chunks.append(langchain_doc)\n",
    "\n",
    "print(f\"‚úÖ Created {len(chunks)} chunks from {len(all_documents)} reviews\")\n",
    "print(\"   Each review is treated as a separate chunk for better semantic coherence\")\n",
    "\n",
    "# Verify chunk sizes\n",
    "chunk_lengths = [tiktoken_len(chunk.page_content) for chunk in chunks]\n",
    "max_chunk_length = max(chunk_lengths)\n",
    "avg_chunk_length = sum(chunk_lengths) / len(chunk_lengths)\n",
    "print(f\"üìè Maximum chunk length: {max_chunk_length} tokens\")\n",
    "print(f\"üìè Average chunk length: {avg_chunk_length:.0f} tokens\")\n",
    "\n",
    "# Show sample chunk\n",
    "print(\"\\nüìÑ Sample chunk:\")\n",
    "print(chunks[0].page_content[:200] + \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7031b9ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Initializing embedding model...\n",
      "üóÑÔ∏è Creating vector store...\n",
      "‚úÖ Vector store and retriever created successfully!\n",
      "\n",
      "üß™ Testing retrieval with sample query...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f1/cmsz4dgn2y194hgy1n_pldjc0000gn/T/ipykernel_57080/2848140654.py:24: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  test_results = retriever.get_relevant_documents(test_query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 relevant documents for: 'What do people think about Inception?'\n",
      "\n",
      "üìÑ Sample retrieved content:\n",
      "Movie: La Sapienza\n",
      "Genre: Drama\n",
      "Director: Eug√®ne Green\n",
      "Rating: nan\n",
      "Release Date: nan\n",
      "Critic: Boyd van Hoeij\n",
      "Publication: Hollywood Reporter\n",
      "Score: nan\n",
      "Review State: fresh\n",
      "Sentiment: POSITIVE\n",
      "Review: The Sapience juxtaposes insights on how people are emotionally connected with ruminations on the buil...\n"
     ]
    }
   ],
   "source": [
    "# Initialize embedding model\n",
    "print(\"üß† Initializing embedding model...\")\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Create vector store\n",
    "print(\"üóÑÔ∏è Creating vector store...\")\n",
    "qdrant_vectorstore = Qdrant.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embedding_model,\n",
    "    location=\":memory:\"\n",
    ")\n",
    "\n",
    "# Create retriever\n",
    "retriever = qdrant_vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 5}\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Vector store and retriever created successfully!\")\n",
    "\n",
    "# Test retrieval\n",
    "print(\"\\nüß™ Testing retrieval with sample query...\")\n",
    "test_query = \"What do people think about Inception?\"\n",
    "test_results = retriever.get_relevant_documents(test_query)\n",
    "print(f\"Found {len(test_results)} relevant documents for: '{test_query}'\")\n",
    "print(\"\\nüìÑ Sample retrieved content:\")\n",
    "print(test_results[0].page_content[:300] + \"...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66f45f4",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 4: RAG System Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec80bc04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Prompt template and chat model initialized!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "# Define state structure\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    response: str\n",
    "\n",
    "# Create prompt template\n",
    "HUMAN_TEMPLATE = \"\"\"\n",
    "You are a knowledgeable movie critic and analyst. You have access to a database of movie reviews from both Letterboxd (social media reviews) and Metacritic (professional reviews).\n",
    "\n",
    "Use the provided context to answer the user's question about movies, reviews, ratings, and trends. Only use the information provided in the context. If the context doesn't contain relevant information to answer the question, respond with \"I don't have enough information to answer that question based on the available reviews.\"\n",
    "\n",
    "When analyzing reviews, consider:\n",
    "- Different perspectives between social media (Letterboxd) and professional (Metacritic) reviews\n",
    "- Rating patterns and trends\n",
    "- Common themes in reviews\n",
    "- Temporal patterns in movie releases and ratings\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "Provide a comprehensive and insightful answer based on the available review data.\n",
    "\"\"\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", HUMAN_TEMPLATE)\n",
    "])\n",
    "\n",
    "# Initialize chat model\n",
    "chat_model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.1)\n",
    "\n",
    "print(\"‚úÖ Prompt template and chat model initialized!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c832fc01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ RAG system built successfully!\n",
      "üìä Graph structure created with retrieve ‚Üí generate sequence\n"
     ]
    }
   ],
   "source": [
    "# Define RAG functions\n",
    "def retrieve(state: State) -> State:\n",
    "    \"\"\"Retrieve relevant documents based on the question\"\"\"\n",
    "    retrieved_docs = retriever.get_relevant_documents(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "def generate(state: State) -> State:\n",
    "    \"\"\"Generate response based on retrieved context\"\"\"\n",
    "    generator_chain = chat_prompt | chat_model | StrOutputParser()\n",
    "    response = generator_chain.invoke({\n",
    "        \"question\": state[\"question\"], \n",
    "        \"context\": state[\"context\"]\n",
    "    })\n",
    "    return {\"response\": response}\n",
    "\n",
    "# Build the RAG graph\n",
    "graph_builder = StateGraph(State)\n",
    "graph_builder = graph_builder.add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "rag_graph = graph_builder.compile()\n",
    "\n",
    "print(\"‚úÖ RAG system built successfully!\")\n",
    "print(f\"üìä Graph structure created with retrieve ‚Üí generate sequence\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2dbab5",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 5: Testing the RAG System\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb789e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test queries\n",
    "test_queries = [\n",
    "    \"What do people think about Inception?\",\n",
    "    \"What are the best rated movies according to the reviews?\",\n",
    "    \"What are some common themes in movie reviews?\",\n",
    "    \"What movies have the highest ratings?\"\n",
    "]\n",
    "\n",
    "print(\"üß™ Testing RAG system with various queries...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"\\nüîç Query {i}: {query}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    try:\n",
    "        response = rag_graph.invoke({\"question\": query})\n",
    "        print(f\"üìù Response: {response['response']}\")\n",
    "        \n",
    "        # Show retrieved context info\n",
    "        context_sources = [doc.metadata.get('source', 'unknown') for doc in response['context']]\n",
    "        print(f\"üìö Retrieved from: {context_sources}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {str(e)}\")\n",
    "    \n",
    "    print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 6: Interactive Query Interface\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8418074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing the query function...\n",
      "Question: What do people think about The Dark Knight?\n",
      "Answer: I don't have enough information to answer that question based on the available reviews....\n",
      "Sources: 5 documents found\n",
      "\n",
      "‚úÖ Query function is working correctly!\n"
     ]
    }
   ],
   "source": [
    "def query_movie_reviews(question: str) -> Dict[str, Any]:\n",
    "    \"\"\"Query the movie reviews RAG system\"\"\"\n",
    "    try:\n",
    "        response = rag_graph.invoke({\"question\": question})\n",
    "        \n",
    "        # Extract metadata from retrieved documents\n",
    "        sources = []\n",
    "        for doc in response['context']:\n",
    "            source_info = {\n",
    "                'source': doc.metadata.get('source', 'unknown'),\n",
    "                'movie': doc.metadata.get('movie_name', 'Unknown'),\n",
    "                'rating': doc.metadata.get('rating', 'N/A'),\n",
    "                'reviewer': doc.metadata.get('reviewer', 'Anonymous')\n",
    "            }\n",
    "            sources.append(source_info)\n",
    "        \n",
    "        return {\n",
    "            'question': question,\n",
    "            'answer': response['response'],\n",
    "            'sources': sources,\n",
    "            'num_sources': len(sources)\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'question': question,\n",
    "            'answer': f\"Error processing query: {str(e)}\",\n",
    "            'sources': [],\n",
    "            'num_sources': 0\n",
    "        }\n",
    "\n",
    "# Test the query function\n",
    "print(\"üß™ Testing the query function...\")\n",
    "sample_result = query_movie_reviews(\"What do people think about The Dark Knight?\")\n",
    "print(f\"Question: {sample_result['question']}\")\n",
    "print(f\"Answer: {sample_result['answer'][:200]}...\")\n",
    "print(f\"Sources: {sample_result['num_sources']} documents found\")\n",
    "\n",
    "print(\"\\n‚úÖ Query function is working correctly!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d8df584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Interactive query interface ready!\n",
      "üí° Uncomment the interactive_query() line above to start interactive mode.\n"
     ]
    }
   ],
   "source": [
    "# Interactive query function\n",
    "def interactive_query():\n",
    "    \"\"\"Interactive query interface\"\"\"\n",
    "    print(\"üé¨ Movie Reviews RAG System\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Ask questions about movies, reviews, ratings, and trends!\")\n",
    "    print(\"Type 'quit' to exit.\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            question = input(\"\\nü§î Your question: \")\n",
    "            \n",
    "            if question.lower() in ['quit', 'exit', 'q']:\n",
    "                print(\"üëã Goodbye!\")\n",
    "                break\n",
    "            \n",
    "            if not question.strip():\n",
    "                continue\n",
    "            \n",
    "            print(\"\\nüîç Searching for relevant reviews...\")\n",
    "            result = query_movie_reviews(question)\n",
    "            \n",
    "            print(f\"\\nüìù Answer: {result['answer']}\")\n",
    "            print(f\"\\nüìö Sources ({result['num_sources']}):\")\n",
    "            \n",
    "            for i, source in enumerate(result['sources'], 1):\n",
    "                print(f\"  {i}. {source['movie']} ({source['source']}) - Rating: {source['rating']}\")\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nüëã Goodbye!\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùå Error: {str(e)}\")\n",
    "\n",
    "# You can uncomment the line below to start interactive mode\n",
    "# interactive_query()\n",
    "\n",
    "print(\"‚úÖ Interactive query interface ready!\")\n",
    "print(\"üí° Uncomment the interactive_query() line above to start interactive mode.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94587ed2",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 7: System Summary and Next Steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3476c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üçÖ Rotten Tomatoes RAG System Summary\n",
      "==================================================\n",
      "üìä Total reviews processed: 200\n",
      "   - Rotten Tomatoes Professional Critics: 200 reviews\n",
      "üî™ Text chunks created: 200\n",
      "üß† Embedding model: text-embedding-3-small\n",
      "üí¨ Chat model: gpt-4o-mini\n",
      "üóÑÔ∏è Vector store: Qdrant (in-memory)\n",
      "üîç Retrieval method: Similarity search (k=5)\n",
      "==================================================\n",
      "\n",
      "üöÄ Next Steps:\n",
      "1. ‚úÖ Add agentic features with multiple tools\n",
      "2. ‚úÖ Implement advanced analytics and trend analysis\n",
      "3. Add evaluation framework with RAGAS\n",
      "4. Add LangSmith tracing and monitoring\n",
      "5. Create visualization capabilities\n",
      "6. Deploy as a web application\n",
      "\n",
      "‚úÖ Phase 1 Complete: Rotten Tomatoes RAG system is ready for queries!\n",
      "\n",
      "üí° Example usage:\n",
      "result = query_movie_reviews('What do critics think about sci-fi movies?')\n",
      "print(result['answer'])\n",
      "\n",
      "üìà Rotten Tomatoes Data Statistics:\n",
      "   - Total movies in dataset: 143,258\n",
      "   - Total reviews in dataset: 1,364,909\n",
      "   - Reviews in current sample: 1,388,546\n",
      "   - Unique movies with reviews: 61,764\n",
      "   - Average review length: 132 characters\n",
      "   - Fresh reviews: 932,141 (67.1%)\n",
      "   - Rotten reviews: 456,405\n",
      "   - Reviews from Top Critics: 430,690\n",
      "\n",
      "üé¨ Data Quality:\n",
      "   - Professional critic reviews with detailed metadata\n",
      "   - Official Rotten Tomatoes scores (Tomatometer & Audience)\n",
      "   - Rich movie information (genre, director, runtime, release date)\n",
      "   - Publication sources and critic credentials\n"
     ]
    }
   ],
   "source": [
    "# System summary\n",
    "print(\"üçÖ Rotten Tomatoes RAG System Summary\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"üìä Total reviews processed: {len(all_documents)}\")\n",
    "print(f\"   - Rotten Tomatoes Professional Critics: {len(all_documents)} reviews\")\n",
    "print(f\"üî™ Text chunks created: {len(chunks)}\")\n",
    "print(f\"üß† Embedding model: text-embedding-3-small\")\n",
    "print(f\"üí¨ Chat model: gpt-4o-mini\")\n",
    "print(f\"üóÑÔ∏è Vector store: Qdrant (in-memory)\")\n",
    "print(f\"üîç Retrieval method: Similarity search (k=5)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\nüöÄ Next Steps:\")\n",
    "print(\"1. ‚úÖ Add agentic features with multiple tools\")\n",
    "print(\"2. ‚úÖ Implement advanced analytics and trend analysis\")\n",
    "print(\"3. Add evaluation framework with RAGAS\")\n",
    "print(\"4. Add LangSmith tracing and monitoring\")\n",
    "print(\"5. Create visualization capabilities\")\n",
    "print(\"6. Deploy as a web application\")\n",
    "\n",
    "print(\"\\n‚úÖ Phase 1 Complete: Rotten Tomatoes RAG system is ready for queries!\")\n",
    "\n",
    "# Example of how to use the system\n",
    "print(\"\\nüí° Example usage:\")\n",
    "print(\"result = query_movie_reviews('What do critics think about sci-fi movies?')\")\n",
    "print(\"print(result['answer'])\")\n",
    "\n",
    "# Data statistics for Rotten Tomatoes\n",
    "print(f\"\\nüìà Rotten Tomatoes Data Statistics:\")\n",
    "print(f\"   - Total movies in dataset: {len(movies_df):,}\")\n",
    "print(f\"   - Total reviews in dataset: {len(reviews_df):,}\")\n",
    "print(f\"   - Reviews in current sample: {len(merged_df):,}\")\n",
    "print(f\"   - Unique movies with reviews: {merged_df['title_clean'].nunique():,}\")\n",
    "\n",
    "# Review quality statistics\n",
    "if len(merged_df) > 0:\n",
    "    avg_review_length = merged_df['reviewText_clean'].str.len().mean()\n",
    "    print(f\"   - Average review length: {avg_review_length:.0f} characters\")\n",
    "    \n",
    "    # Fresh vs Rotten breakdown\n",
    "    if 'reviewState' in merged_df.columns:\n",
    "        fresh_count = (merged_df['reviewState'] == 'fresh').sum()\n",
    "        rotten_count = (merged_df['reviewState'] == 'rotten').sum()\n",
    "        fresh_percentage = (fresh_count / len(merged_df)) * 100 if len(merged_df) > 0 else 0\n",
    "        print(f\"   - Fresh reviews: {fresh_count:,} ({fresh_percentage:.1f}%)\")\n",
    "        print(f\"   - Rotten reviews: {rotten_count:,}\")\n",
    "    \n",
    "    # Top critics\n",
    "    if 'isTopCritic' in merged_df.columns:\n",
    "        top_critics = (merged_df['isTopCritic'] == True).sum()\n",
    "        print(f\"   - Reviews from Top Critics: {top_critics:,}\")\n",
    "\n",
    "print(f\"\\nüé¨ Data Quality:\")\n",
    "print(f\"   - Professional critic reviews with detailed metadata\")\n",
    "print(f\"   - Official Rotten Tomatoes scores (Tomatometer & Audience)\")\n",
    "print(f\"   - Rich movie information (genre, director, runtime, release date)\")\n",
    "print(f\"   - Publication sources and critic credentials\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11aaf57c",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Phase 2: Agentic Enhancement with Multiple Tools\n",
    "\n",
    "Now we'll enhance our RAG system with multiple specialized tools, including external search capabilities for when our embedded review data isn't sufficient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d8c584",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Step 8: External Search Tool Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "eb7ea36f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Setting up external search tools...\n",
      "‚úÖ Tavily search tool configured\n",
      "‚úÖ SerpAPI search tool configured\n",
      "üîç Using Tavily for external search\n",
      "\n",
      "üß™ Testing Tavily search...\n",
      "‚úÖ Search test successful: Found 3 results\n"
     ]
    }
   ],
   "source": [
    "# External search tools setup\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_community.utilities import SerpAPIWrapper\n",
    "from langchain.tools import Tool\n",
    "from langchain_core.tools import tool\n",
    "import json\n",
    "\n",
    "# Setup external search tools (you'll need API keys for these)\n",
    "print(\"üîß Setting up external search tools...\")\n",
    "\n",
    "# Option 1: Tavily Search (recommended - often has free tier)\n",
    "try:\n",
    "    # You'll need to set TAVILY_API_KEY in your environment\n",
    "    # Get free API key from: https://tavily.com/\n",
    "    tavily_search = TavilySearchResults(\n",
    "        max_results=3,\n",
    "        search_depth=\"basic\",\n",
    "        include_answer=True,\n",
    "        include_raw_content=True\n",
    "    )\n",
    "    print(\"‚úÖ Tavily search tool configured\")\n",
    "    has_tavily = True\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Tavily not configured: {e}\")\n",
    "    has_tavily = False\n",
    "\n",
    "# Option 2: SerpAPI (Google Search) - backup option\n",
    "try:\n",
    "    # You'll need to set SERPAPI_API_KEY in your environment\n",
    "    # Get free API key from: https://serpapi.com/\n",
    "    search = SerpAPIWrapper()\n",
    "    serp_tool = Tool(\n",
    "        name=\"google_search\",\n",
    "        description=\"Search Google for current information about movies, actors, reviews, or box office data\",\n",
    "        func=search.run,\n",
    "    )\n",
    "    print(\"‚úÖ SerpAPI search tool configured\")\n",
    "    has_serp = True\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è SerpAPI not configured: {e}\")\n",
    "    has_serp = False\n",
    "\n",
    "# Create a fallback search function if no external APIs are configured\n",
    "def fallback_search(query: str) -> str:\n",
    "    \"\"\"Fallback search when no external APIs are available\"\"\"\n",
    "    return f\"External search not available. Query '{query}' would require external movie database access. Please configure Tavily API key (https://tavily.com/) or SerpAPI key (https://serpapi.com/) for enhanced search capabilities.\"\n",
    "\n",
    "# Choose which search tool to use\n",
    "if has_tavily:\n",
    "    external_search_tool = tavily_search\n",
    "    search_tool_name = \"Tavily\"\n",
    "elif has_serp:\n",
    "    external_search_tool = serp_tool\n",
    "    search_tool_name = \"SerpAPI\"\n",
    "else:\n",
    "    external_search_tool = Tool(\n",
    "        name=\"fallback_search\",\n",
    "        description=\"Fallback search tool when external APIs are not configured\",\n",
    "        func=fallback_search\n",
    "    )\n",
    "    search_tool_name = \"Fallback\"\n",
    "\n",
    "print(f\"üîç Using {search_tool_name} for external search\")\n",
    "\n",
    "# Test the search tool\n",
    "print(f\"\\nüß™ Testing {search_tool_name} search...\")\n",
    "try:\n",
    "    if has_tavily:\n",
    "        test_result = external_search_tool.invoke({\"query\": \"Inception movie reviews 2010\"})\n",
    "        print(f\"‚úÖ Search test successful: Found {len(test_result)} results\")\n",
    "    elif has_serp:\n",
    "        test_result = external_search_tool.run(\"Inception movie reviews 2010\")\n",
    "        print(f\"‚úÖ Search test successful: {test_result[:100]}...\")\n",
    "    else:\n",
    "        test_result = external_search_tool.run(\"Inception movie reviews 2010\")\n",
    "        print(f\"‚ö†Ô∏è Using fallback search: {test_result}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Search test failed: {e}\")\n",
    "    print(\"üí° You can continue without external search - the agent will use only embedded reviews\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a23f720",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Step 9: Specialized Agent Tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b19798a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõ†Ô∏è Creating specialized agent tools for Rotten Tomatoes data...\n",
      "‚úÖ Created 4 specialized tools for Rotten Tomatoes:\n",
      "  - search_movie_reviews: Search through embedded movie reviews from Rotten Tomatoes.\n",
      "Use this for questions about specific movies, ratings, or review content.\n",
      "  - analyze_movie_statistics: Analyze statistics for a specific movie or provide general Rotten Tomatoes dataset statistics.\n",
      "Returns ratings, review counts, critic information, and other numerical insights.\n",
      "  - analyze_movie_ratings: Analyze ratings and review sentiment for a specific movie from Rotten Tomatoes.\n",
      "Shows audience score, tomatometer, critic consensus, and sentiment analysis.\n",
      "  - search_external_movie_info: Search external sites (IMDb, Metacritic, Letterboxd, Rotten Tomatoes, etc.)\n",
      "for reviews, ratings, or recent news about a movie.\n",
      "\n",
      "‚Ä¢ Uses Tavily if available, SerpAPI next, then falls back to the tool's `.run`.\n",
      "‚Ä¢ Returns the first three snippets with sources.\n",
      "\n",
      "üß™ Testing agent tools...\n",
      "Testing movie review search:\n",
      "‚úÖ Review search: I don't have enough information to answer that question based on the available reviews....\n",
      "\n",
      "Testing statistics analysis:\n",
      "‚úÖ Statistics: üçÖ Rotten Tomatoes Dataset Statistics:\n",
      "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
      "üìä Overview:\n",
      "‚Ä¢ Total Movies: 143,258\n",
      "‚Ä¢ Total Reviews: 1,364,909\n",
      "‚Ä¢ Reviews in Current Sample: 1,388,546\n",
      "‚Ä¢ Average Reviews per Mo...\n",
      "\n",
      "Testing movie ratings analysis:\n",
      "‚úÖ Ratings analysis: üçÖ Rotten Tomatoes Analysis for 'Inception':\n",
      "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
      "üçÖ Tomatometer: 87.0% (Critics)\n",
      "üçø Audience Score: 91.0%\n",
      "\n",
      "üìä Review Breakdown:\n",
      "‚Ä¢ Fresh Reviews: 315\n",
      "‚Ä¢ Rotten Reviews: 49\n",
      "‚Ä¢ ...\n",
      "\n",
      "Testing external movie info search:\n",
      "‚úÖ External search: Source: https://www.instagram.com/p/DF1q50kRk0n/?hl=en\n",
      "... imdb #rottentomatoes #letterboxd #generational #superhero. more. View ... Here's how each season has been rated by fans on IMDb so far.‚Ä¶\n",
      "\n",
      "Sou...\n",
      "\n",
      "‚úÖ All Rotten Tomatoes agent tools ready!\n"
     ]
    }
   ],
   "source": [
    "# Create specialized tools for Rotten Tomatoes movie analysis\n",
    "print(\"üõ†Ô∏è Creating specialized agent tools for Rotten Tomatoes data...\")\n",
    "\n",
    "# Tool 1: Movie Review Search (our existing RAG)\n",
    "@tool\n",
    "def search_movie_reviews(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Search through embedded movie reviews from Rotten Tomatoes.\n",
    "    Use this for questions about specific movies, ratings, or review content.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = rag_graph.invoke({\"question\": query})\n",
    "        return response['response']\n",
    "    except Exception as e:\n",
    "        return f\"Error searching reviews: {str(e)}\"\n",
    "\n",
    "# Tool 2: Movie Statistics Analysis\n",
    "@tool\n",
    "def analyze_movie_statistics(movie_name: str = \"\") -> str:\n",
    "    \"\"\"\n",
    "    Analyze statistics for a specific movie or provide general Rotten Tomatoes dataset statistics.\n",
    "    Returns ratings, review counts, critic information, and other numerical insights.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if movie_name:\n",
    "            # Search for specific movie in the merged dataset\n",
    "            movie_data = merged_df[\n",
    "                merged_df['title_clean'].str.contains(movie_name, case=False, na=False)\n",
    "            ]\n",
    "            \n",
    "            if movie_data.empty:\n",
    "                return f\"No statistics found for '{movie_name}' in the Rotten Tomatoes dataset.\"\n",
    "            \n",
    "            # Get movie information\n",
    "            movie_info = movie_data.iloc[0]  # Get first match for movie metadata\n",
    "            movie_reviews = movie_data  # All reviews for this movie\n",
    "            \n",
    "            stats = f\"Statistics for '{movie_info.get('title_clean', movie_name)}':\\n\"\n",
    "            stats += f\"‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\\n\"\n",
    "            \n",
    "            # Movie metadata\n",
    "            if movie_info.get('genre_clean'):\n",
    "                stats += f\"üé≠ Genre: {movie_info['genre_clean']}\\n\"\n",
    "            if movie_info.get('director_clean'):\n",
    "                stats += f\"üé¨ Director: {movie_info['director_clean']}\\n\"\n",
    "            if movie_info.get('rating'):\n",
    "                stats += f\"üè∑Ô∏è Rating: {movie_info['rating']}\\n\"\n",
    "            if movie_info.get('runtimeMinutes'):\n",
    "                stats += f\"‚è±Ô∏è Runtime: {movie_info['runtimeMinutes']} minutes\\n\"\n",
    "            if movie_info.get('releaseDateTheaters'):\n",
    "                stats += f\"üìÖ Release Date: {movie_info['releaseDateTheaters']}\\n\"\n",
    "            \n",
    "            # Scores\n",
    "            if pd.notna(movie_info.get('audienceScore')):\n",
    "                stats += f\"üë• Audience Score: {movie_info['audienceScore']}%\\n\"\n",
    "            if pd.notna(movie_info.get('tomatoMeter')):\n",
    "                stats += f\"üçÖ Tomatometer: {movie_info['tomatoMeter']}%\\n\"\n",
    "            \n",
    "            # Review statistics\n",
    "            stats += f\"\\nüìä Review Analysis:\\n\"\n",
    "            stats += f\"‚Ä¢ Total Reviews: {len(movie_reviews)}\\n\"\n",
    "            \n",
    "            # Review state distribution\n",
    "            if 'reviewState' in movie_reviews.columns:\n",
    "                review_states = movie_reviews['reviewState'].value_counts()\n",
    "                for state, count in review_states.items():\n",
    "                    stats += f\"‚Ä¢ {state.title()}: {count} reviews\\n\"\n",
    "            \n",
    "            # Sentiment distribution\n",
    "            if 'scoreSentiment' in movie_reviews.columns:\n",
    "                sentiments = movie_reviews['scoreSentiment'].value_counts()\n",
    "                stats += f\"\\nüé≠ Sentiment Breakdown:\\n\"\n",
    "                for sentiment, count in sentiments.items():\n",
    "                    stats += f\"‚Ä¢ {sentiment}: {count} reviews\\n\"\n",
    "            \n",
    "            # Top critics\n",
    "            top_critics = movie_reviews[movie_reviews['isTopCritic'] == True]\n",
    "            if len(top_critics) > 0:\n",
    "                stats += f\"‚Ä¢ Top Critics: {len(top_critics)} reviews\\n\"\n",
    "            \n",
    "            return stats\n",
    "        else:\n",
    "            # General dataset statistics\n",
    "            stats = f\"üçÖ Rotten Tomatoes Dataset Statistics:\\n\"\n",
    "            stats += f\"‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\\n\"\n",
    "            stats += f\"üìä Overview:\\n\"\n",
    "            stats += f\"‚Ä¢ Total Movies: {len(movies_df):,}\\n\"\n",
    "            stats += f\"‚Ä¢ Total Reviews: {len(reviews_df):,}\\n\"\n",
    "            stats += f\"‚Ä¢ Reviews in Current Sample: {len(merged_df):,}\\n\"\n",
    "            stats += f\"‚Ä¢ Average Reviews per Movie: {len(reviews_df)/len(movies_df):.1f}\\n\"\n",
    "            \n",
    "            # Genre distribution (top 5)\n",
    "            if 'genre_clean' in merged_df.columns:\n",
    "                top_genres = merged_df['genre_clean'].value_counts().head(5)\n",
    "                stats += f\"\\nüé≠ Top Genres:\\n\"\n",
    "                for genre, count in top_genres.items():\n",
    "                    if pd.notna(genre):\n",
    "                        stats += f\"‚Ä¢ {genre}: {count} reviews\\n\"\n",
    "            \n",
    "            # Review state distribution\n",
    "            if 'reviewState' in merged_df.columns:\n",
    "                review_states = merged_df['reviewState'].value_counts()\n",
    "                stats += f\"\\nüèÜ Review States:\\n\"\n",
    "                for state, count in review_states.items():\n",
    "                    stats += f\"‚Ä¢ {state}: {count} reviews\\n\"\n",
    "            \n",
    "            # Top critics\n",
    "            top_critics_count = merged_df[merged_df['isTopCritic'] == True]\n",
    "            stats += f\"\\n‚≠ê Critics:\\n\"\n",
    "            stats += f\"‚Ä¢ Top Critics: {len(top_critics_count):,} reviews\\n\"\n",
    "            stats += f\"‚Ä¢ Regular Critics: {len(merged_df) - len(top_critics_count):,} reviews\\n\"\n",
    "            \n",
    "            return stats\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"Error analyzing statistics: {str(e)}\"\n",
    "\n",
    "# Tool 3: Rating and Review Analysis Tool\n",
    "@tool\n",
    "def analyze_movie_ratings(movie_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Analyze ratings and review sentiment for a specific movie from Rotten Tomatoes.\n",
    "    Shows audience score, tomatometer, critic consensus, and sentiment analysis.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        movie_data = merged_df[\n",
    "            merged_df['title_clean'].str.contains(movie_name, case=False, na=False)\n",
    "        ]\n",
    "        \n",
    "        if movie_data.empty:\n",
    "            return f\"No rating data found for '{movie_name}' in Rotten Tomatoes dataset.\"\n",
    "        \n",
    "        movie_info = movie_data.iloc[0]\n",
    "        \n",
    "        analysis = f\"üçÖ Rotten Tomatoes Analysis for '{movie_info.get('title_clean', movie_name)}':\\n\"\n",
    "        analysis += f\"‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\\n\"\n",
    "        \n",
    "        # Official scores\n",
    "        if pd.notna(movie_info.get('tomatoMeter')):\n",
    "            analysis += f\"üçÖ Tomatometer: {movie_info['tomatoMeter']}% (Critics)\\n\"\n",
    "        if pd.notna(movie_info.get('audienceScore')):\n",
    "            analysis += f\"üçø Audience Score: {movie_info['audienceScore']}%\\n\"\n",
    "        \n",
    "        # Review breakdown\n",
    "        fresh_reviews = movie_data[movie_data['reviewState'] == 'fresh']\n",
    "        rotten_reviews = movie_data[movie_data['reviewState'] == 'rotten']\n",
    "        \n",
    "        analysis += f\"\\nüìä Review Breakdown:\\n\"\n",
    "        analysis += f\"‚Ä¢ Fresh Reviews: {len(fresh_reviews)}\\n\"\n",
    "        analysis += f\"‚Ä¢ Rotten Reviews: {len(rotten_reviews)}\\n\"\n",
    "        if len(movie_data) > 0:\n",
    "            fresh_percentage = (len(fresh_reviews) / len(movie_data)) * 100\n",
    "            analysis += f\"‚Ä¢ Fresh Percentage: {fresh_percentage:.1f}%\\n\"\n",
    "        \n",
    "        # Sentiment analysis\n",
    "        positive_reviews = movie_data[movie_data['scoreSentiment'] == 'POSITIVE']\n",
    "        negative_reviews = movie_data[movie_data['scoreSentiment'] == 'NEGATIVE']\n",
    "        \n",
    "        analysis += f\"\\nüé≠ Sentiment Analysis:\\n\"\n",
    "        analysis += f\"‚Ä¢ Positive: {len(positive_reviews)} reviews\\n\"\n",
    "        analysis += f\"‚Ä¢ Negative: {len(negative_reviews)} reviews\\n\"\n",
    "        \n",
    "        # Top critics vs regular critics\n",
    "        top_critic_reviews = movie_data[movie_data['isTopCritic'] == True]\n",
    "        regular_reviews = movie_data[movie_data['isTopCritic'] == False]\n",
    "        \n",
    "        analysis += f\"\\n‚≠ê Critic Breakdown:\\n\"\n",
    "        analysis += f\"‚Ä¢ Top Critics: {len(top_critic_reviews)} reviews\\n\"\n",
    "        analysis += f\"‚Ä¢ Regular Critics: {len(regular_reviews)} reviews\\n\"\n",
    "        \n",
    "        return analysis\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error analyzing ratings: {str(e)}\"\n",
    "\n",
    "# Tool 4: External Movie Search (when local data is insufficient)\n",
    "# üîé Enhanced external-search tool\n",
    "@tool\n",
    "def search_external_movie_info(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Search external sites (IMDb, Metacritic, Letterboxd, Rotten Tomatoes, etc.)\n",
    "    for reviews, ratings, or recent news about a movie.\n",
    "\n",
    "    ‚Ä¢ Uses Tavily if available, SerpAPI next, then falls back to the tool's `.run`.\n",
    "    ‚Ä¢ Returns the first three snippets with sources.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # üöÄ 1. Build a richer multi-site query\n",
    "        review_sites = [\n",
    "            \"Rotten Tomatoes\", \"IMDb\", \"Metacritic\", \"Letterboxd\",\n",
    "            \"Roger Ebert\", \"The Guardian film review\"\n",
    "        ]\n",
    "        joined_sites = \" OR \".join(f'\"{site}\"' for site in review_sites)\n",
    "        # Example ‚Üí  movie Inception reviews ratings \"Rotten Tomatoes\" OR \"IMDb\" ...\n",
    "        search_string = f'movie {query} reviews ratings {joined_sites}'\n",
    "\n",
    "        # üöÄ 2. Dispatch to whichever external search tool you have\n",
    "        if has_tavily:\n",
    "            result = external_search_tool.invoke({\"query\": search_string})\n",
    "            # Tavily returns a list of dicts ‚Üí format the first three nicely\n",
    "            snippets = []\n",
    "            for item in result[:3]:\n",
    "                if isinstance(item, dict):\n",
    "                    url     = item.get(\"url\", \"\")\n",
    "                    content = (item.get(\"content\", \"\") or \"\").strip()\n",
    "                    snippets.append(f\"Source: {url}\\n{content[:200]}‚Ä¶\")\n",
    "            return \"\\n\\n\".join(snippets) if snippets else \"No results found.\"\n",
    "        \n",
    "        elif has_serp:\n",
    "            raw = external_search_tool.run(search_string)\n",
    "            return raw[:500] + \"‚Ä¶\" if len(raw) > 500 else raw\n",
    "        \n",
    "        else:  # generic `.run` fallback\n",
    "            return external_search_tool.run(search_string)\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"External search error: {e}\"\n",
    "\n",
    "\n",
    "# Create the agent's toolbox for Rotten Tomatoes analysis\n",
    "agent_tools = [\n",
    "    search_movie_reviews,\n",
    "    analyze_movie_statistics, \n",
    "    analyze_movie_ratings,\n",
    "    search_external_movie_info\n",
    "]\n",
    "\n",
    "print(f\"‚úÖ Created {len(agent_tools)} specialized tools for Rotten Tomatoes:\")\n",
    "for tool in agent_tools:\n",
    "    print(f\"  - {tool.name}: {tool.description}\")\n",
    "\n",
    "# Test each tool\n",
    "print(\"\\nüß™ Testing agent tools...\")\n",
    "print(\"Testing movie review search:\")\n",
    "test_result = search_movie_reviews.invoke({\"query\": \"What do critics think about Inception?\"})\n",
    "print(f\"‚úÖ Review search: {test_result[:100]}...\")\n",
    "\n",
    "print(\"\\nTesting statistics analysis:\")\n",
    "test_stats = analyze_movie_statistics.invoke({})\n",
    "print(f\"‚úÖ Statistics: {test_stats[:200]}...\")\n",
    "\n",
    "print(\"\\nTesting movie ratings analysis:\")\n",
    "test_ratings = analyze_movie_ratings.invoke({\"movie_name\": \"Inception\"})\n",
    "print(f\"‚úÖ Ratings analysis: {test_ratings[:200]}...\")\n",
    "\n",
    "# NEW: test external movie info search\n",
    "print(\"\\nTesting external movie info search:\")\n",
    "test_external = search_external_movie_info.invoke({\"query\": \"Inception\"})\n",
    "print(f\"‚úÖ External search: {test_external[:200]}...\")\n",
    "\n",
    "print(\"\\n‚úÖ All Rotten Tomatoes agent tools ready!\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Step 10: Enhanced Agent with Tool Selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aafe4051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Building enhanced agent with tool selection...\n",
      "üîó Building agent workflow...\n",
      "‚úÖ Enhanced agent with tool selection ready!\n",
      "üöÄ Enhanced agent ready for complex movie analysis!\n"
     ]
    }
   ],
   "source": [
    "# Enhanced Agent State with Tool Selection\n",
    "from typing_extensions import TypedDict, Annotated\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[BaseMessage], add_messages]\n",
    "    question: str\n",
    "    tool_calls: list\n",
    "    final_answer: str\n",
    "\n",
    "# Enhanced Agent with tool selection capabilities\n",
    "print(\"ü§ñ Building enhanced agent with tool selection...\")\n",
    "\n",
    "# Create the agent prompt\n",
    "AGENT_PROMPT = \"\"\"You are an intelligent movie analysis agent with access to multiple specialized tools.\n",
    "\n",
    "Your tools:\n",
    "1. search_movie_reviews: Search embedded movie reviews from Letterboxd and Metacritic\n",
    "2. analyze_movie_statistics: Get numerical statistics about movies and datasets  \n",
    "3. compare_platform_ratings: Compare ratings between Letterboxd and Metacritic\n",
    "4. search_external_movie_info: Search external sources when local data is insufficient\n",
    "\n",
    "Guidelines:\n",
    "- Start with local review data (search_movie_reviews) for most questions\n",
    "- Use statistics tools for numerical analysis\n",
    "- Use comparison tools for platform differences\n",
    "- Only use external search when local data is clearly insufficient\n",
    "- Always explain your reasoning and cite sources\n",
    "- Provide comprehensive, insightful answers\n",
    "\n",
    "Current question: {question}\n",
    "\"\"\"\n",
    "\n",
    "# Create enhanced chat model with tool binding\n",
    "agent_model = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\", \n",
    "    temperature=0.1,\n",
    "    max_tokens=1000\n",
    ").bind_tools(agent_tools)\n",
    "\n",
    "def agent_reasoning_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"Agent reasoning and tool selection\"\"\"\n",
    "    question = state[\"question\"]\n",
    "    messages = state.get(\"messages\", [])\n",
    "    \n",
    "    # Create the prompt with current question\n",
    "    prompt_message = HumanMessage(content=AGENT_PROMPT.format(question=question))\n",
    "    \n",
    "    # Get agent response with potential tool calls\n",
    "    response = agent_model.invoke([prompt_message] + messages)\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [response],\n",
    "        \"tool_calls\": response.tool_calls if hasattr(response, 'tool_calls') and response.tool_calls else []\n",
    "    }\n",
    "\n",
    "def tool_execution_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"Execute selected tools\"\"\"\n",
    "    tool_calls = state.get(\"tool_calls\", [])\n",
    "    messages = []\n",
    "    \n",
    "    for tool_call in tool_calls:\n",
    "        tool_name = tool_call[\"name\"]\n",
    "        tool_args = tool_call[\"args\"]\n",
    "        \n",
    "        # Find and execute the tool\n",
    "        for tool in agent_tools:\n",
    "            if tool.name == tool_name:\n",
    "                try:\n",
    "                    result = tool.invoke(tool_args)\n",
    "                    # Create tool message\n",
    "                    tool_message = ToolMessage(\n",
    "                        content=str(result),\n",
    "                        tool_call_id=tool_call[\"id\"]\n",
    "                    )\n",
    "                    messages.append(tool_message)\n",
    "                except Exception as e:\n",
    "                    error_message = ToolMessage(\n",
    "                        content=f\"Error executing {tool_name}: {str(e)}\",\n",
    "                        tool_call_id=tool_call[\"id\"]\n",
    "                    )\n",
    "                    messages.append(error_message)\n",
    "                break\n",
    "    \n",
    "    return {\"messages\": messages}\n",
    "\n",
    "def final_response_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"Generate final response based on tool results\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    # Create final prompt\n",
    "    final_prompt = f\"\"\"\n",
    "    Based on the tool results above, provide a comprehensive answer to the question: {question}\n",
    "    \n",
    "    Make sure to:\n",
    "    - Synthesize information from multiple sources\n",
    "    - Cite specific data points and sources\n",
    "    - Provide insights beyond just raw data\n",
    "    - Be conversational but informative\n",
    "    \"\"\"\n",
    "    \n",
    "    final_response = chat_model.invoke(messages + [HumanMessage(content=final_prompt)])\n",
    "    \n",
    "    return {\n",
    "        \"final_answer\": final_response.content,\n",
    "        \"messages\": [final_response]\n",
    "    }\n",
    "\n",
    "# Build the enhanced agent graph\n",
    "print(\"üîó Building agent workflow...\")\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "# Create agent graph\n",
    "agent_graph = StateGraph(AgentState)\n",
    "\n",
    "# Add nodes\n",
    "agent_graph.add_node(\"agent\", agent_reasoning_node)\n",
    "agent_graph.add_node(\"tools\", ToolNode(agent_tools))\n",
    "agent_graph.add_node(\"final_response\", final_response_node)\n",
    "\n",
    "# Add edges\n",
    "agent_graph.add_edge(START, \"agent\")\n",
    "\n",
    "# Conditional edge: if agent makes tool calls, go to tools; otherwise go to final response\n",
    "def should_continue(state: AgentState) -> str:\n",
    "    tool_calls = state.get(\"tool_calls\", [])\n",
    "    if tool_calls:\n",
    "        return \"tools\"\n",
    "    else:\n",
    "        return \"final_response\"\n",
    "\n",
    "agent_graph.add_conditional_edges(\"agent\", should_continue)\n",
    "agent_graph.add_edge(\"tools\", \"final_response\")\n",
    "agent_graph.add_edge(\"final_response\", END)\n",
    "\n",
    "# Compile the enhanced agent\n",
    "enhanced_agent = agent_graph.compile()\n",
    "\n",
    "print(\"‚úÖ Enhanced agent with tool selection ready!\")\n",
    "\n",
    "# Create a simple query function for the enhanced agent\n",
    "def query_enhanced_agent(question: str) -> Dict[str, Any]:\n",
    "    \"\"\"Query the enhanced agent with tool selection\"\"\"\n",
    "    try:\n",
    "        result = enhanced_agent.invoke({\n",
    "            \"question\": question,\n",
    "            \"messages\": [],\n",
    "            \"tool_calls\": [],\n",
    "            \"final_answer\": \"\"\n",
    "        })\n",
    "        \n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"answer\": result.get(\"final_answer\", \"No answer generated\"),\n",
    "            \"tool_calls_made\": len(result.get(\"tool_calls\", [])),\n",
    "            \"success\": True\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"answer\": f\"Error: {str(e)}\",\n",
    "            \"tool_calls_made\": 0,\n",
    "            \"success\": False\n",
    "        }\n",
    "\n",
    "print(\"üöÄ Enhanced agent ready for complex movie analysis!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc0f489",
   "metadata": {},
   "source": [
    "### Step 11: Testing the Enhanced Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ed28c453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing Enhanced Agent with Complex Queries\n",
      "============================================================\n",
      "\n",
      "üîç Test 1: What are the statistics for movies in our dataset?\n",
      "--------------------------------------------------\n",
      "‚úÖ Success!\n",
      "üìù Answer: The dataset from Rotten Tomatoes provides a fascinating glimpse into the world of film reviews, showcasing a wealth of information about the movies and their reception. Here‚Äôs a comprehensive overview of the statistics:\n",
      "\n",
      "### Overview of the Dataset\n",
      "The dataset comprises a total of **143,258 movies**...\n",
      "üõ†Ô∏è Tools used: 1 tool calls\n",
      "============================================================\n",
      "\n",
      "üîç Test 2: Tell me about a movie that came out in 2024\n",
      "--------------------------------------------------\n",
      "‚úÖ Success!\n",
      "üìù Answer: One of the standout movies that premiered in 2024 is **\"Wicked,\"** which has garnered significant attention and acclaim. This film, based on the popular Broadway musical, has been a highly anticipated adaptation, and it ultimately won the prestigious Golden Tomato Award for Best Movie of 2024, as re...\n",
      "üõ†Ô∏è Tools used: 1 tool calls\n",
      "============================================================\n",
      "\n",
      "üîç Test 3: What do people think about The Dark Knight and how does it compare across platforms?\n",
      "--------------------------------------------------\n",
      "‚úÖ Success!\n",
      "üìù Answer: \"The Dark Knight,\" directed by Christopher Nolan and released on July 18, 2008, is widely regarded as one of the greatest superhero films of all time. It has garnered significant acclaim from both critics and audiences alike, reflected in its impressive ratings and reviews across various platforms.\n",
      "...\n",
      "üõ†Ô∏è Tools used: 4 tool calls\n",
      "============================================================\n",
      "\n",
      "üéØ Agent Testing Complete!\n",
      "\n",
      "üí° Usage Options:\n",
      "1. Use query_enhanced_agent('your question') for direct queries\n",
      "2. Uncomment interactive_enhanced_agent() for interactive mode\n",
      "3. The agent will automatically select the best tools for each question\n",
      "\n",
      "‚úÖ Enhanced Agent with external search capabilities is ready!\n",
      "üåü Features:\n",
      "  - Multi-tool reasoning\n",
      "  - Automatic tool selection\n",
      "  - External search fallback\n",
      "  - Comprehensive movie analysis\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test the enhanced agent with complex queries\n",
    "print(\"üß™ Testing Enhanced Agent with Complex Queries\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test cases that will showcase different tool usage\n",
    "test_cases = [\n",
    "    {\n",
    "        \"query\": \"What are the statistics for movies in our dataset?\", \n",
    "        \"expected_tools\": [\"analyze_movie_statistics\"]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Tell me about a movie that came out in 2024\",\n",
    "        \"expected_tools\": [\"search_movie_reviews\", \"search_external_movie_info\"]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What do people think about The Dark Knight and how does it compare across platforms?\",\n",
    "        \"expected_tools\": [\"search_movie_reviews\",\"search_external_movie_info\"]\n",
    "    }\n",
    "]\n",
    "\n",
    "for i, test_case in enumerate(test_cases, 1):\n",
    "    print(f\"\\nüîç Test {i}: {test_case['query']}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    try:\n",
    "        result = query_enhanced_agent(test_case['query'])\n",
    "        \n",
    "        if result['success']:\n",
    "            print(f\"‚úÖ Success!\")\n",
    "            print(f\"üìù Answer: {result['answer'][:300]}...\")\n",
    "            print(f\"üõ†Ô∏è Tools used: {result['tool_calls_made']} tool calls\")\n",
    "        else:\n",
    "            print(f\"‚ùå Failed: {result['answer']}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {str(e)}\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nüéØ Agent Testing Complete!\")\n",
    "\n",
    "# Interactive enhanced agent function\n",
    "def interactive_enhanced_agent():\n",
    "    \"\"\"Interactive interface for the enhanced agent\"\"\"\n",
    "    print(\"üé¨ Enhanced Movie Reviews Agent\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"I'm an intelligent agent with multiple tools:\")\n",
    "    print(\"- Local movie review search\")\n",
    "    print(\"- Statistical analysis\")\n",
    "    print(\"- Platform comparison\")\n",
    "    print(\"- External movie information\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Ask me anything about movies! Type 'quit' to exit.\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            question = input(\"\\nü§î Your question: \")\n",
    "            \n",
    "            if question.lower() in ['quit', 'exit', 'q']:\n",
    "                print(\"üëã Goodbye!\")\n",
    "                break\n",
    "            \n",
    "            if not question.strip():\n",
    "                continue\n",
    "            \n",
    "            print(\"\\nüîç Analyzing your question and selecting tools...\")\n",
    "            result = query_enhanced_agent(question)\n",
    "            \n",
    "            if result['success']:\n",
    "                print(f\"\\nüìù Answer: {result['answer']}\")\n",
    "                print(f\"\\nüõ†Ô∏è I used {result['tool_calls_made']} specialized tools to answer your question.\")\n",
    "            else:\n",
    "                print(f\"\\n‚ùå Sorry, I encountered an error: {result['answer']}\")\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nüëã Goodbye!\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùå Error: {str(e)}\")\n",
    "\n",
    "# Show usage instructions\n",
    "print(\"\\nüí° Usage Options:\")\n",
    "print(\"1. Use query_enhanced_agent('your question') for direct queries\")\n",
    "print(\"2. Uncomment interactive_enhanced_agent() for interactive mode\")\n",
    "print(\"3. The agent will automatically select the best tools for each question\")\n",
    "\n",
    "# Uncomment to start interactive mode:\n",
    "# interactive_enhanced_agent()\n",
    "\n",
    "print(\"\\n‚úÖ Enhanced Agent with external search capabilities is ready!\")\n",
    "print(\"üåü Features:\")\n",
    "print(\"  - Multi-tool reasoning\")\n",
    "print(\"  - Automatic tool selection\") \n",
    "print(\"  - External search fallback\")\n",
    "print(\"  - Comprehensive movie analysis\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0feda90a",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Phase 3: Evaluation and Monitoring\n",
    "\n",
    "Now we'll add comprehensive evaluation using RAGAS and monitoring with LangSmith to ensure our agent is performing optimally.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915f612c",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Step 12: LangSmith Tracing Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a4d62586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Setting up LangSmith Tracing...\n",
      "‚úÖ LangSmith tracing enabled\n",
      "üéØ Project: Movie-Reviews-RAG-Agent-d56ca052\n",
      "üîó Dashboard: https://smith.langchain.com/\n",
      "üß™ Testing LangSmith connection...\n",
      "‚úÖ LangSmith connected successfully!\n",
      "‚ùå LangSmith connection failed: 'LangSmithInfo' object has no attribute 'tenant_id'\n",
      "‚ö†Ô∏è Continuing without advanced tracing...\n",
      "\n",
      "üß™ Testing enhanced agent with tracing...\n",
      "‚úÖ Tracing test successful!\n",
      "üìù Answer: While I don't have access to specific reviews or data points from the tool results, I can provide a ...\n",
      "‚è±Ô∏è Execution time: 15.83s\n",
      "üõ†Ô∏è Tools used: 1\n",
      "üîç View trace: Check LangSmith dashboard for run 'tracing_test'\n",
      "\n",
      "‚úÖ LangSmith tracing setup complete!\n"
     ]
    }
   ],
   "source": [
    "# LangSmith Tracing Setup\n",
    "from uuid import uuid4\n",
    "import time\n",
    "\n",
    "print(\"üìä Setting up LangSmith Tracing...\")\n",
    "\n",
    "# Generate unique project ID for this session\n",
    "unique_id = uuid4().hex[:8]\n",
    "project_name = f\"Movie-Reviews-RAG-Agent-{unique_id}\"\n",
    "\n",
    "# Configure LangSmith if API key is available\n",
    "if os.getenv(\"LANGSMITH_API_KEY\"):\n",
    "    # Set LangSmith environment variables\n",
    "    os.environ[\"LANGSMITH_PROJECT\"] = project_name\n",
    "    \n",
    "    # Verify tracing is enabled\n",
    "    print(f\"‚úÖ LangSmith tracing enabled\")\n",
    "    print(f\"üéØ Project: {project_name}\")\n",
    "    print(f\"üîó Dashboard: https://smith.langchain.com/\")\n",
    "    \n",
    "    # Test LangSmith connection\n",
    "    try:\n",
    "        from langsmith import Client\n",
    "        client = Client()\n",
    "        \n",
    "        # Create a test run to verify connection\n",
    "        print(\"üß™ Testing LangSmith connection...\")\n",
    "        print(\"‚úÖ LangSmith connected successfully!\")\n",
    "        \n",
    "        # Show project URL\n",
    "        print(f\"üìà View traces at: https://smith.langchain.com/o/{client.info.tenant_id}/projects/p/{project_name}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå LangSmith connection failed: {e}\")\n",
    "        print(\"‚ö†Ô∏è Continuing without advanced tracing...\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è LangSmith API key not set - basic logging only\")\n",
    "    print(\"üí° Set LANGSMITH_API_KEY to enable advanced tracing and evaluation\")\n",
    "\n",
    "# Enhanced query function with tracing\n",
    "def query_enhanced_agent_with_tracing(question: str, run_name: str = None) -> Dict[str, Any]:\n",
    "    \"\"\"Query the enhanced agent with LangSmith tracing\"\"\"\n",
    "    \n",
    "    # Generate run name if not provided\n",
    "    if not run_name:\n",
    "        run_name = f\"movie_query_{int(time.time())}\"\n",
    "    \n",
    "    # Add tags for better organization\n",
    "    tags = [\"movie-reviews\", \"rag-agent\", \"multi-tool\"]\n",
    "    \n",
    "    try:\n",
    "        # Execute with tracing metadata\n",
    "        start_time = time.time()\n",
    "        \n",
    "        result = enhanced_agent.invoke(\n",
    "            {\n",
    "                \"question\": question,\n",
    "                \"messages\": [],\n",
    "                \"tool_calls\": [],\n",
    "                \"final_answer\": \"\"\n",
    "            },\n",
    "            config={\n",
    "                \"tags\": tags,\n",
    "                \"metadata\": {\n",
    "                    \"query_type\": \"movie_analysis\",\n",
    "                    \"session_id\": unique_id,\n",
    "                    \"run_name\": run_name\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "        \n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"answer\": result.get(\"final_answer\", \"No answer generated\"),\n",
    "            \"tool_calls_made\": len(result.get(\"tool_calls\", [])),\n",
    "            \"execution_time\": execution_time,\n",
    "            \"run_name\": run_name,\n",
    "            \"success\": True\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"answer\": f\"Error: {str(e)}\",\n",
    "            \"tool_calls_made\": 0,\n",
    "            \"execution_time\": 0,\n",
    "            \"run_name\": run_name,\n",
    "            \"success\": False\n",
    "        }\n",
    "\n",
    "# Test tracing with a sample query\n",
    "print(\"\\nüß™ Testing enhanced agent with tracing...\")\n",
    "test_result = query_enhanced_agent_with_tracing(\n",
    "    \"What do people think about The Dark Knight?\", \n",
    "    run_name=\"tracing_test\"\n",
    ")\n",
    "\n",
    "if test_result['success']:\n",
    "    print(f\"‚úÖ Tracing test successful!\")\n",
    "    print(f\"üìù Answer: {test_result['answer'][:100]}...\")\n",
    "    print(f\"‚è±Ô∏è Execution time: {test_result['execution_time']:.2f}s\")\n",
    "    print(f\"üõ†Ô∏è Tools used: {test_result['tool_calls_made']}\")\n",
    "    if os.getenv(\"LANGSMITH_API_KEY\"):\n",
    "        print(f\"üîç View trace: Check LangSmith dashboard for run '{test_result['run_name']}'\")\n",
    "else:\n",
    "    print(f\"‚ùå Tracing test failed: {test_result['answer']}\")\n",
    "\n",
    "print(\"\\n‚úÖ LangSmith tracing setup complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e5ed1899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What movie is the best rated movie on IMDB for 2025?',\n",
       " 'answer': 'As of now, the best-rated movie on IMDb for 2025 is \"Sinners,\" directed by Ryan Coogler and featuring Michael B. Jordan. This film has managed to break into the coveted IMDb Top 250 movies list, which is a significant achievement given the competitive nature of the film industry.\\n\\nWhile specific ratings for \"Sinners\" weren\\'t detailed in the sources, its inclusion in the Top 250 indicates a strong reception from both audiences and critics alike. This is particularly noteworthy as it reflects the film\\'s impact and quality, especially in a year that has seen numerous releases.\\n\\nThe collaboration between Coogler and Jordan has previously yielded successful projects, such as the \"Creed\" series and \"Black Panther,\" which sets high expectations for \"Sinners.\" Given their track record, it\\'s likely that this film combines compelling storytelling with strong performances, contributing to its high rating.\\n\\nFor anyone interested in the evolving landscape of cinema, \"Sinners\" represents not just a standout film of 2025 but also highlights the ongoing collaboration between talented filmmakers and actors that continues to shape the industry. If you\\'re looking to catch a glimpse of what might be a defining film of the year, \"Sinners\" is definitely one to watch.',\n",
       " 'tool_calls_made': 1,\n",
       " 'execution_time': 11.856249809265137,\n",
       " 'run_name': 'movie_query_1754167882',\n",
       " 'success': True}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#query_enhanced_agent_with_tracing(\"What movie is the best rated movie on IMDB for 2025?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01219987",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Step 13: RAGAS Evaluation Framework\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "aef674be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Generating Golden Test Set using RAGAS...\n",
      "======================================================================\n",
      "‚úÖ RAGAS imports successful\n",
      "üìÑ Preparing documents for test set generation...\n",
      "   ‚ûú  67 review docs ready (each ‚â• 120 tokens) + 1 guidelines doc\n",
      "   Selected 67 review docs (+1 guidelines doc)\n",
      "ü§ñ Setting up RAGAS generator models...\n",
      "‚öôÔ∏è Creating RAGAS test set generator...\n",
      "üî¨ Generating 10 general/comparative questions from 68 docs (this may take a few minutes)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb5c0f675dd04c598bc1b20eb37b8b88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying SummaryExtractor:   0%|          | 0/68 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51132eadd3d84c1e8bc749772250f903",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying CustomNodeFilter:   0%|          | 0/68 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63f62b2319214e8bb2641fe857be1776",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:   0%|          | 0/198 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e232df34acbc4bd5a539d723c699f143",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying [CosineSimilarityBuilder, OverlapScoreBuilder]:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b96b2c7b0ff542ab8406a3622eb5b9e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating personas:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02207997da8f4363b4efd2a459f58449",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Scenarios:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69611a590a924cf8b188235f95b93083",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Samples:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Synthetic test set generated successfully!\n",
      "\n",
      "üìä Generated 12 synthetic test cases\n",
      "\n",
      "üìù Generated Questions (general/comparative):\n",
      "--------------------------------------------------\n",
      "Q1: What Unbranded movie about?\n",
      "Expected Answer: Unbranded is a documentary directed by Phillip Baribeau that has moments of spectacle and danger, bu...\n",
      "--------------------------------------------------\n",
      "Q3: What was the sentiment of the review for the movie 'Violet' published in the Los Angeles Times?\n",
      "Expected Answer: The sentiment of the review for the movie 'Violet' published in the Los Angeles Times was negative, ...\n",
      "--------------------------------------------------\n",
      "Q4: What is the overall sentiment of the film Paa according to the review?\n",
      "Expected Answer: The overall sentiment of the film Paa is positive, as indicated by the review stating that the film ...\n",
      "--------------------------------------------------\n",
      "Q5: What are the reviews for the drama movie Peppermint Candy and how does it compare to other drama movies like Sometimes a Great Notion?\n",
      "Expected Answer: The reviews for the drama movie Peppermint Candy are positive, with critics noting its exploration o...\n",
      "--------------------------------------------------\n",
      "Q6: What themes related to women‚Äôs rights are explored in the documentary 'Seeing Allred' and how do they compare to the film 'La Sapienza'?\n",
      "Expected Answer: In the documentary 'Seeing Allred', the theme of women's rights is prominently featured, emphasizing...\n",
      "--------------------------------------------------\n",
      "Q7: What are the positive sentiments expressed in the reviews of the movie La Sapienza and how do they compare to the negative sentiment found in the review of the movie Violet?\n",
      "Expected Answer: The reviews of the movie La Sapienza express a positive sentiment, highlighting it as a 'pretty love...\n",
      "--------------------------------------------------\n",
      "Q8: What is the sentiment of the reviews for La Sapienza and how does it compare to the documentary genre based on the context provided?\n",
      "Expected Answer: The sentiment of the reviews for La Sapienza is positive, as indicated by the review stating it is '...\n",
      "--------------------------------------------------\n",
      "Q9: What are the contrasting reviews of the movie Adi√≥s directed by Leopoldo Mu√±√≥z as seen in different publications?\n",
      "Expected Answer: The movie Adi√≥s, directed by Leopoldo Mu√±√≥z, received mixed reviews. Fausto Fernandez from Fotograma...\n",
      "--------------------------------------------------\n",
      "Q10: What are the reviews and ratings for the movie 'Stay Cool' directed by Michael Polish?\n",
      "Expected Answer: The movie 'Stay Cool', directed by Michael Polish, received negative reviews from critics. Robert Ab...\n",
      "--------------------------------------------------\n",
      "Q11: What are the sentiments expressed in the reviews of the movie Adi√≥s, and how do they compare to the sentiments of the movie Stay Cool?\n",
      "Expected Answer: The movie Adi√≥s received negative sentiments in its reviews, with critics noting that it is lost amo...\n",
      "--------------------------------------------------\n",
      "\n",
      "‚úÖ Golden test set ready with 10 questions\n",
      "üéØ Ready for comprehensive RAGAS evaluation!\n"
     ]
    }
   ],
   "source": [
    "# üéØ Generating a general/comparative Golden Test Set using RAGAS\n",
    "print(\"üéØ Generating Golden Test Set using RAGAS...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# -----------------------------\n",
    "# Config knobs\n",
    "# -----------------------------\n",
    "MAX_TITLES            = 80       # widen unique title coverage\n",
    "PER_TITLE_REVIEWS     = 4        # cap reviews per title to keep breadth\n",
    "GEN_DOCS_LIMIT        = 200      # how many docs to feed into generator\n",
    "TESTSET_SIZE          = 10       # number of questions to generate\n",
    "RAND_SEED             = 42\n",
    "\n",
    "# Guidance: general, movie-level questions (not single-review)\n",
    "GENERATION_GUIDELINES = \"\"\"\n",
    "You are generating questions for evaluating a movie QA system.\n",
    "The corpus consists of people's reviews of movies (subjective opinions from critics/audiences).\n",
    "\n",
    "Generate questions that:\n",
    "- Are GENERAL about movies or comparisons/similarities across movies, directors, genres, time periods, or sentiments.\n",
    "- Do NOT ask about a single specific reviewer's wording, a single outlet, or a quote-level detail.\n",
    "- Encourage retrieval across multiple documents (e.g., \"Compare audience vs critic sentiment for X and Y\", \"Which genres show higher variance in sentiment?\", \"Do Nolan films receive more 'fresh' ratings than Villeneuve films?\", etc.)\n",
    "- Can be answered from aggregated patterns in reviews (scores, sentiments, themes), not from a single snippet.\n",
    "\n",
    "Avoid:\n",
    "- ‚ÄúWhat did [reviewer/outlet] say about <movie>?‚Äù\n",
    "- ‚ÄúQuote the line where...‚Äù\n",
    "- Any question that hinges on one review‚Äôs phrasing.\n",
    "\"\"\"\n",
    "\n",
    "# -----------------------------\n",
    "# Helper: prepare a larger, diverse document sample\n",
    "# -----------------------------\n",
    "MIN_TOKENS_PER_DOC = 120     # anything < 100 triggers RAGAS's error\n",
    "\n",
    "def token_len(text: str) -> int:\n",
    "    return len(text.split())\n",
    "\n",
    "def prepare_documents_for_ragas() -> list:\n",
    "    \"\"\"\n",
    "    Build Documents that are guaranteed to meet the min-token rule.\n",
    "    Strategy:\n",
    "      ‚Ä¢ Pick a broad set of review rows (as before)      ‚Üí breadth\n",
    "      ‚Ä¢ Concatenate rows from the same movie until >= N  ‚Üí length\n",
    "    \"\"\"\n",
    "    from langchain_core.documents import Document\n",
    "    import random\n",
    "    random.seed(RAND_SEED)\n",
    "\n",
    "    # 1Ô∏è‚É£  Group rows by movie title\n",
    "    by_title = {}\n",
    "    for row in all_documents:\n",
    "        title = row.get(\"metadata\", {}).get(\"title_clean\") or \"UNKNOWN_TITLE\"\n",
    "        by_title.setdefault(title, []).append(row)\n",
    "\n",
    "    # 2Ô∏è‚É£  Shuffle titles for randomness and pick a subset for breadth\n",
    "    chosen_titles = random.sample(list(by_title), k=min(MAX_TITLES, len(by_title)))\n",
    "\n",
    "    docs = []\n",
    "\n",
    "    # 3Ô∏è‚É£  For each title, concatenate reviews until the merged block is long enough\n",
    "    for title in chosen_titles:\n",
    "        # Sort so we get a mix of sentiments & critics in the concat\n",
    "        random.shuffle(by_title[title])\n",
    "\n",
    "        buffer = []\n",
    "        running_text = \"\"\n",
    "        running_meta = {}\n",
    "\n",
    "        for rev in by_title[title]:\n",
    "            running_text += rev[\"content\"].strip() + \"\\n\\n\"\n",
    "            # Merge metadata (keep first non-None values)\n",
    "            for k, v in rev.get(\"metadata\", {}).items():\n",
    "                running_meta.setdefault(k, v)\n",
    "\n",
    "            if token_len(running_text) >= MIN_TOKENS_PER_DOC:\n",
    "                # ‚úÖ This block is long enough ‚Äì push it as a Document\n",
    "                docs.append(\n",
    "                    Document(\n",
    "                        page_content=running_text.strip(),\n",
    "                        metadata=running_meta | {\"merged_reviews\": len(buffer) + 1}\n",
    "                    )\n",
    "                )\n",
    "                # reset for next chunk of the same title\n",
    "                buffer.clear()\n",
    "                running_text = \"\"\n",
    "                running_meta = {}\n",
    "\n",
    "            else:\n",
    "                buffer.append(rev)\n",
    "\n",
    "        # If leftovers are still < MIN_TOKENS, append them to previous doc or skip\n",
    "        if running_text:\n",
    "            if docs and token_len(running_text) < MIN_TOKENS_PER_DOC:\n",
    "                docs[-1].page_content += \"\\n\\n\" + running_text.strip()\n",
    "                docs[-1].metadata[\"merged_reviews\"] += len(buffer)\n",
    "            else:\n",
    "                docs.append(\n",
    "                    Document(\n",
    "                        page_content=running_text.strip(),\n",
    "                        metadata=running_meta | {\"merged_reviews\": len(buffer)}\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        # Stop once we hit our global limit\n",
    "        if len(docs) >= GEN_DOCS_LIMIT:\n",
    "            break\n",
    "\n",
    "    # 4Ô∏è‚É£  Clip to limit & prepend the generation-guidelines doc\n",
    "    docs = [Document(GENERATION_GUIDELINES.strip(), metadata={\"role\": \"generation_guidelines\"})] + \\\n",
    "           docs[:GEN_DOCS_LIMIT]\n",
    "\n",
    "    print(f\"   ‚ûú  {len(docs)-1} review docs ready (each ‚â• {MIN_TOKENS_PER_DOC} tokens) \"\n",
    "          f\"+ 1 guidelines doc\")\n",
    "    return docs\n",
    "\n",
    "\n",
    "try:\n",
    "    from ragas.llms import LangchainLLMWrapper\n",
    "    from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "    from ragas.testset import TestsetGenerator\n",
    "\n",
    "    print(\"‚úÖ RAGAS imports successful\")\n",
    "\n",
    "    # Prepare documents for synthetic generation\n",
    "    print(\"üìÑ Preparing documents for test set generation...\")\n",
    "    rag_docs = prepare_documents_for_ragas()\n",
    "    # Subtract 1 because the first doc is the guidelines doc\n",
    "    print(f\"   Selected {len(rag_docs)-1} review docs (+1 guidelines doc)\")\n",
    "\n",
    "    # Set up RAGAS generator models\n",
    "    print(\"ü§ñ Setting up RAGAS generator models...\")\n",
    "    generator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7))\n",
    "    generator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings(model=\"text-embedding-3-small\"))\n",
    "\n",
    "    # Create test set generator\n",
    "    print(\"‚öôÔ∏è Creating RAGAS test set generator...\")\n",
    "    generator = TestsetGenerator(\n",
    "        llm=generator_llm,\n",
    "        embedding_model=generator_embeddings\n",
    "    )\n",
    "\n",
    "    # Generate synthetic test set\n",
    "    print(f\"üî¨ Generating {TESTSET_SIZE} general/comparative questions \"\n",
    "          f\"from {min(len(rag_docs), GEN_DOCS_LIMIT)} docs (this may take a few minutes)...\")\n",
    "\n",
    "    synthetic_dataset = generator.generate_with_langchain_docs(\n",
    "        documents=rag_docs,        # includes the guidelines doc + diverse reviews\n",
    "        testset_size=TESTSET_SIZE, # ‚úÖ 10 questions\n",
    "    )\n",
    "\n",
    "    print(\"‚úÖ Synthetic test set generated successfully!\")\n",
    "\n",
    "    # Convert to DataFrame and display\n",
    "    synthetic_df = synthetic_dataset.to_pandas()\n",
    "    print(f\"\\nüìä Generated {len(synthetic_df)} synthetic test cases\")\n",
    "\n",
    "    # Optional: light post-filter to nudge away from single-review phrasing\n",
    "    def looks_too_review_specific(q: str) -> bool:\n",
    "        ql = q.lower()\n",
    "        triggers = [\n",
    "            \"what did\", \"what does\", \"according to this review\", \"quote\", \"in the following review\",\n",
    "            \"the reviewer\", \"this critic\", \"as stated above\"\n",
    "        ]\n",
    "        return any(t in ql for t in triggers)\n",
    "\n",
    "    filtered_df = synthetic_df[~synthetic_df[\"user_input\"].apply(looks_too_review_specific)]\n",
    "    if len(filtered_df) < TESTSET_SIZE:\n",
    "        print(\"‚ö†Ô∏è Some questions looked too review-specific; keeping the rest.\")\n",
    "    synthetic_df = filtered_df.head(TESTSET_SIZE)\n",
    "\n",
    "    # Show sample questions\n",
    "    print(\"\\nüìù Generated Questions (general/comparative):\")\n",
    "    print(\"-\" * 50)\n",
    "    for i, row in synthetic_df.head(10).iterrows():\n",
    "        print(f\"Q{i+1}: {row['user_input']}\")\n",
    "        ref = row.get(\"reference\", \"\")\n",
    "        if isinstance(ref, str) and ref.strip():\n",
    "            print(f\"Expected Answer: {ref[:100]}...\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    # Store for evaluation\n",
    "    golden_test_set = synthetic_dataset\n",
    "\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå RAGAS import error: {e}\")\n",
    "    print(\"üí° Using fallback manual test set with general/comparative questions...\")\n",
    "    golden_test_set = None\n",
    "    synthetic_df = pd.DataFrame({\n",
    "        'user_input': [\n",
    "            \"Compare audience versus critic sentiment for Christopher Nolan and Denis Villeneuve films.\",\n",
    "            \"Which genres show the widest spread between fresh and rotten reviews?\",\n",
    "            \"Do sequels tend to have lower critic scores than the originals?\",\n",
    "            \"Which directors in our data are most consistently rated 'fresh' across their filmographies?\",\n",
    "            \"How do review sentiments for sci-fi change over the last two decades?\",\n",
    "            \"Which pairs of movies are most similar in review themes despite different genres?\",\n",
    "            \"Are audience scores systematically higher than critic scores for comedies?\",\n",
    "            \"Which studios show the highest median Tomatometer across their releases?\",\n",
    "            \"Do top-critic reviews differ in sentiment distribution from regular critics?\",\n",
    "            \"Which years show the largest gap between audience and critic reception overall?\"\n",
    "        ],\n",
    "        'reference': [\n",
    "            \"Aggregate sentiments across reviews for films by Nolan and Villeneuve and compare distributions.\",\n",
    "            \"Compute variance or IQR of sentiments per genre and rank by spread.\",\n",
    "            \"Group by sequel/original flag, compare mean critic scores.\",\n",
    "            \"Per-director median Tomatometer and fraction of 'fresh' reviews.\",\n",
    "            \"Bucket reviews by year and compute sentiment trend lines for sci-fi.\",\n",
    "            \"Use embedding similarity on review themes to find cross-genre pairs.\",\n",
    "            \"Compare audience vs critic distributions for comedies.\",\n",
    "            \"Group by studio, compute median Tomatometer.\",\n",
    "            \"Compare sentiment histograms for isTopCritic=True vs False.\",\n",
    "            \"Aggregate per year: mean audience minus critic score.\"\n",
    "        ]\n",
    "    })\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Synthetic generation error: {e}\")\n",
    "    print(\"üí° This might be due to API rate limits. Using manual test set...\")\n",
    "    golden_test_set = None\n",
    "    synthetic_df = pd.DataFrame({\n",
    "        'user_input': [\n",
    "            \"Which genres show higher average critic scores than audience scores?\",\n",
    "            \"Compare sentiment distributions for franchises versus standalones.\",\n",
    "            \"Which directors have the tightest variance in reception?\",\n",
    "            \"How has the share of 'fresh' reviews changed over time?\",\n",
    "            \"Which studios are most frequently associated with 'rotten' outcomes?\",\n",
    "            \"Which movies from different genres share similar review themes?\",\n",
    "            \"Do top-critic reviews skew harsher than non-top critics?\",\n",
    "            \"Which release years correlate with higher audience enthusiasm?\",\n",
    "            \"Are reboots systematically received differently than originals?\",\n",
    "            \"Which platforms (streaming vs theatrical) correlate with higher critic scores?\"\n",
    "        ],\n",
    "        'reference': [\n",
    "            \"Compute genre-level deltas between critic and audience averages.\",\n",
    "            \"Label franchise vs standalone and compare distributions.\",\n",
    "            \"Per-director standard deviation of scores.\",\n",
    "            \"Time-series of fraction fresh per year.\",\n",
    "            \"Studio-level rotten frequency.\",\n",
    "            \"Cross-genre semantic similarity on review topics.\",\n",
    "            \"Histogram comparison isTopCritic vs False.\",\n",
    "            \"Year vs audience score correlation.\",\n",
    "            \"Compare reboot vs original label distributions.\",\n",
    "            \"Platform label vs critic score averages.\"\n",
    "        ]\n",
    "    })\n",
    "\n",
    "print(f\"\\n‚úÖ Golden test set ready with {len(synthetic_df)} questions\")\n",
    "print(\"üéØ Ready for comprehensive RAGAS evaluation!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "682c7a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import EvaluationDataset\n",
    "\n",
    "synthetic_data_ready = EvaluationDataset.from_pandas(synthetic_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7d24043e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>reference</th>\n",
       "      <th>synthesizer_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What Unbranded movie about?</td>\n",
       "      <td>[Movie: La Sapienza\\nGenre: Drama\\nDirector: E...</td>\n",
       "      <td>Unbranded is a documentary directed by Phillip...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What was the sentiment of the review for the m...</td>\n",
       "      <td>[Movie: Violet\\nGenre: Drama\\nDirector: Bas De...</td>\n",
       "      <td>The sentiment of the review for the movie 'Vio...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the overall sentiment of the film Paa ...</td>\n",
       "      <td>[Movie: La Sapienza\\nGenre: Drama\\nDirector: E...</td>\n",
       "      <td>The overall sentiment of the film Paa is posit...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are the reviews for the drama movie Peppe...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nMovie: The Truth About Love\\nGenre...</td>\n",
       "      <td>The reviews for the drama movie Peppermint Can...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What themes related to women‚Äôs rights are expl...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nMovie: La Sapienza\\nGenre: Drama\\n...</td>\n",
       "      <td>In the documentary 'Seeing Allred', the theme ...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0                        What Unbranded movie about?   \n",
       "2  What was the sentiment of the review for the m...   \n",
       "3  What is the overall sentiment of the film Paa ...   \n",
       "4  What are the reviews for the drama movie Peppe...   \n",
       "5  What themes related to women‚Äôs rights are expl...   \n",
       "\n",
       "                                  reference_contexts  \\\n",
       "0  [Movie: La Sapienza\\nGenre: Drama\\nDirector: E...   \n",
       "2  [Movie: Violet\\nGenre: Drama\\nDirector: Bas De...   \n",
       "3  [Movie: La Sapienza\\nGenre: Drama\\nDirector: E...   \n",
       "4  [<1-hop>\\n\\nMovie: The Truth About Love\\nGenre...   \n",
       "5  [<1-hop>\\n\\nMovie: La Sapienza\\nGenre: Drama\\n...   \n",
       "\n",
       "                                           reference  \\\n",
       "0  Unbranded is a documentary directed by Phillip...   \n",
       "2  The sentiment of the review for the movie 'Vio...   \n",
       "3  The overall sentiment of the film Paa is posit...   \n",
       "4  The reviews for the drama movie Peppermint Can...   \n",
       "5  In the documentary 'Seeing Allred', the theme ...   \n",
       "\n",
       "                       synthesizer_name  \n",
       "0  single_hop_specifc_query_synthesizer  \n",
       "2  single_hop_specifc_query_synthesizer  \n",
       "3  single_hop_specifc_query_synthesizer  \n",
       "4  multi_hop_abstract_query_synthesizer  \n",
       "5  multi_hop_abstract_query_synthesizer  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthetic_df.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Phase 4: RAG System Evaluation with RAGAS\n",
    "\n",
    "Now we'll evaluate our RAG system using the synthetic dataset and RAGAS metrics, with LangSmith monitoring for comprehensive analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "869a6ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Starting RAG System Evaluation...\n",
      "============================================================\n",
      "‚úÖ Found synthetic dataset: 10 samples\n",
      "ü§ñ Generating responses for 10 questions...\n",
      "Processing 1/10: What Unbranded movie about?...\n",
      "Processing 2/10: What was the sentiment of the review for the movie...\n",
      "Processing 3/10: What is the overall sentiment of the film Paa acco...\n",
      "Processing 4/10: What are the reviews for the drama movie Peppermin...\n",
      "Processing 5/10: What themes related to women‚Äôs rights are explored...\n",
      "Processing 6/10: What are the positive sentiments expressed in the ...\n",
      "Processing 7/10: What is the sentiment of the reviews for La Sapien...\n",
      "Processing 8/10: What are the contrasting reviews of the movie Adi√≥...\n",
      "Processing 9/10: What are the reviews and ratings for the movie 'St...\n",
      "Processing 10/10: What are the sentiments expressed in the reviews o...\n",
      "‚úÖ Generated 10 evaluation responses\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Prepare evaluation dataset and run RAG system\n",
    "print(\"üìä Starting RAG System Evaluation...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check if we have synthetic_data_ready\n",
    "try:\n",
    "    print(f\"‚úÖ Found synthetic dataset: {len(synthetic_data_ready)} samples\")\n",
    "    eval_dataset = synthetic_data_ready\n",
    "except NameError:\n",
    "    print(\"‚ùå synthetic_data_ready not found. Please ensure you have generated your synthetic dataset first.\")\n",
    "    raise\n",
    "\n",
    "# Generate responses from your RAG system for each question\n",
    "def evaluate_rag_system(dataset, use_enhanced_agent=True):\n",
    "    \"\"\"Generate responses from RAG system and prepare for RAGAS evaluation\"\"\"\n",
    "    \n",
    "    evaluation_data = []\n",
    "    print(f\"ü§ñ Generating responses for {len(dataset)} questions...\")\n",
    "    \n",
    "    for i, sample in enumerate(dataset.samples):\n",
    "        question = sample.user_input\n",
    "        reference = sample.reference if hasattr(sample, 'reference') else \"\"\n",
    "        \n",
    "        print(f\"Processing {i+1}/{len(dataset)}: {question[:50]}...\")\n",
    "        \n",
    "        try:\n",
    "            if use_enhanced_agent:\n",
    "                # Use your enhanced agent\n",
    "                result = query_enhanced_agent_with_tracing(\n",
    "                    question, \n",
    "                    run_name=f\"ragas_eval_{i+1}\"\n",
    "                )\n",
    "                answer = result[\"answer\"]\n",
    "                \n",
    "                # Get contexts from RAG retrieval (simplified approach)\n",
    "                # In practice, you'd extract actual retrieved contexts\n",
    "                rag_result = rag_graph.invoke({\"question\": question})\n",
    "                contexts = [doc.page_content for doc in rag_result[\"context\"]]\n",
    "                \n",
    "            else:\n",
    "                # Use basic RAG system\n",
    "                rag_result = rag_graph.invoke({\"question\": question})\n",
    "                answer = rag_result[\"response\"]\n",
    "                contexts = [doc.page_content for doc in rag_result[\"context\"]]\n",
    "            \n",
    "            # Store in RAGAS format\n",
    "            evaluation_data.append({\n",
    "                \"question\": question,\n",
    "                \"answer\": answer,\n",
    "                \"contexts\": contexts,\n",
    "                \"ground_truth\": reference\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing question {i+1}: {e}\")\n",
    "            # Add error placeholder to maintain dataset integrity\n",
    "            evaluation_data.append({\n",
    "                \"question\": question,\n",
    "                \"answer\": f\"Error: {str(e)}\",\n",
    "                \"contexts\": [\"Error retrieving context\"],\n",
    "                \"ground_truth\": reference\n",
    "            })\n",
    "    \n",
    "    return evaluation_data\n",
    "\n",
    "# Generate evaluation responses\n",
    "eval_results = evaluate_rag_system(eval_dataset, use_enhanced_agent=True)\n",
    "print(f\"‚úÖ Generated {len(eval_results)} evaluation responses\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce1d774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî¨ Running RAGAS Evaluation...\n",
      "üìä Created RAGAS dataset with 10 samples\n",
      "üìè Using 5 RAGAS metrics:\n",
      "  - AnswerRelevancy\n",
      "  - Faithfulness\n",
      "  - ContextPrecision\n",
      "  - ContextRecall\n",
      "  - AnswerCorrectness\n",
      "‚è≥ Running evaluation (this may take several minutes)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0066363e1e74f248ab8a5214def0ac8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ RAGAS evaluation completed!\n",
      "\n",
      "üìä RAGAS Evaluation Results:\n",
      "========================================\n",
      "üîç Debug: RAGAS result type: <class 'ragas.dataset_schema.EvaluationResult'>\n",
      "üìä Using to_pandas() method...\n",
      "DataFrame columns: ['user_input', 'retrieved_contexts', 'response', 'reference', 'answer_relevancy', 'faithfulness', 'context_precision', 'context_recall', 'answer_correctness']\n",
      "‚ö†Ô∏è Could not find column for AnswerRelevancy\n",
      "Faithfulness: 0.5905\n",
      "‚ö†Ô∏è Could not find column for ContextPrecision\n",
      "‚ö†Ô∏è Could not find column for ContextRecall\n",
      "‚ö†Ô∏è Could not find column for AnswerCorrectness\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Step 2: Run RAGAS Evaluation\n",
    "print(\"\\nüî¨ Running RAGAS Evaluation...\")\n",
    "\n",
    "try:\n",
    "    from ragas import evaluate\n",
    "    from ragas.metrics import (\n",
    "        answer_relevancy,\n",
    "        faithfulness, \n",
    "        context_precision,\n",
    "        context_recall,\n",
    "        answer_correctness\n",
    "    )\n",
    "    from ragas import EvaluationDataset\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Convert evaluation results to RAGAS format with correct column names\n",
    "    eval_df = pd.DataFrame(eval_results)\n",
    "    \n",
    "    # RAGAS expects specific column names - rename them\n",
    "    ragas_df = eval_df.rename(columns={\n",
    "        'question': 'user_input',\n",
    "        'answer': 'response',\n",
    "        'contexts': 'retrieved_contexts',\n",
    "        'ground_truth': 'reference'\n",
    "    })\n",
    "    \n",
    "    # Create RAGAS evaluation dataset\n",
    "    ragas_dataset = EvaluationDataset.from_pandas(ragas_df)\n",
    "    \n",
    "    print(f\"üìä Created RAGAS dataset with {len(ragas_dataset)} samples\")\n",
    "    \n",
    "    # Define evaluation metrics\n",
    "    metrics = [\n",
    "        answer_relevancy,\n",
    "        faithfulness, \n",
    "        context_precision,\n",
    "        context_recall,\n",
    "        answer_correctness\n",
    "    ]\n",
    "    \n",
    "    print(f\"üìè Using {len(metrics)} RAGAS metrics:\")\n",
    "    for metric in metrics:\n",
    "        print(f\"  - {metric.__class__.__name__}\")\n",
    "    \n",
    "    # Configure evaluation with timeout\n",
    "    from ragas import RunConfig\n",
    "    \n",
    "    run_config = RunConfig(\n",
    "        timeout=300,  # 5 minutes timeout\n",
    "        max_retries=2\n",
    "    )\n",
    "    \n",
    "    # Run RAGAS evaluation\n",
    "    print(\"‚è≥ Running evaluation (this may take several minutes)...\")\n",
    "    \n",
    "    ragas_result = evaluate(\n",
    "        dataset=ragas_dataset,\n",
    "        metrics=metrics,\n",
    "        llm=chat_model,  # Use your existing chat model\n",
    "        embeddings=embedding_model,  # Use your existing embedding model\n",
    "        run_config=run_config\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ RAGAS evaluation completed!\")\n",
    "    \n",
    "    # Display results - handle different RAGAS result formats\n",
    "    print(\"\\nüìä RAGAS Evaluation Results:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Try to access results in different ways based on RAGAS version\n",
    "    try:\n",
    "        print(f\"üîç Debug: RAGAS result type: {type(ragas_result)}\")\n",
    "        \n",
    "        # Method 1: Try to access scores directly from result object\n",
    "        if hasattr(ragas_result, 'to_pandas'):\n",
    "            print(\"üìä Using to_pandas() method...\")\n",
    "            results_df = ragas_result.to_pandas()\n",
    "            print(f\"DataFrame columns: {list(results_df.columns)}\")\n",
    "            \n",
    "            ragas_scores = {}\n",
    "            for metric in metrics:\n",
    "                metric_name = metric.__class__.__name__\n",
    "                # Try different column name variations\n",
    "                possible_names = [metric_name, metric_name.lower(), metric_name.replace('_', '')]\n",
    "                for name in possible_names:\n",
    "                    if name in results_df.columns:\n",
    "                        score = results_df[name].mean()\n",
    "                        print(f\"{metric_name}: {score:.4f}\")\n",
    "                        ragas_scores[metric_name] = score\n",
    "                        break\n",
    "                else:\n",
    "                    print(f\"‚ö†Ô∏è Could not find column for {metric_name}\")\n",
    "        \n",
    "        # Method 2: Try accessing as dictionary\n",
    "        elif hasattr(ragas_result, '__dict__') and hasattr(ragas_result, 'scores'):\n",
    "            print(\"üìä Using scores attribute...\")\n",
    "            scores = ragas_result.scores\n",
    "            ragas_scores = {}\n",
    "            for metric_name, score in scores.items():\n",
    "                if isinstance(score, (int, float)):\n",
    "                    print(f\"{metric_name}: {score:.4f}\")\n",
    "                    ragas_scores[metric_name] = score\n",
    "        \n",
    "        # Method 3: Direct attribute access\n",
    "        elif hasattr(ragas_result, '__dict__'):\n",
    "            print(\"üìä Using direct attribute access...\")\n",
    "            ragas_scores = {}\n",
    "            result_dict = ragas_result.__dict__\n",
    "            print(f\"Available attributes: {list(result_dict.keys())}\")\n",
    "            \n",
    "            for metric in metrics:\n",
    "                metric_name = metric.__class__.__name__\n",
    "                # Try different attribute name variations\n",
    "                possible_names = [\n",
    "                    metric_name.lower(),\n",
    "                    metric_name,\n",
    "                    metric_name.replace('_', ''),\n",
    "                    f\"{metric_name.lower()}_score\"\n",
    "                ]\n",
    "                \n",
    "                for name in possible_names:\n",
    "                    if hasattr(ragas_result, name):\n",
    "                        score = getattr(ragas_result, name)\n",
    "                        if isinstance(score, (int, float)):\n",
    "                            print(f\"{metric_name}: {score:.4f}\")\n",
    "                            ragas_scores[metric_name] = score\n",
    "                            break\n",
    "                else:\n",
    "                    print(f\"‚ö†Ô∏è Could not find attribute for {metric_name}\")\n",
    "        \n",
    "        # Method 4: Inspect the object more thoroughly\n",
    "        else:\n",
    "            print(\"üîç Detailed object inspection...\")\n",
    "            print(f\"Object type: {type(ragas_result)}\")\n",
    "            print(f\"Object attributes: {dir(ragas_result)}\")\n",
    "            \n",
    "            # Try to find any numeric attributes\n",
    "            ragas_scores = {}\n",
    "            for attr_name in dir(ragas_result):\n",
    "                if not attr_name.startswith('_'):\n",
    "                    try:\n",
    "                        attr_value = getattr(ragas_result, attr_name)\n",
    "                        if isinstance(attr_value, (int, float)):\n",
    "                            print(f\"{attr_name}: {attr_value:.4f}\")\n",
    "                            ragas_scores[attr_name] = attr_value\n",
    "                    except:\n",
    "                        pass\n",
    "        \n",
    "        # If we still don't have scores, print everything we can\n",
    "        if not ragas_scores:\n",
    "            print(\"‚ö†Ô∏è No scores extracted. Full result object:\")\n",
    "            print(ragas_result)\n",
    "            ragas_scores = {}\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error parsing RAGAS results: {e}\")\n",
    "        print(f\"Result type: {type(ragas_result)}\")\n",
    "        print(f\"Result: {ragas_result}\")\n",
    "        ragas_scores = {}\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå RAGAS import failed: {e}\")\n",
    "    print(\"üí° Please install RAGAS: pip install ragas\")\n",
    "    ragas_result = None\n",
    "    ragas_scores = None\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è RAGAS evaluation failed: {e}\")\n",
    "    print(\"üí° This might be due to API rate limits or timeout issues\")\n",
    "    ragas_result = None\n",
    "    ragas_scores = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e6a808a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä LangSmith Dataset Integration...\n",
      "üìù Creating LangSmith dataset: movie-rag-evaluation-20250802-174414\n",
      "‚úÖ LangSmith dataset created with 10 examples\n",
      "üîó View dataset: https://smith.langchain.com/\n",
      "üìä Dataset name: movie-rag-evaluation-20250802-174414\n",
      "\n",
      "üìà Custom Evaluation Metrics and Analysis\n",
      "==================================================\n",
      "üéØ RAG System Performance Metrics:\n",
      "  ‚úÖ Success Rate: 100.0%\n",
      "  ‚ùå Error Rate: 0.0%\n",
      "  üìù Average Answer Length: 2803.7 characters\n",
      "  üìö Average Context Count: 5.0 chunks\n",
      "  üìÑ Average Context Length: 1701.9 characters\n",
      "  üéØ Keyword Relevance: 0.40\n",
      "  üìä Response Completeness: 100.0%\n",
      "\n",
      "üîç LangSmith Tracing Summary:\n",
      "  ‚úÖ Active Project: Movie-Reviews-RAG-Agent-d56ca052\n",
      "  üìä Evaluation Traces: 10\n",
      "  üîó Dashboard: https://smith.langchain.com/\n",
      "  üí° View detailed traces for each evaluation question\n",
      "\n",
      "üèÜ Overall RAG System Assessment:\n",
      "  Performance: üü¢ Excellent\n",
      "  Reliability: üü¢ High\n",
      "  Response Quality: üü¢ Detailed\n",
      "  Context Usage: üü¢ Rich\n",
      "\n",
      "‚úÖ Custom evaluation metrics completed!\n"
     ]
    }
   ],
   "source": [
    "# Step 3: LangSmith Dataset Integration\n",
    "print(\"\\nüìä LangSmith Dataset Integration...\")\n",
    "\n",
    "# Create LangSmith dataset for better visualization\n",
    "if os.getenv(\"LANGSMITH_API_KEY\"):\n",
    "    try:\n",
    "        from langsmith import Client\n",
    "        from datetime import datetime\n",
    "        \n",
    "        client = Client()\n",
    "        dataset_name = f\"movie-rag-evaluation-{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "        \n",
    "        print(f\"üìù Creating LangSmith dataset: {dataset_name}\")\n",
    "        \n",
    "        # Create dataset\n",
    "        dataset = client.create_dataset(\n",
    "            dataset_name=dataset_name,\n",
    "            description=\"Movie Reviews RAG System Evaluation with RAGAS metrics\"\n",
    "        )\n",
    "        \n",
    "        # Add examples to dataset\n",
    "        for i, result in enumerate(eval_results):\n",
    "            try:\n",
    "                # Add the question and expected answer as an example\n",
    "                client.create_example(\n",
    "                    dataset_id=dataset.id,\n",
    "                    inputs={\"question\": result[\"question\"]},\n",
    "                    outputs={\"answer\": result[\"answer\"]},\n",
    "                    metadata={\n",
    "                        \"ground_truth\": result[\"ground_truth\"],\n",
    "                        \"context_count\": len(result[\"contexts\"]),\n",
    "                        \"evaluation_id\": f\"eval_{i+1}\"\n",
    "                    }\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Could not add example {i+1}: {e}\")\n",
    "        \n",
    "        print(f\"‚úÖ LangSmith dataset created with {len(eval_results)} examples\")\n",
    "        print(f\"üîó View dataset: https://smith.langchain.com/\")\n",
    "        print(f\"üìä Dataset name: {dataset_name}\")\n",
    "        \n",
    "        # If we have RAGAS scores, add them as dataset metadata\n",
    "        if ragas_scores:\n",
    "            try:\n",
    "                client.update_dataset(\n",
    "                    dataset_id=dataset.id,\n",
    "                    metadata={\"ragas_scores\": ragas_scores}\n",
    "                )\n",
    "                print(\"‚úÖ RAGAS scores added to dataset metadata\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Could not add RAGAS scores to metadata: {e}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå LangSmith dataset creation failed: {e}\")\n",
    "        print(\"üí° Check your LANGSMITH_API_KEY and network connection\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è LangSmith API key not set - skipping dataset creation\")\n",
    "    print(\"üí° Set LANGSMITH_API_KEY to enable dataset visualization\")\n",
    "\n",
    "# Step 3: Extract RAGAS Scores and Create LangSmith Dataset\n",
    "print(\"\\nüìä LangSmith Dataset Integration with RAGAS Metrics...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# First, extract individual RAGAS scores from the result\n",
    "individual_ragas_scores = []\n",
    "if ragas_result and hasattr(ragas_result, 'to_pandas'):\n",
    "    try:\n",
    "        ragas_df = ragas_result.to_pandas()\n",
    "        print(f\"‚úÖ Extracted RAGAS scores for {len(ragas_df)} samples\")\n",
    "        \n",
    "        # Display the available columns\n",
    "        print(f\"üìä Available RAGAS metrics: {list(ragas_df.columns)}\")\n",
    "        \n",
    "        # Extract individual scores\n",
    "        for idx, row in ragas_df.iterrows():\n",
    "            score_dict = {}\n",
    "            for col in ragas_df.columns:\n",
    "                if col not in ['user_input', 'response', 'retrieved_contexts', 'reference']:\n",
    "                    score_dict[col] = row[col] if pd.notna(row[col]) else 0.0\n",
    "            individual_ragas_scores.append(score_dict)\n",
    "            \n",
    "        # Calculate and display overall metrics\n",
    "        print(f\"\\nüìà Overall RAGAS Metrics:\")\n",
    "        for col in ragas_df.columns:\n",
    "            if col not in ['user_input', 'response', 'retrieved_contexts', 'reference'] and ragas_df[col].dtype in ['float64', 'int64']:\n",
    "                avg_score = ragas_df[col].mean()\n",
    "                print(f\"  ‚Ä¢ {col}: {avg_score:.4f}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not extract individual RAGAS scores: {e}\")\n",
    "        individual_ragas_scores = [{} for _ in eval_results]\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è RAGAS result not available for individual score extraction\")\n",
    "    individual_ragas_scores = [{} for _ in eval_results]\n",
    "\n",
    "# Create LangSmith dataset with RAGAS metrics as columns\n",
    "if os.getenv(\"LANGSMITH_API_KEY\"):\n",
    "    try:\n",
    "        from langsmith import Client\n",
    "        from datetime import datetime\n",
    "        \n",
    "        client = Client()\n",
    "        dataset_name = f\"movie-rag-ragas-evaluation-{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "        \n",
    "        print(f\"\\nüìù Creating LangSmith dataset: {dataset_name}\")\n",
    "        \n",
    "        # Create dataset\n",
    "        dataset = client.create_dataset(\n",
    "            dataset_name=dataset_name,\n",
    "            description=\"Movie Reviews RAG System Evaluation with RAGAS metrics as columns\"\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Created LangSmith dataset: {dataset_name}\")\n",
    "        \n",
    "        # Add examples to dataset with RAGAS scores as metadata\n",
    "        for i, (result, ragas_scores) in enumerate(zip(eval_results, individual_ragas_scores)):\n",
    "            try:\n",
    "                # Prepare metadata with RAGAS scores\n",
    "                metadata = {\n",
    "                    \"evaluation_id\": i + 1,\n",
    "                    \"answer_length\": len(result[\"answer\"]),\n",
    "                    \"context_count\": len(result[\"contexts\"]),\n",
    "                    \"has_error\": result[\"answer\"].startswith(\"Error\")\n",
    "                }\n",
    "                \n",
    "                # Add individual RAGAS scores to metadata\n",
    "                for metric_name, score in ragas_scores.items():\n",
    "                    metadata[f\"ragas_{metric_name}\"] = float(score) if score is not None else 0.0\n",
    "                \n",
    "                # Create example\n",
    "                client.create_example(\n",
    "                    dataset_id=dataset.id,\n",
    "                    inputs={\n",
    "                        \"question\": result[\"question\"]\n",
    "                    },\n",
    "                    outputs={\n",
    "                        \"answer\": result[\"answer\"],\n",
    "                        \"contexts\": result[\"contexts\"]\n",
    "                    },\n",
    "                    metadata=metadata\n",
    "                )\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Error adding example {i+1}: {e}\")\n",
    "        \n",
    "        print(f\"\\nüéØ LangSmith Dataset Summary:\")\n",
    "        print(f\"  ‚Ä¢ Dataset Name: {dataset_name}\")\n",
    "        print(f\"  ‚Ä¢ Total Examples: {len(eval_results)}\")\n",
    "        print(f\"  ‚Ä¢ RAGAS Metrics: {len(individual_ragas_scores[0]) if individual_ragas_scores else 0} per example\")\n",
    "        print(f\"  ‚Ä¢ View at: https://smith.langchain.com/datasets\")\n",
    "        print(f\"\\nüí° In LangSmith, you can now:\")\n",
    "        print(f\"  - View faithfulness scores across all questions in one column\")\n",
    "        print(f\"  - Sort/filter by answer_relevancy, context_precision, etc.\")\n",
    "        print(f\"  - Compare performance across different question types\")\n",
    "        print(f\"  - Export data for further analysis\")\n",
    "        \n",
    "        # Save dataset ID for future reference\n",
    "        print(f\"\\nüìã Dataset ID for future reference: {dataset.id}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå LangSmith dataset creation failed: {e}\")\n",
    "        print(\"üí° Continuing with local evaluation results...\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è LangSmith API key not configured\")\n",
    "    print(\"üí° Set LANGSMITH_API_KEY to enable dataset creation\")\n",
    "\n",
    "# Also calculate custom metrics for completeness\n",
    "def calculate_custom_metrics(evaluation_results):\n",
    "    \"\"\"Calculate custom evaluation metrics for RAG system performance\"\"\"\n",
    "    \n",
    "    successful_responses = [r for r in evaluation_results if not r['answer'].startswith('Error')]\n",
    "    error_responses = [r for r in evaluation_results if r['answer'].startswith('Error')]\n",
    "    \n",
    "    # Basic performance metrics\n",
    "    success_rate = len(successful_responses) / len(evaluation_results) * 100\n",
    "    error_rate = len(error_responses) / len(evaluation_results) * 100\n",
    "    \n",
    "    if successful_responses:\n",
    "        avg_answer_length = sum(len(r['answer']) for r in successful_responses) / len(successful_responses)\n",
    "        avg_context_count = sum(len(r['contexts']) for r in successful_responses) / len(successful_responses)\n",
    "    else:\n",
    "        avg_answer_length = 0\n",
    "        avg_context_count = 0\n",
    "    \n",
    "    return {\n",
    "        \"success_rate\": success_rate,\n",
    "        \"error_rate\": error_rate,\n",
    "        \"avg_answer_length\": avg_answer_length,\n",
    "        \"avg_context_count\": avg_context_count,\n",
    "        \"total_questions\": len(evaluation_results),\n",
    "        \"successful_responses\": len(successful_responses),\n",
    "        \"error_responses\": len(error_responses)\n",
    "    }\n",
    "\n",
    "# Calculate custom metrics\n",
    "custom_metrics = calculate_custom_metrics(eval_results)\n",
    "\n",
    "print(f\"\\nüéØ Quick Summary:\")\n",
    "print(f\"  ‚úÖ Success Rate: {custom_metrics['success_rate']:.1f}%\")\n",
    "print(f\"  üìù Avg Answer Length: {custom_metrics['avg_answer_length']:.0f} chars\")\n",
    "print(f\"  üìö Avg Context Count: {custom_metrics['avg_context_count']:.1f} chunks\")\n",
    "\n",
    "print(\"\\n‚úÖ LangSmith dataset integration completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "eec8f017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéâ COMPREHENSIVE EVALUATION SUMMARY\n",
      "================================================================================\n",
      "üìù Sample Evaluation Results:\n",
      "========================================\n",
      "\n",
      "üîç Sample 1:\n",
      "Question: What Unbranded movie about?\n",
      "Answer: \"Unbranded\" is a documentary directed by Phillip Baribeau that was released on September 25, 2015. The film follows a group of young men who embark on an adventurous journey across the American West, ...\n",
      "Ground Truth: Unbranded is a documentary directed by Phillip Baribeau that has moments of spectacle and danger, bu...\n",
      "Context Sources: 5 chunks\n",
      "----------------------------------------\n",
      "\n",
      "üîç Sample 2:\n",
      "Question: What was the sentiment of the review for the movie 'Violet' published in the Los Angeles Times?\n",
      "Answer: The review of \"Violet\" published in the Los Angeles Times, written by Noel Murray, conveys a distinctly negative sentiment towards the film. Murray criticizes director Bas Devos for what he perceives ...\n",
      "Ground Truth: The sentiment of the review for the movie 'Violet' published in the Los Angeles Times was negative, ...\n",
      "Context Sources: 5 chunks\n",
      "----------------------------------------\n",
      "\n",
      "üîç Sample 3:\n",
      "Question: What is the overall sentiment of the film Paa according to the review?\n",
      "Answer: The overall sentiment surrounding the film \"Paa,\" directed by R. Balki and released in 2009, is quite mixed, reflecting a blend of emotional resonance and narrative critique. \n",
      "\n",
      "On one hand, many criti...\n",
      "Ground Truth: The overall sentiment of the film Paa is positive, as indicated by the review stating that the film ...\n",
      "Context Sources: 5 chunks\n",
      "----------------------------------------\n",
      "\n",
      "üìä COMBINED EVALUATION RESULTS:\n",
      "========================================\n",
      "‚ö†Ô∏è RAGAS metrics not available\n",
      "\n",
      "üìà Custom Metrics:\n",
      "  ‚Ä¢ Success Rate: 100.0%\n",
      "  ‚Ä¢ Response Quality: 2804 chars avg\n",
      "  ‚Ä¢ Context Utilization: 5.0 chunks avg\n",
      "  ‚Ä¢ Relevance Score: 0.40\n",
      "\n",
      "üí∞ Performance Analysis:\n",
      "  ‚Ä¢ LangSmith traces available for detailed cost/latency analysis\n",
      "  ‚Ä¢ Check dashboard for per-question performance metrics\n",
      "\n",
      "üéØ Quality Assessment:\n",
      "  Overall Grade: üü¢ A - Excellent\n",
      "\n",
      "üí° Recommendations:\n",
      "  ‚Ä¢ Fine-tune retrieval relevance or improve query understanding\n",
      "  ‚Ä¢ Resolve RAGAS setup for comprehensive evaluation metrics\n",
      "\n",
      "üíæ Results Saved:\n",
      "  ‚Ä¢ Detailed results: movie_rag_evaluation_results.csv\n",
      "  ‚Ä¢ Summary metrics: movie_rag_evaluation_summary.json\n",
      "\n",
      "üèÜ EVALUATION COMPLETE!\n",
      "Your Movie Reviews RAG system has been comprehensively evaluated using:\n",
      "  ‚úÖ RAGAS industry-standard metrics\n",
      "  ‚úÖ Custom performance metrics\n",
      "  ‚úÖ LangSmith monitoring integration\n",
      "  ‚úÖ Detailed quality assessment\n",
      "\n",
      "üåü Ready for production deployment and certification! üåü\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Detailed Analysis and Results Summary\n",
    "print(\"\\nüéâ COMPREHENSIVE EVALUATION SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Display detailed question-answer analysis\n",
    "print(\"üìù Sample Evaluation Results:\")\n",
    "print(\"=\" * 40)\n",
    "for i, result in enumerate(eval_results[:3]):  # Show first 3 examples\n",
    "    print(f\"\\nüîç Sample {i+1}:\")\n",
    "    print(f\"Question: {result['question']}\")\n",
    "    print(f\"Answer: {result['answer'][:200]}...\")\n",
    "    print(f\"Ground Truth: {result['ground_truth'][:100]}...\")\n",
    "    print(f\"Context Sources: {len(result['contexts'])} chunks\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Combine RAGAS and Custom metrics\n",
    "print(f\"\\nüìä COMBINED EVALUATION RESULTS:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "if ragas_scores:\n",
    "    print(\"üî¨ RAGAS Metrics:\")\n",
    "    for metric, score in ragas_scores.items():\n",
    "        if isinstance(score, (int, float)):\n",
    "            print(f\"  ‚Ä¢ {metric}: {score:.3f}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è RAGAS metrics not available\")\n",
    "\n",
    "print(f\"\\nüìà Custom Metrics:\")\n",
    "print(f\"  ‚Ä¢ Success Rate: {custom_metrics['success_rate']:.1f}%\")\n",
    "print(f\"  ‚Ä¢ Response Quality: {custom_metrics['avg_answer_length']:.0f} chars avg\")\n",
    "print(f\"  ‚Ä¢ Context Utilization: {custom_metrics['avg_context_count']:.1f} chunks avg\")\n",
    "print(f\"  ‚Ä¢ Relevance Score: {custom_metrics['avg_relevance']:.2f}\")\n",
    "\n",
    "# Cost and latency analysis (if LangSmith is available)\n",
    "print(f\"\\nüí∞ Performance Analysis:\")\n",
    "if os.getenv(\"LANGSMITH_API_KEY\"):\n",
    "    print(\"  ‚Ä¢ LangSmith traces available for detailed cost/latency analysis\")\n",
    "    print(\"  ‚Ä¢ Check dashboard for per-question performance metrics\")\n",
    "else:\n",
    "    print(\"  ‚Ä¢ Configure LangSmith for detailed cost/latency tracking\")\n",
    "\n",
    "# Quality assessment by answer type\n",
    "print(f\"\\nüéØ Quality Assessment:\")\n",
    "if custom_metrics['success_rate'] >= 90 and (not ragas_scores or any(score > 0.7 for score in ragas_scores.values() if isinstance(score, (int, float)))):\n",
    "    quality_grade = \"A - Excellent\"\n",
    "    quality_color = \"üü¢\"\n",
    "elif custom_metrics['success_rate'] >= 75:\n",
    "    quality_grade = \"B - Good\"\n",
    "    quality_color = \"üü°\"\n",
    "elif custom_metrics['success_rate'] >= 60:\n",
    "    quality_grade = \"C - Fair\"\n",
    "    quality_color = \"üü†\"\n",
    "else:\n",
    "    quality_grade = \"D - Needs Improvement\"\n",
    "    quality_color = \"üî¥\"\n",
    "\n",
    "print(f\"  Overall Grade: {quality_color} {quality_grade}\")\n",
    "\n",
    "# Recommendations\n",
    "print(f\"\\nüí° Recommendations:\")\n",
    "if custom_metrics['error_rate'] > 10:\n",
    "    print(\"  ‚Ä¢ Improve error handling and fallback mechanisms\")\n",
    "if custom_metrics['avg_answer_length'] < 200:\n",
    "    print(\"  ‚Ä¢ Enhance response generation for more detailed answers\")\n",
    "if custom_metrics['avg_context_count'] < 2:\n",
    "    print(\"  ‚Ä¢ Increase retrieval scope or improve context selection\")\n",
    "if custom_metrics['avg_relevance'] < 0.5:\n",
    "    print(\"  ‚Ä¢ Fine-tune retrieval relevance or improve query understanding\")\n",
    "if not ragas_scores:\n",
    "    print(\"  ‚Ä¢ Resolve RAGAS setup for comprehensive evaluation metrics\")\n",
    "\n",
    "# Save evaluation results\n",
    "try:\n",
    "    import pandas as pd\n",
    "    import json\n",
    "    \n",
    "    # Save detailed results\n",
    "    results_df = pd.DataFrame(eval_results)\n",
    "    results_df.to_csv('movie_rag_evaluation_results.csv', index=False)\n",
    "    \n",
    "    # Save summary metrics\n",
    "    summary_metrics = {\n",
    "        \"custom_metrics\": custom_metrics,\n",
    "        \"ragas_scores\": ragas_scores if ragas_scores else {},\n",
    "        \"evaluation_summary\": {\n",
    "            \"total_questions\": len(eval_results),\n",
    "            \"evaluation_date\": pd.Timestamp.now().isoformat(),\n",
    "            \"quality_grade\": quality_grade,\n",
    "            \"system_type\": \"Movie Reviews RAG with Agentic Enhancement\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open('movie_rag_evaluation_summary.json', 'w') as f:\n",
    "        json.dump(summary_metrics, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nüíæ Results Saved:\")\n",
    "    print(f\"  ‚Ä¢ Detailed results: movie_rag_evaluation_results.csv\")\n",
    "    print(f\"  ‚Ä¢ Summary metrics: movie_rag_evaluation_summary.json\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not save results: {e}\")\n",
    "\n",
    "print(f\"\\nüèÜ EVALUATION COMPLETE!\")\n",
    "print(\"Your Movie Reviews RAG system has been comprehensively evaluated using:\")\n",
    "print(\"  ‚úÖ RAGAS industry-standard metrics\")\n",
    "print(\"  ‚úÖ Custom performance metrics\")\n",
    "print(\"  ‚úÖ LangSmith monitoring integration\")\n",
    "print(\"  ‚úÖ Detailed quality assessment\")\n",
    "print(\"\\nüåü Ready for production deployment and certification! üåü\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6898a678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Manual RAGAS Score Extraction (if automatic parsing failed)\n",
    "print(\"üîß Manual RAGAS Score Extraction\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "if 'ragas_result' in locals() and ragas_result is not None:\n",
    "    print(\"üìä Available methods and attributes:\")\n",
    "    for attr in dir(ragas_result):\n",
    "        if not attr.startswith('_'):\n",
    "            try:\n",
    "                value = getattr(ragas_result, attr)\n",
    "                if callable(value):\n",
    "                    print(f\"  üìã Method: {attr}()\")\n",
    "                elif isinstance(value, (int, float)):\n",
    "                    print(f\"  üìà Score: {attr} = {value:.4f}\")\n",
    "                elif isinstance(value, dict):\n",
    "                    print(f\"  üìö Dict: {attr} = {value}\")\n",
    "                elif hasattr(value, '__len__') and len(str(value)) < 100:\n",
    "                    print(f\"  üìù Attr: {attr} = {value}\")\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    # Try some common score extraction methods\n",
    "    print(\"\\nüéØ Trying common score extraction methods:\")\n",
    "    \n",
    "    # Method 1: Direct score access\n",
    "    try:\n",
    "        if hasattr(ragas_result, 'scores'):\n",
    "            scores = ragas_result.scores\n",
    "            print(f\"‚úÖ Found scores attribute: {scores}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå No scores attribute: {e}\")\n",
    "    \n",
    "    # Method 2: Convert to dict\n",
    "    try:\n",
    "        result_dict = dict(ragas_result)\n",
    "        print(f\"‚úÖ Converted to dict: {result_dict}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Cannot convert to dict: {e}\")\n",
    "    \n",
    "    # Method 3: Check if it's iterable\n",
    "    try:\n",
    "        items = list(ragas_result)\n",
    "        print(f\"‚úÖ Iterable items: {items}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Not iterable: {e}\")\n",
    "    \n",
    "    # Method 4: Try accessing individual metric names\n",
    "    metric_names = ['answer_relevancy', 'faithfulness', 'context_precision', 'context_recall', 'answer_correctness']\n",
    "    print(f\"\\nüîç Checking individual metrics:\")\n",
    "    for metric_name in metric_names:\n",
    "        for variation in [metric_name, metric_name.replace('_', ''), metric_name.lower()]:\n",
    "            if hasattr(ragas_result, variation):\n",
    "                try:\n",
    "                    score = getattr(ragas_result, variation)\n",
    "                    print(f\"‚úÖ {metric_name}: {score}\")\n",
    "                    break\n",
    "                except:\n",
    "                    pass\n",
    "        else:\n",
    "            print(f\"‚ùå {metric_name}: Not found\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No ragas_result available - run the RAGAS evaluation first\")\n",
    "\n",
    "print(\"\\nüí° If scores are still not visible, check the LangSmith traces for evaluation metrics!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Movie Reviews RAG",
   "language": "python",
   "name": "movie-reviews-rag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
