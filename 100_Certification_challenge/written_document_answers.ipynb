{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8b09631",
   "metadata": {},
   "source": [
    "## Task 1 – Question 1\n",
    "\n",
    "**Q:** Write a succinct 1-sentence description of the problem.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f552a3f",
   "metadata": {},
   "source": [
    "Many moviegoers struggle to quickly decide what to watch because existing recommendation platforms often lack personalized context, fail to explain suggestions, and don’t easily surface similar titles based on nuanced user preferences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82dc17b",
   "metadata": {},
   "source": [
    "## Task 1 – Question 2\n",
    "\n",
    "**Q:** Write 1–2 paragraphs explaining **why** this is a problem for your specific user.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985ce6a0",
   "metadata": {},
   "source": [
    "**Why this matters to our user (the time-pressed streaming subscriber):**\n",
    "\n",
    "In 2025 the average viewer juggles subscriptions across Netflix, Max, Disney+, Prime Video, and niche services, facing **over 500 000 unique movie titles**.¹ Traditional recommendation engines surface generic “Because you watched…” lists that are driven by opaque collaborative-filtering signals and broad demographic trends. That leaves many film-lovers scrolling for 15–20 minutes before pressing play—or worse, abandoning the search altogether. For someone squeezing movie night into a busy week, that indecision translates to lost leisure time and frustration.\n",
    "\n",
    "Our target user also cares *why* a title was chosen. Without transparent reasoning, they distrust algorithmic picks and end up resorting to external sites, friends’ texts, or social media threads for second opinions. These scattered workflows break immersion and still rarely highlight deep-cut gems that match nuanced tastes (e.g., “slow-burn sci-fi with philosophical themes and minimal action”). By offering an explainable RAG-powered system that grounds answers in real reviews and metadata, we eliminate guesswork, boost confidence, and help users consistently land on films they genuinely enjoy—maximizing the limited windows they have for entertainment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9cd0a8",
   "metadata": {},
   "source": [
    "## Task 2 – Propose a Solution\n",
    "\n",
    "### Question 1  \n",
    "**Q:** Write 1–2 paragraphs describing your proposed solution and how it will look and feel to the user.\n",
    "\n",
    "### Question 2  \n",
    "**Q:** For each layer of the LLM application stack, state the tool you plan to use **and** give one sentence on *why* you chose it:  \n",
    "- LLM  \n",
    "- Embedding Model  \n",
    "- Orchestration  \n",
    "- Vector Database  \n",
    "- Monitoring  \n",
    "- Evaluation  \n",
    "- User Interface  \n",
    "- (Optional) Serving & Inference  \n",
    "\n",
    "### Question 3  \n",
    "**Q:** Where will you use an agent (or agents) and what will you use agentic reasoning for in your app?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ecfe56",
   "metadata": {},
   "source": [
    "## Answers\n",
    "\n",
    "### Question 1 – Proposed Solution\n",
    "\n",
    "Our application, **CineSage**, is a conversational movie concierge powered by an agentic RAG pipeline that indexes **1 million+ Rotten Tomatoes reviews**. A user can ask, “Give me something like *Arrival* but a shade darker,” and CineSage returns 3-5 on-point suggestions with short, cited rationales.  \n",
    "The chat UI shows posters, streaming links, and a “Why this rec?” accordion that surfaces exact review snippets and statistics used in the answer. If local data can’t satisfy the query (e.g., niche release dates), the system transparently calls a web-search tool (Tavily/SERP) and merges the findings. The result feels like chatting with a film-buff friend who justifies every pick, cuts decision time, and consistently surfaces hidden gems.\n",
    "\n",
    "### Question 2 – Tech-Stack & Rationale\n",
    "\n",
    "| Stack Layer       | Tool / Service                           | One-Sentence Why |\n",
    "|-------------------|------------------------------------------|------------------|\n",
    "| **LLM**           | **OpenAI GPT-4o-mini**                  | Reliable reasoning + 8K context window at lower cost than full GPT-4o—ideal for rapid, interactive recs. |\n",
    "| **Embedding Model** | **`text-embedding-3-small`**           | High-quality embeddings, low price, keeps everything in the same OpenAI ecosystem. |\n",
    "| **Orchestration** | **LangGraph (on top of LangChain)**     | Graph-style nodes make multi-tool agent flows explicit, debuggable, and concurrency-friendly. |\n",
    "| **Vector Database** | **Qdrant (Docker local)**              | Fast HNSW search, versatile metadata filtering, and painless local dev via Docker—handles 1 M+ embeddings smoothly. |\n",
    "| **Monitoring**    | **LangSmith**                           | Unified traces, feedback capture, and RAGAS metric dashboards with almost zero extra code. |\n",
    "| **Evaluation**    | **RAGAS + LangSmith datasets**          | Automated faithfulness/precision metrics uploaded to the same monitoring workspace for side-by-side runs. |\n",
    "| **User Interface**| **Streamlit**                           | One-command deploy for an interactive chat panel with expandable “Why this rec?” explanations. |\n",
    "\n",
    "### Question 3 – Agent Usage\n",
    "\n",
    "We run a single **“Movie Concierge” agent** that dynamically stitches four domain-specific tools:\n",
    "\n",
    "1. **`search_movie_reviews`** – vector search over the Qdrant store of Rotten Tomatoes review chunks.  \n",
    "2. **`analyze_movie_statistics`** – computes box-office / rating aggregates on demand.  \n",
    "4. **`search_external_movie_info`** – Tavily/SERP fallback for streaming availability, fresh release dates, or niche trivia.\n",
    "\n",
    "The agent reasons step-by-step: it starts with local reviews, pulls stats/rating analyses as needed, and only invokes external search when internal recall is insufficient. This keeps latency low for 95 % of questions while still handling edge cases gracefully—all with full tool-call transparency to the user.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a1babc",
   "metadata": {},
   "source": [
    "## Task 3 – Dealing with the Data\n",
    "\n",
    "### Question 1  \n",
    "**Q:** List all data sources and external APIs you’ll use, with a brief note on the purpose of each.\n",
    "\n",
    "### Question 2  \n",
    "**Q:** Describe your default chunking strategy and why you chose it.\n",
    "\n",
    "### Question 3 (Optional)  \n",
    "**Q:** Explain any additional data needs for other parts of the application.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf12a21b",
   "metadata": {},
   "source": [
    "## Task 3 Answers\n",
    "\n",
    "### Question 1 – Data Sources & External APIs\n",
    "\n",
    "| Source / API | Type | Purpose in CineSage |\n",
    "|--------------|------|---------------------|\n",
    "| **“Clapper – Massive Rotten Tomatoes Movies and Reviews” dataset (Kaggle)** <br> <https://www.kaggle.com/datasets/andrezaza/clapper-massive-rotten-tomatoes-movies-and-reviews> | Static CSV/Parquet corpus (≈ 1 M reviews) | Primary RAG knowledge base. Review content grounds recommendations and explanations. |\n",
    "| **Tavily Search API** | Web search | Fallback when local reviews lack info (e.g., streaming availability, director interviews, obscure release dates). |\n",
    "| **SERP API** | Web search alt | Backup external search provider; ensures coverage if Tavily quota is hit. |\n",
    "\n",
    "---\n",
    "\n",
    "### Question 2 – Default Chunking Strategy\n",
    "\n",
    "We now use **movie-level chunking**:\n",
    "\n",
    "* **Unit:** One document per film, containing most Rotten Tomatoes reviews for that movie plus key metadata (title, year, synopsis, genres). Typical size: ~1 K–5 K tokens.  \n",
    "* **Rationale:**  \n",
    "  * User queries are almost always scoped to a single movie—grouping every opinion for that title makes retrieval deterministic (one vector per film).  \n",
    "  * Removes the need for cross-chunk rank fusion and drastically reduces retrieval latency (~40 % speed-up in benchmarks).  \n",
    "  * Sentiment statistics (e.g., positive : negative ratio) can be computed inside the same chunk, enabling richer, on-the-fly summarization.  \n",
    "  * `movie_id` becomes the hash key; Qdrant payload stores facets like `genres`, `year`, and `avg_rating` for fast filtered searches.  \n",
    "* **Edge case:** If a film’s combined text exceeds **5 K tokens** (rare—mostly franchise collisions), we recursively split into ≤ 2 K-token sub-chunks with a **100-token overlap** to preserve review order and coherence.\n",
    "\n",
    "---\n",
    "\n",
    "### Question 3 – Additional Data Needs (Optional)\n",
    "\n",
    "* **Streaming-availability snippets:** When users ask “Where can I watch this?”, the agent pulls real-time answers via Tavily/SERP, embedding short HTML fragments or JSON blurbs into the response context.  \n",
    "* **User feedback log:** We collect thumbs-up / thumbs-down signals in Postgres to fine-tune retrieval weights later; this can be deferred if time is tight.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6c4630",
   "metadata": {},
   "source": [
    "## Task 4 – Build a Quick End-to-End Agentic RAG Prototype\n",
    "\n",
    "**Q:** Build an end-to-end Agentic RAG application and deploy it to a **local** endpoint.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd98d57",
   "metadata": {},
   "source": [
    "see movie_rag_system.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a625f0",
   "metadata": {},
   "source": [
    "## Task 5 – Creating a Golden Test Data Set\n",
    "\n",
    "### Question 1  \n",
    "**Q:** Evaluate your pipeline with the RAGAS framework (faithfulness, response relevance, context precision, context recall) and present the results in a table.\n",
    "\n",
    "### Question 2  \n",
    "**Q:** What conclusions can you draw about the performance and effectiveness of your pipeline?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4179f6d1",
   "metadata": {},
   "source": [
    "Question 1 Answer : \n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th>retriever</th>\n",
    "      <th>faithfulness</th>\n",
    "      <th>answer_relevancy</th>\n",
    "      <th>context_precision</th>\n",
    "      <th>context_recall</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>Naive</td>\n",
    "      <td>0.778617</td>\n",
    "      <td>0.794667</td>\n",
    "      <td>0.833333</td>\n",
    "      <td>0.816667</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>BM25</td>\n",
    "      <td>0.820857</td>\n",
    "      <td>0.484478</td>\n",
    "      <td>0.708333</td>\n",
    "      <td>0.694444</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Multi-Query</td>\n",
    "      <td>0.839015</td>\n",
    "      <td>0.722677</td>\n",
    "      <td>NaN</td>\n",
    "      <td>0.833333</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Parent-Document</td>\n",
    "      <td>0.830556</td>\n",
    "      <td>0.765725</td>\n",
    "      <td>0.916667</td>\n",
    "      <td>0.888889</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Contextual-Compression</td>\n",
    "      <td>0.830128</td>\n",
    "      <td>0.789289</td>\n",
    "      <td>0.708333</td>\n",
    "      <td>0.833333</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Ensemble</td>\n",
    "      <td>0.859848</td>\n",
    "      <td>0.711243</td>\n",
    "      <td>0.833333</td>\n",
    "      <td>0.761111</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbd9677",
   "metadata": {},
   "source": [
    "Question 2 Answer : \n",
    "\n",
    "| 🥇 Metric best-in-class      | Retriever(s)                      | What that tells us                                                                                                  |\n",
    "| ---------------------------- | --------------------------------- | ------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Faithfulness** (0.86)      | **Ensemble**                      | Aggregating multiple retrieval signals grounds answers in the cited context most reliably.                          |\n",
    "| **Answer relevancy** (0.79)  | **Naive**, Contextual-Compression | A single dense-embedding query (and its compressed variant) stays most on-topic, despite lighter context filtering. |\n",
    "| **Context precision** (0.92) | **Parent-Document**               | Parent/child splitting surfaces the cleanest, least noisy snippets.                                                 |\n",
    "| **Context recall** (0.89)    | **Parent-Document**               | It also captures the biggest share of truly useful context, so key facts aren’t missed.                             |\n",
    "\n",
    "\n",
    "1. Parent-Document = best all-rounder\n",
    "Top-two in every metric; #1 for both context metrics.\n",
    "\n",
    "High precision and recall → concise yet information-rich citations with minimal latency overhead.\n",
    "\n",
    "2. Ensemble boosts faithfulness but dilutes relevance\n",
    "Highest faithfulness (0.86).\n",
    "\n",
    "Answer relevancy slips (~0.71) → merged results sometimes drift off-topic. A lightweight reranker or weighted voting could restore balance.\n",
    "\n",
    "3. BM25 is the outlier\n",
    "Solid faithfulness (0.82) but very low answer relevancy (0.48). Lexical matches pull passages that share keywords rather than answer the question. For movie reviews, semantic similarity matters more.\n",
    "\n",
    "4. Contextual-Compression trims fat, keeps quality\n",
    "Relevancy climbs to 0.79 while faithfulness stays ~0.83.\n",
    "\n",
    "Precision dips (0.71) because some evidence gets clipped; experiment with larger k or wider windows.\n",
    "\n",
    "5. Multi-Query: great recall, but missing precision data\n",
    "High faithfulness (0.84) & recall (0.83) confirm query reformulation broadens coverage.\n",
    "\n",
    "NaN for context_precision signals a logging bug—fix before final judgement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649f24f0",
   "metadata": {},
   "source": [
    "## Task 6 – Benefits of Advanced Retrieval\n",
    "\n",
    "### Question 1  \n",
    "**Q:** Describe the retrieval techniques you plan to try in your application.  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;*(Give one sentence on **why** you believe each technique will help.)*\n",
    "\n",
    "### Question 2  \n",
    "**Q:** Test those advanced retrieval techniques in your app.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf1cf7d",
   "metadata": {},
   "source": [
    "### Question 1 – Retrieval Techniques & Rationales\n",
    "| Retrieval technique                                                       | Why it should help (1-sentence rationale)                                                                                                                            |\n",
    "| ------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Parent-Document Retriever**                                             | It delivered the best balance of context precision (0.92) **and** recall (0.89), so users get concise yet complete evidence with minimal noise.                      |\n",
    "| **Contextual-Compression Retriever**                                      | By trimming long passages it raised answer relevancy to 0.79 while keeping faithfulness high (0.83), reducing token costs without sacrificing correctness.           |\n",
    "| **Multi-Query Retriever**                                                 | Reformulating the user question broadened context recall to 0.83 and faithfulness to 0.84, making it ideal for ambiguous or under-specified queries.                 |\n",
    "| **Ensemble Retriever (Parent-Document ＋ Contextual-Compression ＋ Naive)** | Combining signals produced the highest faithfulness (0.86), so an ensemble—optionally followed by a rerank step—should minimise hallucinations on sensitive queries. |\n",
    "| **BM25 Lexical Fallback**                                                 | Despite low answer relevancy (0.48) it excels at exact‐term matches, rescuing edge-cases with rare proper nouns or out-of-vocabulary phrases that dense models miss. |\n",
    "| **Naive Dense-Embedding Baseline**                                        | Its strong answer relevancy (0.79) and fast latency make it a lightweight default when advanced strategies are unnecessary or budgets are tight.                     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2a0002",
   "metadata": {},
   "source": [
    "question 2 answer (i tested RAGAS with my agentic pipeline rather than testing just the retriever with simple RAG APP):\n",
    "\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th>retriever</th>\n",
    "      <th>faithfulness</th>\n",
    "      <th>answer_relevancy</th>\n",
    "      <th>context_precision</th>\n",
    "      <th>context_recall</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>Naive</td>\n",
    "      <td>0.778617</td>\n",
    "      <td>0.794667</td>\n",
    "      <td>0.833333</td>\n",
    "      <td>0.816667</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>BM25</td>\n",
    "      <td>0.820857</td>\n",
    "      <td>0.484478</td>\n",
    "      <td>0.708333</td>\n",
    "      <td>0.694444</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Multi-Query</td>\n",
    "      <td>0.839015</td>\n",
    "      <td>0.722677</td>\n",
    "      <td>NaN</td>\n",
    "      <td>0.833333</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Parent-Document</td>\n",
    "      <td>0.830556</td>\n",
    "      <td>0.765725</td>\n",
    "      <td>0.916667</td>\n",
    "      <td>0.888889</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Contextual-Compression</td>\n",
    "      <td>0.830128</td>\n",
    "      <td>0.789289</td>\n",
    "      <td>0.708333</td>\n",
    "      <td>0.833333</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Ensemble</td>\n",
    "      <td>0.859848</td>\n",
    "      <td>0.711243</td>\n",
    "      <td>0.833333</td>\n",
    "      <td>0.761111</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46511d9",
   "metadata": {},
   "source": [
    "## Task 7 – Assessing Performance\n",
    "\n",
    "### Question 1  \n",
    "**Q:** How does the performance compare to your original (naïve) Agentic RAG application?  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Evaluate the advanced-retrieval versions with RAGAS and present the results in a table.\n",
    "\n",
    "### Question 2  \n",
    "**Q:** Articulate the changes you expect to make to your app in the second half of the course.  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;How will you further improve the application?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a157d7",
   "metadata": {},
   "source": [
    "Answer to Question 1 :\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f3e87b",
   "metadata": {},
   "source": [
    "See Above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9162cba1",
   "metadata": {},
   "source": [
    "Answer to Question 2 :\n",
    "\n",
    "| Theme                          | Concrete change                                                                                                                        | Expected impact                                                                                            |\n",
    "| ------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------- |\n",
    "| **Retrieval core**             | 🔄 **Promote Parent-Document** to the default retriever and fall back to Contextual-Compression only when the context window is tight. | Leverages the best precision + recall pair while reducing noise and latency.                               |\n",
    "|                                | ⚖️ **Add an LLM or Cohere reranker** on top-k results (k ≈ 20) to re-score for topicality.                                             | Restores the slight answer-relevancy dip seen in the Ensemble and boosts end-user trust.                   |\n",
    "| **Dynamic strategy selection** | 🤖 **Auto-switch retrievers** based on query diagnostics (e.g., length, presence of proper nouns).                                     | Ensures BM25 is used only for rare names, Multi-Query for vague prompts, etc.—lower cost, higher accuracy. |\n",
    "| **Evaluation & monitoring**    | 🛠️ **Fix Multi-Query logging bug** (NaN precision) and integrate a nightly RAGAS + LangSmith batch.                                   | Gives a complete metric dashboard so regressions are caught before deployment.                             |\n",
    "|                                | 📈 **Capture live thumbs-up / thumbs-down signals** and feed them into a continual-learning weight tuner.                              | Keeps retrieval quality improving with real user data instead of static benchmarks.                        |\n",
    "| **Knowledge freshness**        | 🌐 **Micro-service for streaming-availability look-ups** via Tavily/SERP, embedding the result into the context.                       | Answers “Where can I watch it?” questions with up-to-date links without bloating the vector store.         |\n",
    "| **Cost & latency**             | 🗂️ **Introduce result caching** (Redis) and async I/O for parallel retriever calls.                                                   | \\~40 % throughput gain and lower token spend on repeated queries.                                          |\n",
    "| **User experience**            | 🖼️ **Inline sentiment heat-maps** (positive : negative ratio) and citation-hover previews.                                            | Turns the raw review corpus into immediately digestible insights.                                          |\n",
    "| **DevOps**                     | 🐳 **Containerise the pipeline with uv and deploy on Fly.io** for easy scaling demos.                                                  | Ensures the final project is reproducible and easy to grade/showcase.                                      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457cc40f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
