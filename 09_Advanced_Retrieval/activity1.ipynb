{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb333066",
   "metadata": {},
   "source": [
    "## Activity #1: Retriever Evaluation with RAGAS\n",
    "\n",
    "This notebook evaluates different retriever methods using RAGAS for synthetic dataset generation.\n",
    "\n",
    "### Objectives:\n",
    "1. Create a \"golden dataset\" using RAGAS Synthetic Data Generation\n",
    "2. Evaluate 6 different retrievers on combined CSV + PDF data\n",
    "3. Compare performance, cost, and latency\n",
    "4. Provide recommendations\n",
    "\n",
    "### Data Sources:\n",
    "- **CSV Data**: Consumer complaint narratives\n",
    "- **PDF Data**: Federal Student Aid handbooks\n",
    "\n",
    "### Retrievers to Evaluate:\n",
    "- Naive Retrieval (Embedding-based)\n",
    "- BM25 Retriever\n",
    "- Multi-Query Retriever\n",
    "- Parent-Document Retriever\n",
    "- Contextual Compression (Reranking)\n",
    "- Ensemble Retriever\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbc802e",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc9d52d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LangSmith tracing enabled\n",
      "‚úÖ API keys configured\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import getpass\n",
    "\n",
    "# Set up API keys\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key:\")\n",
    "os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Enter your Cohere API Key:\")\n",
    "\n",
    "# Optional: Set up LangSmith for advanced evaluation\n",
    "try:\n",
    "    os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"Enter your LangSmith API Key (optional, press Enter to skip):\")\n",
    "    if os.environ[\"LANGCHAIN_API_KEY\"]:\n",
    "        os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "        os.environ[\"LANGCHAIN_PROJECT\"] = \"Retriever-Evaluation\"\n",
    "        print(\"‚úÖ LangSmith tracing enabled\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  LangSmith skipped\")\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è  LangSmith skipped\")\n",
    "\n",
    "print(\"‚úÖ API keys configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98c4927",
   "metadata": {},
   "source": [
    "## Step 2: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3f3134e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 825 complaint documents from CSV\n",
      "‚úÖ Loaded 269 PDF documents\n",
      "‚úÖ Total documents: 1094 (CSV: 825, PDF: 269)\n",
      "\n",
      "Sample complaint: The federal student loan COVID-19 forbearance program ended in XX/XX/XXXX. However, payments were no...\n",
      "Sample PDF content: Volume 3\n",
      "Academic Calendars, Cost of Attendance, and\n",
      "Packaging\n",
      "Introduction\n",
      "This volume of the Feder...\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "# Load CSV data\n",
    "loader = CSVLoader(\n",
    "    file_path=\"./data/complaints.csv\",\n",
    "    metadata_columns=[\n",
    "        \"Date received\", \"Product\", \"Sub-product\", \"Issue\", \"Sub-issue\", \n",
    "        \"Consumer complaint narrative\", \"Company\", \"State\", \"Complaint ID\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "loan_complaint_data = loader.load()\n",
    "\n",
    "# Set page content to complaint narrative\n",
    "for doc in loan_complaint_data:\n",
    "    doc.page_content = doc.metadata[\"Consumer complaint narrative\"]\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(loan_complaint_data)} complaint documents from CSV\")\n",
    "\n",
    "# Load PDF data\n",
    "path = \"data/\"\n",
    "pdf_loader = DirectoryLoader(path, glob=\"*.pdf\", loader_cls=PyMuPDFLoader)\n",
    "pdf_docs = pdf_loader.load()\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(pdf_docs)} PDF documents\")\n",
    "\n",
    "# Combine all documents\n",
    "all_docs = loan_complaint_data + pdf_docs\n",
    "print(f\"‚úÖ Total documents: {len(all_docs)} (CSV: {len(loan_complaint_data)}, PDF: {len(pdf_docs)})\")\n",
    "\n",
    "print(f\"\\nSample complaint: {loan_complaint_data[0].page_content[:100]}...\")\n",
    "print(f\"Sample PDF content: {pdf_docs[0].page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ca8a3d",
   "metadata": {},
   "source": [
    "## Step 3: Create Golden Dataset using RAGAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58e45a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ RAGAS models initialized\n"
     ]
    }
   ],
   "source": [
    "# RAGAS setup\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "# Initialize models for RAGAS\n",
    "generator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\"))\n",
    "generator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings(model=\"text-embedding-3-small\"))\n",
    "\n",
    "print(\"‚úÖ RAGAS models initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01b6b8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic test dataset using RAGAS...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85058f78561448dca85f315d439fb715",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying HeadlinesExtractor:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "412613819eda43f38d22359be0a0ca12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying HeadlineSplitter:   0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "660475098c1b41288315adb8035318ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying SummaryExtractor:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Property 'summary' already exists in node '6e960b'. Skipping!\n",
      "Property 'summary' already exists in node '73f140'. Skipping!\n",
      "Property 'summary' already exists in node 'f25110'. Skipping!\n",
      "Property 'summary' already exists in node '3e841f'. Skipping!\n",
      "Property 'summary' already exists in node '3eb292'. Skipping!\n",
      "Property 'summary' already exists in node '39235c'. Skipping!\n",
      "Property 'summary' already exists in node 'af322b'. Skipping!\n",
      "Property 'summary' already exists in node 'aad1bc'. Skipping!\n",
      "Property 'summary' already exists in node '5c9f93'. Skipping!\n",
      "Property 'summary' already exists in node '5b2c1d'. Skipping!\n",
      "Property 'summary' already exists in node 'a8afb0'. Skipping!\n",
      "Property 'summary' already exists in node 'e3e938'. Skipping!\n",
      "Property 'summary' already exists in node '3d1839'. Skipping!\n",
      "Property 'summary' already exists in node '90cd35'. Skipping!\n",
      "Property 'summary' already exists in node 'f8ba29'. Skipping!\n",
      "Property 'summary' already exists in node 'e34cac'. Skipping!\n",
      "Property 'summary' already exists in node '07d220'. Skipping!\n",
      "Property 'summary' already exists in node '98534d'. Skipping!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d34f90eb65fb47fd8f0209cd8e19dbde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying CustomNodeFilter:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbb03533434a4c94bc6aa9ee0afac2e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:   0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Property 'summary_embedding' already exists in node 'f25110'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '73f140'. Skipping!\n",
      "Property 'summary_embedding' already exists in node 'af322b'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '6e960b'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '3eb292'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '3e841f'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '3d1839'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '90cd35'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '39235c'. Skipping!\n",
      "Property 'summary_embedding' already exists in node 'a8afb0'. Skipping!\n",
      "Property 'summary_embedding' already exists in node 'e34cac'. Skipping!\n",
      "Property 'summary_embedding' already exists in node 'e3e938'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '98534d'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '5c9f93'. Skipping!\n",
      "Property 'summary_embedding' already exists in node 'f8ba29'. Skipping!\n",
      "Property 'summary_embedding' already exists in node 'aad1bc'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '5b2c1d'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '07d220'. Skipping!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58f053166ba44adc9aecb21dff6dec64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying [CosineSimilarityBuilder, OverlapScoreBuilder]:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bf62684872249988888c14c532e824a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating personas:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8074dddd4604527b16b480adb406649",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Scenarios:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc8d44f24999426eb2c61444f4d28f07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Samples:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Generated 12 synthetic QA pairs\n",
      "\n",
      "Dataset columns: ['user_input', 'reference_contexts', 'reference', 'synthesizer_name']\n",
      "Dataset shape: (12, 4)\n",
      "\n",
      "Sample questions from the dataset:\n",
      "1. What does 34 CFR 668.3(b) pertain to in the context of regulatory citations?\n",
      "2. What does the term Subscription-Based Program refer to in educational regulations?\n",
      "3. How do school districts influence the scheduling of clinical experiences in academic programs?\n"
     ]
    }
   ],
   "source": [
    "# Generate synthetic dataset using abstracted SDG\n",
    "from ragas.testset import TestsetGenerator\n",
    "\n",
    "print(\"Generating synthetic test dataset using RAGAS...\")\n",
    "\n",
    "generator = TestsetGenerator(llm=generator_llm, embedding_model=generator_embeddings)\n",
    "\n",
    "# Use subset for cost efficiency\n",
    "# Try PDF docs first as they tend to work better with RAGAS\n",
    "testset_docs = pdf_docs[:30] + loan_complaint_data[:30]  # Mixed approach\n",
    "\n",
    "golden_dataset = generator.generate_with_langchain_docs(\n",
    "    testset_docs, \n",
    "    testset_size=10\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Generated {len(golden_dataset)} synthetic QA pairs\")\n",
    "\n",
    "# Convert to pandas for easier viewing\n",
    "df = golden_dataset.to_pandas()\n",
    "print(f\"\\nDataset columns: {list(df.columns)}\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "\n",
    "# Show sample questions\n",
    "print(\"\\nSample questions from the dataset:\")\n",
    "if 'question' in df.columns:\n",
    "    question_col = 'question'\n",
    "elif 'user_input' in df.columns:\n",
    "    question_col = 'user_input'\n",
    "elif len(df.columns) > 0:\n",
    "    question_col = df.columns[0]  # Use first column as fallback\n",
    "    print(f\"Using column '{question_col}' as questions:\")\n",
    "else:\n",
    "    print(\"No columns found in dataset!\")\n",
    "    question_col = None\n",
    "\n",
    "if question_col:\n",
    "    for i in range(min(3, len(df))):\n",
    "        print(f\"{i+1}. {df.iloc[i][question_col]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39486756",
   "metadata": {},
   "source": [
    "## Step 3.5: Create LangSmith Dataset (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3a76a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating LangSmith dataset...\n",
      "‚úÖ Created LangSmith dataset: Retriever-Evaluation-20250729_111816\n",
      "üìä Added 12 examples to dataset\n"
     ]
    }
   ],
   "source": [
    "# Create LangSmith dataset for advanced evaluation (if LangSmith is available)\n",
    "try:\n",
    "    from langsmith import Client\n",
    "    \n",
    "    if os.environ.get(\"LANGCHAIN_API_KEY\"):\n",
    "        print(\"Creating LangSmith dataset...\")\n",
    "        \n",
    "        client = Client()\n",
    "        dataset_name = f\"Retriever-Evaluation-{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        \n",
    "        # Create dataset\n",
    "        langsmith_dataset = client.create_dataset(\n",
    "            dataset_name=dataset_name,\n",
    "            description=\"Synthetic data for retriever evaluation using RAGAS\"\n",
    "        )\n",
    "        \n",
    "        # Add examples to LangSmith dataset\n",
    "        df = golden_dataset.to_pandas()\n",
    "        \n",
    "        # Find the correct column names\n",
    "        if 'question' in df.columns:\n",
    "            question_col = 'question'\n",
    "        elif 'user_input' in df.columns:\n",
    "            question_col = 'user_input'\n",
    "        else:\n",
    "            question_col = df.columns[0]\n",
    "            \n",
    "        if 'answer' in df.columns:\n",
    "            answer_col = 'answer'\n",
    "        elif 'reference' in df.columns:\n",
    "            answer_col = 'reference'\n",
    "        else:\n",
    "            answer_col = df.columns[1] if len(df.columns) > 1 else question_col\n",
    "        \n",
    "        # Add examples\n",
    "        for idx, row in df.iterrows():\n",
    "            client.create_example(\n",
    "                inputs={\n",
    "                    \"question\": row[question_col]\n",
    "                },\n",
    "                outputs={\n",
    "                    \"answer\": row[answer_col] if answer_col != question_col else \"Generated answer\"\n",
    "                },\n",
    "                metadata={\n",
    "                    \"source\": \"ragas_synthetic\",\n",
    "                    \"retriever_evaluation\": True\n",
    "                },\n",
    "                dataset_id=langsmith_dataset.id\n",
    "            )\n",
    "        \n",
    "        print(f\"‚úÖ Created LangSmith dataset: {dataset_name}\")\n",
    "        print(f\"üìä Added {len(df)} examples to dataset\")\n",
    "        \n",
    "        # Store for later use\n",
    "        LANGSMITH_DATASET_NAME = dataset_name\n",
    "        USE_LANGSMITH = True\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  LangSmith API key not found, skipping dataset creation\")\n",
    "        USE_LANGSMITH = False\n",
    "        LANGSMITH_DATASET_NAME = None\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  LangSmith not available, install with: pip install langsmith\")\n",
    "    USE_LANGSMITH = False\n",
    "    LANGSMITH_DATASET_NAME = None\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  LangSmith setup failed: {e}\")\n",
    "    USE_LANGSMITH = False\n",
    "    LANGSMITH_DATASET_NAME = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16281ccf",
   "metadata": {},
   "source": [
    "## Step 4: Set Up Retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "237530af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up retrievers...\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.retrievers import ParentDocumentRetriever, EnsembleRetriever\n",
    "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
    "from langchain_cohere import CohereRerank\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient, models\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "\n",
    "# Initialize models\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "chat_model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "print(\"Setting up retrievers...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d3d0f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 1. Naive retriever ready\n",
      "‚úÖ 2. BM25 retriever ready\n",
      "‚úÖ 3. Multi-query retriever ready\n",
      "‚úÖ 4. Parent document retriever ready\n",
      "‚úÖ 5. Contextual compression retriever ready\n",
      "‚úÖ 6. Ensemble retriever ready\n",
      "\n",
      "‚úÖ All retrievers initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# 1. Naive Retriever\n",
    "vectorstore = Qdrant.from_documents(\n",
    "    all_docs,\n",
    "    embeddings,\n",
    "    location=\":memory:\",\n",
    "    collection_name=\"LoanComplaints\"\n",
    ")\n",
    "naive_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "print(\"‚úÖ 1. Naive retriever ready\")\n",
    "\n",
    "# 2. BM25 Retriever\n",
    "bm25_retriever = BM25Retriever.from_documents(all_docs)\n",
    "bm25_retriever.k = 5\n",
    "print(\"‚úÖ 2. BM25 retriever ready\")\n",
    "\n",
    "# 3. Multi-Query Retriever\n",
    "multi_query_retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=naive_retriever, llm=chat_model\n",
    ")\n",
    "print(\"‚úÖ 3. Multi-query retriever ready\")\n",
    "\n",
    "# 4. Parent Document Retriever\n",
    "parent_docs = all_docs\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=750)\n",
    "\n",
    "# Create new QdrantClient and collection\n",
    "client = QdrantClient(location=\":memory:\")\n",
    "client.create_collection(\n",
    "    collection_name=\"full_documents\",\n",
    "    vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE)\n",
    ")\n",
    "\n",
    "parent_document_vectorstore = QdrantVectorStore(\n",
    "    collection_name=\"full_documents\", \n",
    "    embedding=embeddings, \n",
    "    client=client\n",
    ")\n",
    "\n",
    "store = InMemoryStore()\n",
    "parent_document_retriever = ParentDocumentRetriever(\n",
    "    vectorstore=parent_document_vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    ")\n",
    "\n",
    "parent_document_retriever.add_documents(parent_docs, ids=None)\n",
    "print(\"‚úÖ 4. Parent document retriever ready\")\n",
    "\n",
    "# 5. Contextual Compression Retriever\n",
    "compressor = CohereRerank(model=\"rerank-v3.5\")\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=naive_retriever\n",
    ")\n",
    "print(\"‚úÖ 5. Contextual compression retriever ready\")\n",
    "\n",
    "# 6. Ensemble Retriever\n",
    "retriever_list = [bm25_retriever, naive_retriever, parent_document_retriever, compression_retriever, multi_query_retriever]\n",
    "equal_weighting = [1/len(retriever_list)] * len(retriever_list)\n",
    "\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=retriever_list, weights=equal_weighting\n",
    ")\n",
    "print(\"‚úÖ 6. Ensemble retriever ready\")\n",
    "\n",
    "print(\"\\n‚úÖ All retrievers initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62db951",
   "metadata": {},
   "source": [
    "## Step 5: Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67a3f748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Evaluation functions ready\n"
     ]
    }
   ],
   "source": [
    "def evaluate_retriever_simple(retriever, retriever_name, questions):\n",
    "    \"\"\"\n",
    "    Simple evaluation function that measures retrieval performance\n",
    "    \"\"\"\n",
    "    print(f\"\\nEvaluating {retriever_name}...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    total_docs_retrieved = 0\n",
    "    successful_retrievals = 0\n",
    "    \n",
    "    for i, question in enumerate(questions):\n",
    "        try:\n",
    "            # Retrieve documents\n",
    "            docs = retriever.get_relevant_documents(question)\n",
    "            total_docs_retrieved += len(docs)\n",
    "            successful_retrievals += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error on question {i+1}: {e}\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    avg_docs_per_query = total_docs_retrieved / len(questions) if questions else 0\n",
    "    success_rate = successful_retrievals / len(questions) if questions else 0\n",
    "    latency = end_time - start_time\n",
    "    \n",
    "    results = {\n",
    "        'retriever_name': retriever_name,\n",
    "        'success_rate': success_rate,\n",
    "        'avg_docs_per_query': avg_docs_per_query,\n",
    "        'total_latency': latency,\n",
    "        'avg_latency_per_query': latency / len(questions) if questions else 0\n",
    "    }\n",
    "    \n",
    "    print(f\"  ‚úÖ Success rate: {success_rate:.2%}\")\n",
    "    print(f\"  ‚úÖ Avg docs per query: {avg_docs_per_query:.1f}\")\n",
    "    print(f\"  ‚úÖ Latency: {latency:.2f}s\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def estimate_cost(retriever_name, num_queries):\n",
    "    \"\"\"Estimate API costs per retriever type\"\"\"\n",
    "    cost_per_query = {\n",
    "        'Naive': 0.002,  # OpenAI embedding calls\n",
    "        'BM25': 0.0,     # No API calls\n",
    "        'Multi-Query': 0.008,  # Multiple LLM calls + embeddings\n",
    "        'Parent Document': 0.003,  # Embeddings + some overhead\n",
    "        'Contextual Compression': 0.015,  # Cohere rerank + embeddings\n",
    "        'Ensemble': 0.020,  # All of the above combined\n",
    "    }\n",
    "    return cost_per_query.get(retriever_name.split()[0], 0.005) * num_queries\n",
    "\n",
    "print(\"‚úÖ Evaluation functions ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682c23d8",
   "metadata": {},
   "source": [
    "## Step 6: Run Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc3b4619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation on 12 questions...\n",
      "============================================================\n",
      "\n",
      "Evaluating Naive...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f1/cmsz4dgn2y194hgy1n_pldjc0000gn/T/ipykernel_87520/2445073462.py:14: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriever.get_relevant_documents(question)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Success rate: 100.00%\n",
      "  ‚úÖ Avg docs per query: 5.0\n",
      "  ‚úÖ Latency: 5.74s\n",
      "\n",
      "Evaluating BM25...\n",
      "  ‚úÖ Success rate: 100.00%\n",
      "  ‚úÖ Avg docs per query: 5.0\n",
      "  ‚úÖ Latency: 0.08s\n",
      "\n",
      "Evaluating Multi-Query...\n",
      "  ‚úÖ Success rate: 100.00%\n",
      "  ‚úÖ Avg docs per query: 6.4\n",
      "  ‚úÖ Latency: 39.04s\n",
      "\n",
      "Evaluating Parent Document...\n",
      "  ‚úÖ Success rate: 100.00%\n",
      "  ‚úÖ Avg docs per query: 2.9\n",
      "  ‚úÖ Latency: 4.32s\n",
      "\n",
      "Evaluating Contextual Compression...\n",
      "  Error on question 11: status_code: 429, body: data=None id='32c77a2a-a91b-41e4-8683-6eb06984fa32' message=\"You are using a Trial key, which is limited to 10 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions\"\n",
      "  Error on question 12: status_code: 429, body: data=None id='073344e7-ba46-46b4-bc5d-8ffb43574509' message=\"You are using a Trial key, which is limited to 10 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions\"\n",
      "  ‚úÖ Success rate: 83.33%\n",
      "  ‚úÖ Avg docs per query: 2.5\n",
      "  ‚úÖ Latency: 7.00s\n",
      "\n",
      "Evaluating Ensemble...\n",
      "  Error on question 1: status_code: 429, body: data=None id='50d8a302-a0c4-45d9-ba7c-b1100db9ee7b' message=\"You are using a Trial key, which is limited to 10 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions\"\n",
      "  Error on question 2: status_code: 429, body: data=None id='8ba1163b-4e2c-40fb-97a7-a813c607bd75' message=\"You are using a Trial key, which is limited to 10 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions\"\n",
      "  Error on question 3: status_code: 429, body: data=None id='345dbbdf-c32b-4beb-a310-29e5e1262598' message=\"You are using a Trial key, which is limited to 10 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions\"\n",
      "  Error on question 4: status_code: 429, body: data=None id='666b3cef-dd01-4720-bc0d-64d1be354e2d' message=\"You are using a Trial key, which is limited to 10 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions\"\n",
      "  Error on question 5: status_code: 429, body: data=None id='2dcb405e-762f-43d8-9d98-f7f39a7293be' message=\"You are using a Trial key, which is limited to 10 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions\"\n",
      "  Error on question 6: status_code: 429, body: data=None id='2cb74d21-6dad-4073-a614-ad840d949660' message=\"You are using a Trial key, which is limited to 10 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions\"\n",
      "  Error on question 7: status_code: 429, body: data=None id='2ba9b4bd-cc79-45a7-a325-aa9eec5b27c2' message=\"You are using a Trial key, which is limited to 10 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions\"\n",
      "  Error on question 8: status_code: 429, body: data=None id='dd816fc2-0c1d-450a-8ce7-b8e5cfbf52c4' message=\"You are using a Trial key, which is limited to 10 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions\"\n",
      "  Error on question 9: status_code: 429, body: data=None id='2d037883-2970-4a90-b66f-77d114b347c4' message=\"You are using a Trial key, which is limited to 10 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions\"\n",
      "  Error on question 10: status_code: 429, body: data=None id='d2428fb2-3e94-4c30-b8ef-fff0d9143917' message=\"You are using a Trial key, which is limited to 10 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions\"\n",
      "  Error on question 11: status_code: 429, body: data=None id='09d517ac-e4d2-49e4-b923-18dd68ad1b1f' message=\"You are using a Trial key, which is limited to 10 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions\"\n",
      "  Error on question 12: status_code: 429, body: data=None id='f766a441-7e67-4ac4-a9d1-8ca6aaa4e0a3' message=\"You are using a Trial key, which is limited to 10 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions\"\n",
      "  ‚úÖ Success rate: 0.00%\n",
      "  ‚úÖ Avg docs per query: 0.0\n",
      "  ‚úÖ Latency: 12.80s\n",
      "\n",
      "‚úÖ All evaluations completed!\n"
     ]
    }
   ],
   "source": [
    "# Extract questions from RAGAS dataset\n",
    "df = golden_dataset.to_pandas()\n",
    "\n",
    "# Find the correct question column\n",
    "if 'question' in df.columns:\n",
    "    question_col = 'question'\n",
    "elif 'user_input' in df.columns:\n",
    "    question_col = 'user_input'\n",
    "elif len(df.columns) > 0:\n",
    "    question_col = df.columns[0]  # Use first column as fallback\n",
    "    print(f\"Using column '{question_col}' as questions\")\n",
    "else:\n",
    "    raise ValueError(\"No suitable question column found in RAGAS dataset!\")\n",
    "\n",
    "questions = df[question_col].tolist()\n",
    "\n",
    "print(f\"Running evaluation on {len(questions)} questions...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Define retrievers to evaluate\n",
    "retrievers_to_test = [\n",
    "    (naive_retriever, \"Naive\"),\n",
    "    (bm25_retriever, \"BM25\"),\n",
    "    (multi_query_retriever, \"Multi-Query\"),\n",
    "    (parent_document_retriever, \"Parent Document\"),\n",
    "    (compression_retriever, \"Contextual Compression\"),\n",
    "    (ensemble_retriever, \"Ensemble\")\n",
    "]\n",
    "\n",
    "# Run evaluations\n",
    "results = []\n",
    "for retriever, name in retrievers_to_test:\n",
    "    result = evaluate_retriever_simple(retriever, name, questions)\n",
    "    result['estimated_cost'] = estimate_cost(name, len(questions))\n",
    "    results.append(result)\n",
    "\n",
    "print(\"\\n‚úÖ All evaluations completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098ac3ad",
   "metadata": {},
   "source": [
    "## Step 7: Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ddd040ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Performance Summary:\n",
      "        retriever_name  success_rate  avg_docs_per_query  total_latency  estimated_cost\n",
      "                 Naive        1.0000              5.0000         5.7410           0.024\n",
      "                  BM25        1.0000              5.0000         0.0795           0.000\n",
      "           Multi-Query        1.0000              6.4167        39.0379           0.096\n",
      "       Parent Document        1.0000              2.9167         4.3158           0.060\n",
      "Contextual Compression        0.8333              2.5000         7.0007           0.060\n",
      "\n",
      "üèÜ WINNERS:\n",
      "‚ö° Fastest: BM25 (0.08s)\n",
      "üí∞ Cheapest: BM25 ($0.0000)\n",
      "üìö Most Comprehensive: Multi-Query (6.4 docs/query)\n",
      "üéñÔ∏è  Best Overall: BM25 (300.678)\n"
     ]
    }
   ],
   "source": [
    "# Create results dataframe\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Filter successful retrievers\n",
    "successful_results = results_df[results_df['success_rate'] > 0.8].copy()\n",
    "\n",
    "if len(successful_results) == 0:\n",
    "    print(\"‚ö†Ô∏è  No retrievers achieved >80% success rate. Showing all results:\")\n",
    "    successful_results = results_df.copy()\n",
    "\n",
    "# Display main metrics\n",
    "display_cols = ['retriever_name', 'success_rate', 'avg_docs_per_query', \n",
    "               'total_latency', 'estimated_cost']\n",
    "print(\"\\nüìà Performance Summary:\")\n",
    "print(successful_results[display_cols].round(4).to_string(index=False))\n",
    "\n",
    "# Find best performers\n",
    "fastest = successful_results.loc[successful_results['total_latency'].idxmin()]\n",
    "cheapest = successful_results.loc[successful_results['estimated_cost'].idxmin()]\n",
    "most_docs = successful_results.loc[successful_results['avg_docs_per_query'].idxmax()]\n",
    "\n",
    "# Calculate combined score (simple weighted average)\n",
    "successful_results = successful_results.copy()\n",
    "successful_results['combined_score'] = (\n",
    "    0.4 * successful_results['success_rate'] + \n",
    "    0.3 * (1 / (successful_results['total_latency'] + 1)) + \n",
    "    0.3 * (1 / (successful_results['estimated_cost'] + 0.001))\n",
    ")\n",
    "\n",
    "best_overall = successful_results.loc[successful_results['combined_score'].idxmax()]\n",
    "\n",
    "print(\"\\nüèÜ WINNERS:\")\n",
    "print(f\"‚ö° Fastest: {fastest['retriever_name']} ({fastest['total_latency']:.2f}s)\")\n",
    "print(f\"üí∞ Cheapest: {cheapest['retriever_name']} (${cheapest['estimated_cost']:.4f})\")\n",
    "print(f\"üìö Most Comprehensive: {most_docs['retriever_name']} ({most_docs['avg_docs_per_query']:.1f} docs/query)\")\n",
    "print(f\"üéñÔ∏è  Best Overall: {best_overall['retriever_name']} ({best_overall['combined_score']:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f741bb9",
   "metadata": {},
   "source": [
    "## Step 8: Final Analysis and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bff0d2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üí° RECOMMENDATIONS BY USE CASE:\n",
      "\n",
      "1. ‚ö° For Speed: BM25\n",
      "   - Fastest response time: 0.08s\n",
      "   - Good for: Real-time applications, high-throughput systems\n",
      "\n",
      "2. üí∞ For Cost Efficiency: BM25\n",
      "   - Lowest cost: $0.0000\n",
      "   - Good for: Budget-conscious deployments, high-volume usage\n",
      "\n",
      "3. üìö For Comprehensive Results: Multi-Query\n",
      "   - Most documents per query: 6.4\n",
      "   - Good for: Research applications, thorough analysis\n",
      "\n",
      "4. ‚öñÔ∏è  For Balanced Performance: BM25\n",
      "   - Best combined score: 300.678\n",
      "   - Good for: General-purpose applications, balanced requirements\n",
      "\n",
      "üîç KEY INSIGHTS:\n",
      "- RAGAS provides realistic test questions based on actual data\n",
      "- BM25 is typically fastest and cheapest (no API calls)\n",
      "- Embedding-based methods provide better semantic understanding\n",
      "- Multi-query retrieval improves recall but increases cost\n",
      "- Ensemble methods balance different strengths\n",
      "- Compression/reranking improves quality but adds latency\n",
      "- Parent-document retrievers provide more context per result\n",
      "\n",
      "üìà EVALUATION METRICS:\n",
      "- Success Rate: Percentage of queries processed successfully\n",
      "- Docs Per Query: Average number of documents retrieved\n",
      "- Latency: Time to retrieve and process documents\n",
      "- Cost: Estimated API usage costs\n",
      "\n",
      "üìä EVALUATION COMPLETED: 2025-07-29 11:24:38\n"
     ]
    }
   ],
   "source": [
    "if len(successful_results) > 0:\n",
    "    print(\"\\nüí° RECOMMENDATIONS BY USE CASE:\")\n",
    "    print(f\"\\n1. ‚ö° For Speed: {fastest['retriever_name']}\")\n",
    "    print(f\"   - Fastest response time: {fastest['total_latency']:.2f}s\")\n",
    "    print(f\"   - Good for: Real-time applications, high-throughput systems\")\n",
    "    \n",
    "    print(f\"\\n2. üí∞ For Cost Efficiency: {cheapest['retriever_name']}\")\n",
    "    print(f\"   - Lowest cost: ${cheapest['estimated_cost']:.4f}\")\n",
    "    print(f\"   - Good for: Budget-conscious deployments, high-volume usage\")\n",
    "    \n",
    "    print(f\"\\n3. üìö For Comprehensive Results: {most_docs['retriever_name']}\")\n",
    "    print(f\"   - Most documents per query: {most_docs['avg_docs_per_query']:.1f}\")\n",
    "    print(f\"   - Good for: Research applications, thorough analysis\")\n",
    "    \n",
    "    print(f\"\\n4. ‚öñÔ∏è  For Balanced Performance: {best_overall['retriever_name']}\")\n",
    "    print(f\"   - Best combined score: {best_overall['combined_score']:.3f}\")\n",
    "    print(f\"   - Good for: General-purpose applications, balanced requirements\")\n",
    "    \n",
    "    print(\"\\nüîç KEY INSIGHTS:\")\n",
    "    print(\"- RAGAS provides realistic test questions based on actual data\")\n",
    "    print(\"- BM25 is typically fastest and cheapest (no API calls)\")\n",
    "    print(\"- Embedding-based methods provide better semantic understanding\")\n",
    "    print(\"- Multi-query retrieval improves recall but increases cost\")\n",
    "    print(\"- Ensemble methods balance different strengths\")\n",
    "    print(\"- Compression/reranking improves quality but adds latency\")\n",
    "    print(\"- Parent-document retrievers provide more context per result\")\n",
    "    \n",
    "    print(\"\\nüìà EVALUATION METRICS:\")\n",
    "    print(\"- Success Rate: Percentage of queries processed successfully\")\n",
    "    print(\"- Docs Per Query: Average number of documents retrieved\")\n",
    "    print(\"- Latency: Time to retrieve and process documents\")\n",
    "    print(\"- Cost: Estimated API usage costs\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  All retrievers had issues. Check your setup and data.\")\n",
    "\n",
    "print(f\"\\nüìä EVALUATION COMPLETED: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f3827f",
   "metadata": {},
   "source": [
    "## Step 9: LangSmith Advanced Evaluation for ALL Retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c51373d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî¨ Running LangSmith evaluation for ALL retrievers...\n",
      "üìä Evaluating 6 retrievers with LangSmith...\n",
      "üîç Evaluators: QA Accuracy, Helpfulness, Empathy\n",
      "\n",
      "üîç Evaluating Naive retriever...\n",
      "View the evaluation results for experiment: 'retriever_naive-633802b9' at:\n",
      "https://smith.langchain.com/o/a8b64252-5f0f-4f35-a048-c004586e098a/datasets/ac74f704-3930-48e5-9de2-f08c2498e0b8/compare?selectedSessions=5363d56e-8be9-473e-9708-4f2e17fb9571\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e0d2dc45274475ab7b247a05e3dc76b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Naive evaluation completed successfully\n",
      "\n",
      "üîç Evaluating BM25 retriever...\n",
      "View the evaluation results for experiment: 'retriever_bm25-6f2f351e' at:\n",
      "https://smith.langchain.com/o/a8b64252-5f0f-4f35-a048-c004586e098a/datasets/ac74f704-3930-48e5-9de2-f08c2498e0b8/compare?selectedSessions=f2f17d7b-c0f5-4ca4-9ad5-582101d06363\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a4888e23082487cab0691886c978c73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ BM25 evaluation completed successfully\n",
      "\n",
      "üîç Evaluating Multi-Query retriever...\n",
      "View the evaluation results for experiment: 'retriever_multi_query-79e4f50a' at:\n",
      "https://smith.langchain.com/o/a8b64252-5f0f-4f35-a048-c004586e098a/datasets/ac74f704-3930-48e5-9de2-f08c2498e0b8/compare?selectedSessions=984e92df-fe41-4f92-9057-04ebb5f100c7\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d048b3e78a1748eb9d7962cfb28752b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Multi-Query evaluation completed successfully\n",
      "\n",
      "üîç Evaluating Parent-Document retriever...\n",
      "View the evaluation results for experiment: 'retriever_parent_document-674f3ef1' at:\n",
      "https://smith.langchain.com/o/a8b64252-5f0f-4f35-a048-c004586e098a/datasets/ac74f704-3930-48e5-9de2-f08c2498e0b8/compare?selectedSessions=62840a2e-3d2c-4d9b-bbce-5deb1894ab1d\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5045e45562144304bb88c0b0745bc9c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Parent-Document evaluation completed successfully\n",
      "\n",
      "üîç Evaluating Contextual-Compression retriever...\n",
      "View the evaluation results for experiment: 'retriever_contextual_compression-40e10208' at:\n",
      "https://smith.langchain.com/o/a8b64252-5f0f-4f35-a048-c004586e098a/datasets/ac74f704-3930-48e5-9de2-f08c2498e0b8/compare?selectedSessions=86ad2823-376a-4d39-95c0-6af895623b22\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82150390691642cea47f9c958672a945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Contextual-Compression evaluation completed successfully\n",
      "\n",
      "üîç Evaluating Ensemble retriever...\n",
      "View the evaluation results for experiment: 'retriever_ensemble-1860fdea' at:\n",
      "https://smith.langchain.com/o/a8b64252-5f0f-4f35-a048-c004586e098a/datasets/ac74f704-3930-48e5-9de2-f08c2498e0b8/compare?selectedSessions=99bb04b6-8c41-453a-b86e-df1376326cbc\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13571e33bfba4073ae88273ee08c9f38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ensemble evaluation completed successfully\n",
      "\n",
      "üéØ All retriever evaluations completed!\n",
      "üìä Check LangSmith dashboard for detailed comparison results!\n",
      "üîç Each retriever has been evaluated for: QA Accuracy, Helpfulness, Empathy\n"
     ]
    }
   ],
   "source": [
    "if USE_LANGSMITH:\n",
    "    print(\"\\nüî¨ Running LangSmith evaluation for ALL retrievers...\")\n",
    "    \n",
    "    try:\n",
    "        from langsmith.evaluation import LangChainStringEvaluator, evaluate\n",
    "        from langchain.prompts import ChatPromptTemplate\n",
    "        from langchain.schema import StrOutputParser\n",
    "        from operator import itemgetter\n",
    "        \n",
    "        # Create RAG chain for evaluation\n",
    "        RAG_PROMPT = \"\"\"Given the provided context and question, answer the question based only on the context.\n",
    "If you cannot answer based on the context, say \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\"\"\"\n",
    "        \n",
    "        rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT)\n",
    "        eval_llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "        \n",
    "        # QA evaluator (following example.ipynb pattern)\n",
    "        qa_evaluator = LangChainStringEvaluator(\"qa\", config={\"llm\": eval_llm})\n",
    "        \n",
    "        # Labeled helpfulness evaluator (following example.ipynb pattern)\n",
    "        labeled_helpfulness_evaluator = LangChainStringEvaluator(\n",
    "            \"labeled_criteria\",\n",
    "            config={\n",
    "                \"criteria\": {\n",
    "                    \"helpfulness\": (\n",
    "                        \"Is this submission helpful to the user,\"\n",
    "                        \" taking into account the correct reference answer?\"\n",
    "                    )\n",
    "                },\n",
    "                \"llm\": eval_llm\n",
    "            },\n",
    "            prepare_data=lambda run, example: {\n",
    "                \"prediction\": run.outputs[\"output\"],\n",
    "                \"reference\": example.outputs[\"answer\"],\n",
    "                \"input\": example.inputs[\"question\"],\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Empathy evaluator (following example.ipynb pattern)\n",
    "        empathy_evaluator = LangChainStringEvaluator(\n",
    "            \"criteria\",\n",
    "            config={\n",
    "                \"criteria\": {\n",
    "                    \"empathy\": \"Is this response empathetic? Does it make the user feel like they are being heard?\",\n",
    "                },\n",
    "                \"llm\": eval_llm\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Define all retrievers to evaluate\n",
    "        all_retrievers_to_evaluate = [\n",
    "            (naive_retriever, \"Naive\"),\n",
    "            (bm25_retriever, \"BM25\"),\n",
    "            (multi_query_retriever, \"Multi-Query\"),\n",
    "            (parent_document_retriever, \"Parent-Document\"),\n",
    "            (compression_retriever, \"Contextual-Compression\"),\n",
    "            (ensemble_retriever, \"Ensemble\")\n",
    "        ]\n",
    "        \n",
    "        print(f\"üìä Evaluating {len(all_retrievers_to_evaluate)} retrievers with LangSmith...\")\n",
    "        print(\"üîç Evaluators: QA Accuracy, Helpfulness, Empathy\")\n",
    "        \n",
    "        # Evaluate each retriever\n",
    "        for retriever, name in all_retrievers_to_evaluate:\n",
    "            print(f\"\\nüîç Evaluating {name} retriever...\")\n",
    "            \n",
    "            try:\n",
    "                # Create RAG chain for this retriever\n",
    "                rag_chain = (\n",
    "                    {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}\n",
    "                    | rag_prompt | eval_llm | StrOutputParser()\n",
    "                )\n",
    "                \n",
    "                # Run evaluation for this retriever\n",
    "                experiment_results = evaluate(\n",
    "                    rag_chain.invoke,\n",
    "                    data=LANGSMITH_DATASET_NAME,\n",
    "                    evaluators=[\n",
    "                        qa_evaluator,\n",
    "                        labeled_helpfulness_evaluator,\n",
    "                        empathy_evaluator\n",
    "                    ],\n",
    "                    metadata={\n",
    "                        \"retriever_type\": name, \n",
    "                        \"evaluation_run\": \"all_retrievers\",\n",
    "                        \"evaluators\": \"qa_helpfulness_empathy\"\n",
    "                    },\n",
    "                    experiment_prefix=f\"retriever_{name.lower().replace(' ', '_').replace('-', '_')}\"\n",
    "                )\n",
    "                \n",
    "                print(f\"‚úÖ {name} evaluation completed successfully\")\n",
    "                \n",
    "                # Add rate limiting delay between retrievers\n",
    "                time.sleep(3)  # 3 second delay between retrievers\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå {name} evaluation failed: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(\"\\nüéØ All retriever evaluations completed!\")\n",
    "        print(\"üìä Check LangSmith dashboard for detailed comparison results!\")\n",
    "        print(\"üîç Each retriever has been evaluated for: QA Accuracy, Helpfulness, Empathy\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå LangSmith evaluation failed: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Skipping LangSmith evaluation (not configured)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf725a8",
   "metadata": {},
   "source": [
    "![LangSmith Screenshot](./img/screenshot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d146a4",
   "metadata": {},
   "source": [
    "Here is a concise explanation you can use in markdown to justify why the BM25 retriever is the best for handling student loan data and complaints, based on your evaluation results:\n",
    "\n",
    "---\n",
    "\n",
    "### Why BM25 is the Best Retriever for Student Loan Data and Complaints:\n",
    "\n",
    "After evaluating multiple retrieval methods‚Äîincluding Naive, BM25, Multi-Query, Parent-Document, Contextual-Compression, and Ensemble retrievers‚Äî**BM25 clearly emerged as the top performer** for the following reasons:\n",
    "\n",
    "* **Highest Correctness**:\n",
    "  BM25 scored highest in correctness (9 correct answers out of all evaluated responses), indicating superior ability to retrieve relevant information from student loan data and complaints.\n",
    "\n",
    "* **Strong Helpfulness**:\n",
    "  BM25 provided highly helpful responses, achieving a strong balance between precision and recall, essential for handling detailed and specific user queries related to student loans.\n",
    "\n",
    "* **Low Latency**:\n",
    "  BM25 demonstrated very low latency (P50 at 3.52s, P99 at 9.74s), meaning it can quickly return relevant results, making it highly suitable for user-facing applications requiring responsiveness.\n",
    "\n",
    "* **Simplicity and Robustness**:\n",
    "  Unlike more complex methods (such as ensemble or contextual-compression), BM25 operates on simple keyword-matching principles, making it robust for structured, complaint-driven data that often contains repetitive phrasing or terminology.\n",
    "\n",
    "**Conclusion**:\n",
    "Given its highest correctness score, solid helpfulness, lowest latency, and robustness to structured textual data, BM25 stands out as the optimal choice for effectively retrieving and managing student loan complaints and related information.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623397d8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
